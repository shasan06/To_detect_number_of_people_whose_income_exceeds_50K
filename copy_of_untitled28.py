# -*- coding: utf-8 -*-
"""Copy of Untitled28.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PEwEGsDiuBHFM6MxitZqjbzI7QQXbluA
"""

import numpy as np
import pandas as pd 
from pandas import read_csv
import matplotlib.pyplot as plt
import seaborn as sns
from keras.layers import Dense, Dropout,Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.models import Sequential
from tensorflow.keras.models import Sequential
from keras.layers import Dropout
from keras.layers.core import Activation
from keras.optimizers import Optimizer
from sklearn.preprocessing import LabelBinarizer
from keras.utils import to_categorical 
from keras import backend as K
import keras
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from numpy import set_printoptions
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE , chi2
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.decomposition import PCA
from keras.wrappers.scikit_learn import KerasClassifier
from matplotlib import pyplot
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix, classification_report
import math
import time
from sklearn.metrics import mean_squared_error

"""**Data Preparation**

1. Step 1 
One flaw in the dataset I noticed is that, it contains question marks. So I  tried to remove this.



"""

#Step1a Preparing the training data
#https://www.geeksforgeeks.org/use-of-na_values-parameter-in-read_csv-function-of-pandas-in-python/
sample1 = 'adult.data'
sample2 = 'adult.test'
names = ('age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','class')
#data_train = read_csv(filename1, names=names, index_col=0)
#data_test = read_csv(filename2, names=names)
question_mark_traindata = pd.read_csv(sample1,names= names, na_values=[' ?','?'])
question_mark_testdata = pd.read_csv(sample2, names= names, na_values=[' ?','?'])

# To visualise the train data that has question marks in the categorical columns. Hence there are 32561 rows and 15 columns
#question_mark_traindata.shape
question_mark_traindata
#question_mark_traindata.describe

# To visualise the test data that has question marks in the categorical columns. Hence there are 16282 rows and 15 columns
#question_mark_testdata.shape
question_mark_testdata

# So dropping rows which has question_mark in the train dataset. We can see below that from 32561 rows, we have 30162 rows = 2399 rows with question marks are dropped 
train_data = question_mark_traindata.dropna()
train_data # train data is now cleaned

# So dropping rows which has question_mark in the test dataset. We can see below that from 16282 rows, we have 15060 rows = 1222 rows with question marks are dropped 
test_data = question_mark_testdata.dropna()
test_data # test data is now cleaned

"""Step 2 -- Using Label Encoder"""

#Step1b Now using Label Encoder on train dataset so that the categorical variables are converted into continuous variables. Label Encoder compares the data and assigns labels according to the comparision made.
labelencoder_X = LabelEncoder()
train_data1 = train_data.apply(labelencoder_X.fit_transform)
train_data1

#Now using Label Encoder on test dataset so that the categorical variables are converted into continuous variables. Label Encoder compares the data and assigns labels according to the comparision made.
labelencoder_Y = LabelEncoder()
test_data1 = test_data.apply(labelencoder_Y.fit_transform)
test_data1

"""Step 3- Using One hot Encoding. Concluded not to use at too many features and will lead to the dimensionality problem.
https://stats.stackexchange.com/questions/371750/using-categorical-feature-as-both-a-continuous-feature-and-also-doing-one-hot-e
"""

# #Step 3 the columns are convered to 0s and 1s by using one hot encoder. With One hot encoding we cannot remove any features. So no filter, Wrapper or Embedded method needed. Also no need to even Scale the data.
# #onehotvector gives a feature for every categorical option. No Need to do Feature extraction and Feature selection when using one hot encoder.
# array = train_data1.values #first get the train data into arrays
# X_train = array[:, 0:14] #all rows of all columns except the last column which is a target variable
# Y_train = array[:, 14] # the last columns
# from sklearn.compose import ColumnTransformer


# ct = ColumnTransformer(
#     [('oh_enc', OneHotEncoder(sparse=False), [1, 3, 5, 6, 7, 8, 9, 13]),],  # the column numbers I want to apply this to
#     remainder='passthrough'  # This leaves the rest of my columns in place
# )
# X_traindata = ct.fit_transform(X_train)
# X_traindata

# #print(ct.fit_transform(X)) # Notice the output is a string
# Z_train = pd.DataFrame(X_traindata)
# Z_train

"""Step 4 Using Minmax Scalar"""

#Step 4a MinmaxScalar on Training dataset
X_train = train_data1.values[:, 0:14] #all rows of all columns except the last column which is a target variable
Y_train = train_data1.values[:, 14]
#rescaled data between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
rescaledX = scaler.fit_transform(X_train)
#summarize transformed data
set_printoptions(precision=3)
print(rescaledX)
M = pd.DataFrame(rescaledX) #M shows that train data has been scaled
M

#Step 4b MinmaxScalar on Testing dataset
X_test = test_data1.values[:, 0:14] #all rows of all columns except the last column which is a target variable
Y_test = test_data1.values[:, 14]
#rescaled data between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
rescaledY = scaler.fit_transform(X_test)
#summarize transformed data
set_printoptions(precision=3)
print(rescaledY)
N = pd.DataFrame(rescaledY) #M shows that test data has been scaled
N

"""Step 5 Using the Feature Selection(FS) Methods
Step 5a FS - Wraper Method - RFE i.e Recursive Feature Elimination
"""

#Step 5a Wrapper Method Applied on the training dataset
print('Recursive Feature Elimination')
print()
wrapper_model = LogisticRegression(solver='liblinear') 
wrapper_model = RFE(wrapper_model, 10) 
wrapper_model_train = wrapper_model.fit(rescaledX,Y_train)
wrapper_model_train_features = wrapper_model_train.transform(rescaledX)
print(wrapper_model_train_features)
print()
print(wrapper_model_train_features.shape)
print('\n')

#Step 5a Wrapper Method Applied on the testing dataset
print('Recursive Feature Elimination')
print()
wrapper_model = LogisticRegression(solver='liblinear') 
wrapper_model = RFE(wrapper_model, 10) 
wrapper_model_test = wrapper_model.fit(rescaledY,Y_test)
wrapper_model_test_features = wrapper_model_test.transform(rescaledY)
print(wrapper_model_test_features)
print()
print(wrapper_model_test_features.shape)
print('\n')

"""Step 5b Using Feature Selection - Embedded Method i.e ExtraTreesClassifier"""

#Step 5b Embedded Method Applied on the training dataset
print('ExtraTreesClassifier')
print()
embedded_model = ExtraTreesClassifier(n_estimators=10)
embedded_model_train = embedded_model.fit(rescaledX,Y_train)
clf_emb_model = SelectFromModel(embedded_model_train , prefit=True)
embedded_model_train_features = clf_emb_model.transform(rescaledX)
print(embedded_model_train_features)
print()
print(embedded_model_train_features.shape)
print('\n')

#Step 5b Embedded Method Applied on the testing dataset
print('ExtraTreesClassifier')
print()
embedded_model = ExtraTreesClassifier(n_estimators=10)
embedded_model_test = embedded_model.fit(rescaledY,Y_test)
clf_emb_model = SelectFromModel(embedded_model_test , prefit=True)
embedded_model_test_features = clf_emb_model.transform(rescaledY)
print(embedded_model_test_features)
print()
print(embedded_model_test_features.shape)
print('\n')

"""Step 5c Using Filter Method - SelectKbest"""

#Step 5c Filter Method Applied on the training dataset
print('selectKBest')
print()
selectKbest_model = SelectKBest(score_func=chi2, k=10)
fs = selectKbest_model.fit(rescaledX, Y_train)
# summarize scores
set_printoptions(precision=3)
#print(fit.score_)
selectKbest_train_features = fs.transform(rescaledX)
# summarize selected features
print(selectKbest_train_features)
print()
print(selectKbest_train_features.shape)
print('\n')

#Step 5c Filter Method Applied on the testing dataset
print('selectKBest')
print()
selectKbest_model = SelectKBest(score_func=chi2, k=10)
fs = selectKbest_model.fit(rescaledY, Y_test)
# summarize scores
set_printoptions(precision=3)
#print(fit.score_)
selectKbest_test_features = fs.transform(rescaledY)
# summarize selected features
print(selectKbest_test_features)
print()
print(selectKbest_test_features.shape)
print('\n')

"""Step 6 Applying Feature Extraction Principal Component Analysis (PCA) Technique each on all the feature selection inorder to prepare the data"""

#Step 6a PCA on Wrapper Method Applied on the training dataset
print('PCA on Wrapper Method (Recursive Feature Elimination)')
print()
pca = PCA(n_components=6, random_state = 7) 
pca_train = pca.fit(wrapper_model_train_features) 
pca_wrapper_train_features = pca_train.transform(wrapper_model_train_features)
print(pca_wrapper_train_features)
print()
print(pca_wrapper_train_features.shape)
print('\n')

#Step 6a PCA on Wrapper Method Applied on the testing dataset
pca_test = pca.fit(wrapper_model_test_features) 
pca_wrapper_test_features = pca_test.transform(wrapper_model_test_features)
print(pca_wrapper_test_features)
print()
print(pca_wrapper_test_features.shape)
print('\n')

#Step 6b PCA on Embedded Method Applied on the training dataset
print('PCA on Embedded Method (ExtraTreesClassifier)')
print()
pca_train = pca.fit(embedded_model_train_features) 
pca_embedded_train_features = pca_train.transform(embedded_model_train_features)
print(pca_embedded_train_features)
print()
print(pca_embedded_train_features.shape)
print('\n')

#Step 6b PCA on Embedded Method Applied on the testing dataset
pca_test = pca.fit(embedded_model_test_features) 
pca_embedded_test_features = pca_test.transform(embedded_model_test_features)
print(pca_embedded_test_features)
print()
print(pca_embedded_test_features.shape)
print('\n')

#Step 6c PCA on Filter Method Applied on the training dataset
print('PCA on Filter Method (selectKBest)')
print()
pca_train = pca.fit(selectKbest_train_features) 
pca_selectKbest_train_features = pca_train.transform(selectKbest_train_features)
print(pca_selectKbest_train_features)
print()
print(pca_selectKbest_train_features.shape)
print('\n')

#Step 6c PCA on Filter Method Applied on the testing dataset
pca_test = pca.fit(selectKbest_test_features) 
pca_selectKbest_test_features = pca_test.transform(selectKbest_test_features)
print(pca_selectKbest_test_features)
print()
print(pca_selectKbest_test_features.shape)
print('\n')

"""Step 7 Class Wame is used as an optimiser inplace of stochastic gradient descent(adam)-- We will see which dataset performs best on the Neural Network that uses Wame -- This is done to hypertune the parameters for the model"""

#Step 7a using WAME
from tensorflow.keras.optimizers import Optimizer, RMSprop
from tensorflow.keras import backend as K
import numpy as np
if K.backend() == 'tensorflow':
  import tensorflow as tf

class WAME(Optimizer):#Creating a custom optimiser WAME

    def __init__(self, learning_rate=0.001, alpha_1 = 0.9, alpha_2 = 0.999,                  # this is the constructor of class WAME. It creates the optimiser.
                 epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,
                 zeta_min=1e-2, zeta_max=1e2, alpha_a=0.9,
                 **kwargs):#3 alpha
        
        super(WAME, self).__init__(**kwargs)
        self.__dict__.update(locals())
        self.iterations = K.variable(0)
        self.learning_rate = K.variable(learning_rate, name='learning_rate')
        self.alpha_1 = K.variable(alpha_1)#3 alpha
        self.alpha_2 = K.variable(alpha_2)#2
        self.alpha_a = K.variable(alpha_a)#1
        #self.zeta = K.variable(zeta)
        self.decay = K.variable(decay)
        self.eta_plus = K.variable(eta_plus)
        self.eta_minus = K.variable(eta_minus)
        self.zeta_min = zeta_min
        self.zeta_max = zeta_max
        self.inital_decay = decay
    #@K.symbolic
    def get_updates(self, params, constraints, loss): # This update method has everything betweem line 4 and 12 of the algorithm i.e the for loop given in the paper WAME
        grads = self.get_gradients(loss, params)
        self.updates = [K.update_add(self.iterations, 1)]

        lr = self.lr
        if self.inital_decay > 0:
            lr *= (1. / (1. + self.decay * self.iterations))

        t = self.iterations + 1
        lr_t = lr * K.sqrt(1. - K.pow(self.alpha_2, t)) / (1. - K.pow(self.alpha_1, t))

        shapes = [K.get_variable_shape(p) for p in params]
        prev_grads = [K.zeros(shape) for shape in shapes] 
        prev_param = [K.zeros(shape) for shape in shapes]
        ms = [K.zeros(shape) for shape in shapes]
        vs = [K.zeros(shape) for shape in shapes]
        accs = [K.ones(shape) for shape in shapes]
        acc_ms = [K.ones(shape) for shape in shapes]
        acc_vs = [K.ones(shape) for shape in shapes]
        self.weights = [self.iterations] + ms + vs


        for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,
                acc_ms, acc_vs, prev_grads, prev_param):

            change = pg * g  # This is line four and six
            change_below_zero = K.less(change,0.) #boolean true or false
            change_above_zero = K.greater(change,0.) #boolean true or false
            a_t = K.switch(
                change_below_zero, # switch statement to jump to line 6
                a * self.eta_minus, # otherwise use line 7
                K.switch(change_above_zero, a * self.eta_plus, a) # if line 4 is true go to line 5, otheriwse leave a as it was
            )
            a_clipped = K.clip(a_t, self.eta_min, self.eta_max) #end of line 5 and 7 i.e min and max value of eta
            v_t = (self.alpha_2 * v) + (1. - self.alpha_2) * K.square(g) # this is line 9
            am_t = (self.alpha_a * am) + (1. - self.alpha_a) * a_clipped  # line 10 
            a_rate = a_clipped / am_t                                     #line 11 i.e am_t == Î¸ij(t) as given in the paper
            p_t = p - lr_t * a_rate * g / (K.sqrt(v_t) + self.epsilon)    # line 12

            new_p = p_t 
            #apply constraints
            if p in constraints:
              c = constraints[p]
              new_p = c(new_p)

            self.updates.append(K.update(v, v_t))
            self.updates.append(K.update(p, new_p))
            self.updates.append(K.update(pg, p))
            self.updates.append(K.update(a, a_t))
            self.updates.append(K.update(am, am_t))
            self.updates.append(K.update(pp, p))
        return self.updates

    def get_config(self):
        config = {'lr': float(K.get_value(self.lr)),
                  'alpha_1': float(K.get_value(self.alpha_1)),
                  'alpha_2': float(K.get_value(self.alpha_2)),
                  'alpha_a': float(K.get_value(self.alpha_a)),
                  'eta_plus': float(K.get_value(self.eta_plus)),
                  'eta_minus': float(K.get_value(self.eta_minus)),
                  'zeta_min': float(self.zeta_min),
                  'zeta_max': float(self.zeta_max),
                  'epsilon': self.epsilon}
        base_config = super(WAME, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

opt = WAME(name="WAME")

"""Step 7b -- Just checking the model is able to use the WAME class"""

#pca with Wrapper dataset
#i will see which dataset gives better result on this neural network
from tensorflow.python.keras import regularizers
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
epochs = [5, 7, 10]
batches = [20, 30, 40]
#regularizers_choice =  [ regularizers.l2(0.01), regularizers.l2(0.001), regularizers.l2(0.0001), regularizers.l1(0.01), regularizers.l1(0.001), regularizers.l1(0.0001)]

#One hidden layer used on Wrapper dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  #mlp_model.add(Dense(20, activation='relu'))
  #mlp_model.add(Dense(30, activation='relu'))               #six hidden layers
  #mlp_model.add(Dense(40, activation='relu'))
  #mlp_model.add(Dense(45, activation='relu'))
  #mlp_model.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# create model
model_keras = KerasClassifier(build_fn=creating_model, verbose=0)

# grid search epochs, batch size and optimizer
param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) 
grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)
grid_result = grid.fit(pca_wrapper_train_features, Y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# pca with Embedded dataset. This dataset is chosen by K fold analysis in step 8 with 6 hidden layers
#i will see which dataset gives better result on this neural network
from tensorflow.python.keras import regularizers
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
epochs = [5, 7, 10]
batches = [20, 30, 40]
#regularizers_choice =  [ regularizers.l2(0.01), regularizers.l2(0.001), regularizers.l2(0.0001), regularizers.l1(0.01), regularizers.l1(0.001), regularizers.l1(0.0001)]

#One hidden layer used on Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  #mlp_model.add(Dense(20, activation='relu'))
  #mlp_model.add(Dense(30, activation='relu'))               #six hidden layers
  #mlp_model.add(Dense(40, activation='relu'))
  #mlp_model.add(Dense(45, activation='relu'))
  #mlp_model.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# create model
model_keras = KerasClassifier(build_fn=creating_model, verbose=0)

# grid search epochs, batch size and optimizer
param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) 
grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)
grid_result = grid.fit(pca_embedded_train_features, Y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# # pca with Filter dataset
#i will see which dataset gives better result on this neural network
from tensorflow.python.keras import regularizers
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
epochs = [5, 7, 10]
batches = [20, 30, 40]
#regularizers_choice =  [ regularizers.l2(0.01), regularizers.l2(0.001), regularizers.l2(0.0001), regularizers.l1(0.01), regularizers.l1(0.001), regularizers.l1(0.0001)]

#One hidden layer used on Filter dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  #mlp_model.add(Dense(20, activation='relu'))
  #mlp_model.add(Dense(30, activation='relu'))               #six hidden layers
  #mlp_model.add(Dense(40, activation='relu'))
  #mlp_model.add(Dense(45, activation='relu'))
  #mlp_model.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# create model
model_keras = KerasClassifier(build_fn=creating_model, verbose=0)

# grid search epochs, batch size and optimizer
param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) 
grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)
grid_result = grid.fit(pca_selectKbest_train_features, Y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

"""roughly estimate without using K fold to analyse which dataset to use for the model

pca_wrapper_train_features ---- 0.838804 

pca_embedded_train_features ----- 0.839832   ---- so i chose pca with embedded method dataset for slightly better performance

pca_selectKbest_train_features ---- 0.827233

STEP 8 KFOLD IS USED TO COMPARE WHICH MODEL FITS BEST ON WHICH DATASET
Comparing which dataset(i.e the one obtained from pca with filter, wrapper or embedded method)performs best on which model, how many hidden layers to be used.
"""

#step8a for pca_Wrapper dataset 
#checking which algorithm to use for the pca data
# Compare Algorithms

#1 hidden layer used on pca_Wrapper dataset
def creating_model1(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #one hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #2 hidden layer used on pca_Wrapper dataset
def creating_model2(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))               #two hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #3 hidden layer used on pca_Wrapper dataset
def creating_model3(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #three hidden layers
  classifier.add(Dense(20, activation='relu'))               #three hidden layers
  classifier.add(Dense(30, activation='relu'))               #three hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier


  #4 hidden layer used on pca_Wrapper dataset
def creating_model4(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #four hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #5 hidden layer used on pca_Wrapper dataset
def creating_model5(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #five hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #6 hidden layer used on pca_Wrapper dataset
def creating_model6(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# prepare models
models = []
models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))


# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_wrapper_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of HL(Hidden Layer)')
plt.ylabel('Model Accuracy')
pyplot.show()
print('\n')
print('\n')

#step8b for pca_Embedded dataset 
#checking which algorithm to use for the pca data
# Compare Algorithms

#1 hidden layer used on pca_Embedded dataset
def creating_model1(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #one hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #2 hidden layer used on pca_Embedded dataset
def creating_model2(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))              #two hidden layers
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #3 hidden layer used on pca_Embedded dataset
def creating_model3(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #three hidden layers
  classifier.add(Dense(20, activation='relu'))               #three hidden layers
  classifier.add(Dense(30, activation='relu'))               #three hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier


  #4 hidden layer used on pca_Embedded dataset
def creating_model4(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #four hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #5 hidden layer used on pca_Embedded dataset
def creating_model5(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #five hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #6 hidden layer used on pca_Embedded dataset
def creating_model6(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# prepare models
models = []
models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))


# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Model Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of HL(Hidden Layer)')
plt.ylabel('Model Accuracy')
pyplot.show()
print('\n')
print('\n')

#step8c for pca_Filter dataset 
#checking which algorithm to use for the pca data
# Compare Algorithms

#1 hidden layer used on pca_Filter dataset
def creating_model1(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #one hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #2 hidden layer used on pca_Filter dataset
def creating_model2(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))              #two hidden layers
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #3 hidden layer used on pca_Filter dataset
def creating_model3(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #three hidden layers
  classifier.add(Dense(20, activation='relu'))               #three hidden layers
  classifier.add(Dense(30, activation='relu'))               #three hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier


  #4 hidden layer used on pca_Filter dataset
def creating_model4(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #four hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #5 hidden layer used on pca_Filter dataset
def creating_model5(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #five hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #6 hidden layer used on pca_Filter dataset
def creating_model6(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# prepare models
models = []
models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))


# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_selectKbest_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Model Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of HL(Hidden Layer)')
plt.ylabel('Model Accuracy')
pyplot.show()
print('\n')
print('\n')

"""Step 9 Hypertuning the parameters Now

I chose pca_embedded dataset as it has high accuracy(around 85.0 to 85.5 % ) as shown in the graph with 6 hidden layer neural network model.
"""

# Step 9a Hypertuning the model parameter now
# pca with Embedded dataset. This dataset is chosen by K fold analysis with 6 hidden layers
#i will see which dataset gives better result on this neural network
from tensorflow.python.keras import regularizers
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
optimizers = ['WAME']#learning_rate=0.001
inits = ['uniform', 'glorot_uniform'] 
epochs = [5, 7, 10]
batches = [20, 30, 40]



#6 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# create model
model_keras = KerasClassifier(build_fn=creating_model, verbose=0)

# grid search epochs, batch size and optimizer
param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) 
grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)
grid_result = grid.fit(pca_embedded_train_features, Y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# Step 9B Hypertuning the LEARNING RATES AS WELL To see the best learning rate for the model
# pca with Embedded dataset. This dataset is chosen by K fold analysis with 6 hidden layers
#i will see which dataset gives better result on this neural network
from tensorflow.python.keras import regularizers
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
optimizers = ['WAME', 'WAME(learning_rate=0.01)', 'WAME(learning_rate=0.0001)', 'WAME(learning_rate=0.00001)'] # By default the learning rate is 0.001 in class WAME
inits = ['uniform', 'glorot_uniform'] 
epochs = [5, 7, 10]
batches = [20, 30, 40]



#6 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# create model
model_keras = KerasClassifier(build_fn=creating_model, verbose=0)

# grid search epochs, batch size and optimizer
param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) 
grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)
grid_result = grid.fit(pca_embedded_train_features, Y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

"""Step 10 Evaluating and Testing the result on Testing dataset now"""

# the accuracy for the default learning rates 0.001 is 83%
#STEP 10 Testing 


#6 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('ANN WITH 6 HIDDEN LAYERS', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))


for name, model in models:
  print('\n')
  print("Model : ", name)
  model_build_time = time.time()
  model.fit(pca_embedded_train_features, Y_train)
  print('\n')
  print("Time to build ANN Model in seconds : %.4f " % round(time.time()-model_build_time,4))
  print('\n')
  start_time = time.time()
  y_pred = model.predict(pca_embedded_test_features)
  print("Time to test the ANN Model in seconds : %.4f " % round(time.time()-start_time,4))
  print('\n')
  matrix = confusion_matrix(Y_test, y_pred)
  print("Time elapsed in seconds: %.4f " % round(time.time()-model_build_time, 4))
  print('\n')
  print(matrix)
  print('\n')
  report = classification_report(Y_test, y_pred)
  print(report)
  print('\n')

# the accuracy for the best hyperparameters with best learning rates 0.0001 is 83%. Hence when using the best hyperparameter the accuracy for the model is increased to 72% from 69%
#STEP 10 Testing 
#aCCURACY IS 83%

#6 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('ANN WITH 6 HIDDEN LAYERS', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


for name, model in models:
  print('\n')
  print("Model : ", name)
  model_build_time = time.time()
  model.fit(pca_embedded_train_features, Y_train)
  print('\n')
  print("Time to build ANN Model in seconds : %.4f " % round(time.time()-model_build_time,4))
  print('\n')
  start_time = time.time()
  y_pred = model.predict(pca_embedded_test_features)
  print("Time to test the ANN Model in seconds : %.4f " % round(time.time()-start_time,4))
  print('\n')
  matrix = confusion_matrix(Y_test, y_pred)
  print("Time elapsed in seconds: %.4f " % round(time.time()-model_build_time, 4))
  print('\n')
  print(matrix)
  print('\n')
  report = classification_report(Y_test, y_pred)
  print(report)
  print('\n')

"""Step 11 Some of the graphs to analyse"""

#1 Graph of No of hidden layers vs Accuracy ans MSE of the Model
#hidden layer used on pca_Embedded dataset
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
def creating_model1(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #one hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #2 hidden layer used on pca_Embedded dataset
def creating_model2(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))              #two hidden layers
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #3 hidden layer used on pca_Embedded dataset
def creating_model3(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #three hidden layers
  classifier.add(Dense(20, activation='relu'))               #three hidden layers
  classifier.add(Dense(30, activation='relu'))               #three hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier


  #4 hidden layer used on pca_Embedded dataset
def creating_model4(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #four hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #5 hidden layer used on pca_Embedded dataset
def creating_model5(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #five hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #6 hidden layer used on pca_Embedded dataset
def creating_model6(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# prepare models
models = []
models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))


# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Accuracy Rate')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of Hidden Layer(HL)')
plt.ylabel('Generalization Accuracy')
pyplot.show()
print('\n')
print('\n')

#https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/
#Graphs of Mean Square Error
# def calc_train_error(pca_embedded_train_features, Y_train, model):
#     '''returns in-sample error for already fit model.'''
#     predictions = model.predict(pca_embedded_train_features)
#     mse = mean_squared_error(Y_train, predictions)
#     rmse = np.sqrt(mse)
#     return mse
    
# def calc_validation_error(pca_embedded_test_features, Y_test, model):
#     '''returns out-of-sample error for already fit model.'''
#     predictions = models.predict(pca_embedded_test_features)
#     mse = mean_squared_error(Y_test, predictions)
#     rmse = np.sqrt(mse)
#     return mse
    
# def calc_metrics(pca_embedded_train_features, Y_train, pca_embedded_test_features, Y_test, model):
#     '''fits model and returns the RMSE for in-sample error and out-of-sample error'''
#     model.fit(pca_embedded_train_features, Y_train)
#     train_error = calc_train_error(pca_embedded_train_features, Y_train, model)
#     validation_error = calc_validation_error(pca_embedded_test_features, Y_test, model)
#     return train_error, validation_error



# # evaluate each model in turn
# results = []
# names = []
# scoring = calc_validation_error(pca_embedded_test_features, Y_test, models)
# for name, model in models:
#   kfold = KFold(n_splits=10, random_state=7,shuffle=True)
#   cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
#   results.append(cv_results)
#   names.append(name)
#   msg = "%s: %f (%f)" % (name, cv_results.calc_validation_error())
#   print(msg)
# # boxplot algorithm comparison
# fig = pyplot.figure()
# fig.suptitle('Mean Squared Error')
# ax = fig.add_subplot(111)
# pyplot.boxplot(results)
# ax.set_xticklabels(names)
# plt.xlabel('No. of Hidden Layer(HL)')
# plt.ylabel('Generalization Error Rate')
# pyplot.show()
# print('\n')
# print('\n')

#to take out mean square error
def creating_model1(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #one hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #2 hidden layer used on pca_Embedded dataset
def creating_model2(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))              #two hidden layers
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #3 hidden layer used on pca_Embedded dataset
def creating_model3(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #three hidden layers
  classifier.add(Dense(20, activation='relu'))               #three hidden layers
  classifier.add(Dense(30, activation='relu'))               #three hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier


  #4 hidden layer used on pca_Embedded dataset
def creating_model4(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #four hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #5 hidden layer used on pca_Embedded dataset
def creating_model5(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #five hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #6 hidden layer used on pca_Embedded dataset
def creating_model6(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model

# prepare models
models = []
models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))

results = []
names=[]
for name, model in models:
   kfold = KFold(n_splits=10, random_state=7,shuffle=True)
   cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold)
   results.append(cv_results)
   model.fit(pca_embedded_train_features, Y_train)
   y_pred = model.predict(pca_embedded_test_features)
   #predictions = models.predict(pca_embedded_test_features)
   mse = mean_squared_error(Y_test,  y_pred)
   results.append(mse)
   names.append(name)
   msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
   print(msg)
  # msg = "%s: " % (name)
  # print(msg)

# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Squared Error')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of Hidden Layer(HL)')
plt.ylabel('Generalization Error Rate')
pyplot.show()
print('\n')
print('\n')

#2 Graph of No of Nodes vs Accuracy ans MSE of the Model
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
def creating_model1(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #one hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #2 hidden layer used on pca_Embedded dataset
def creating_model2(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))              #two hidden layers
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #3 hidden layer used on pca_Embedded dataset
def creating_model3(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu'))               #three hidden layers
  classifier.add(Dense(20, activation='relu'))               #three hidden layers
  classifier.add(Dense(30, activation='relu'))               #three hidden layers
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier


  #4 hidden layer used on pca_Embedded dataset
def creating_model4(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #four hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #5 hidden layer used on pca_Embedded dataset
def creating_model5(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #five hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  #6 hidden layer used on pca_Embedded dataset
def creating_model6(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# prepare models
models = []
models.append(('10', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('30', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('60', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('100', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('145', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('195', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))


# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Accuracy Rate')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of Nodes')
plt.ylabel('Generalization Accuracy')
pyplot.show()
print('\n')
print('\n')
# again evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  model.fit(pca_embedded_train_features, Y_train)
  y_pred = model.predict(pca_embedded_test_features)
  mse = mean_squared_error(Y_test,  y_pred)
  results.append(mse)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Squarred Error')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of Nodes')
plt.ylabel('Generalization Error Rate')
pyplot.show()

#3 Graph of No of Epochs for our model  to see Accuracy and MSE of the Model
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 


  #6 hidden layer used on pca_Embedded dataset
def creating_model6(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# prepare models, as this model6 is chosen now we will analyse the effect of epochs increase to this model with same batch size
models = []
models.append(('6', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 6, init = 'glorot_uniform', optimizer='WAME')))
models.append(('8', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 8, init = 'glorot_uniform', optimizer='WAME')))
models.append(('10', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))
models.append(('12', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 12, init = 'glorot_uniform', optimizer='WAME')))
models.append(('14', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 14, init = 'glorot_uniform', optimizer='WAME')))
models.append(('16', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 16, init = 'glorot_uniform', optimizer='WAME')))


# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Accuracy Rate')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of Epochs')
plt.ylabel('Generalization Accuracy')
pyplot.show()
print('\n')
print('\n')
# again evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  model.fit(pca_embedded_train_features, Y_train)
  y_pred = model.predict(pca_embedded_test_features)
  mse = mean_squared_error(Y_test,  y_pred)
  results.append(mse)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Squarred Error')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('No. of Epochs')
plt.ylabel('Generalization Error Rate')
pyplot.show()

#4 Graph for learning rates for our  model to see Accuracy and MSE of the Model
# optimizers = ['WAME']
# inits = ['uniform', 'glorot_uniform'] 
#optimizers = ['WAME', 'WAME(learning_rate=0.01)', 'WAME(learning_rate=0.0001)', 'WAME(learning_rate=0.00001)']

  #6 hidden layer used on pca_Embedded dataset
def creating_model6(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

# prepare models, as this model6 is chosen now we will analyse the effect of epochs increase to this model with same batch size
models = []
models.append(('0.00001', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME(learning_rate=0.00001)')))
models.append(('0.0001', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME(learning_rate=0.0001)')))
models.append(('0.001', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME(learning_rate=0.001)')))
models.append(('0.01', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME(learning_rate=0.01)')))



# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Accuracy Rate')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('Learning Rate')
plt.ylabel('Generalization Accuracy')
pyplot.show()
print('\n')
print('\n')
# again evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  model.fit(pca_embedded_train_features, Y_train)
  y_pred = model.predict(pca_embedded_test_features)
  mse = mean_squared_error(Y_test,  y_pred)
  results.append(mse)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Squarred Error')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('Learning Rate')
plt.ylabel('Generalization Error Rate')
pyplot.show()

#5 Graph for Activation for our  model to see Accuracy and MSE of the Model

#6 hidden layer used on pca_Embedded dataset
def creating_model1(optimizer=optimizers, init=inits):# input and hidden uses relu and output layer uses sigmoid
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  def creating_model2(optimizer=optimizers, init=inits):
    classifier = Sequential() #Sequential module Initialises the ANN
    classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
    classifier.add(Dense(10, activation='relu')) 
    classifier.add(Dense(20, activation='relu'))
    classifier.add(Dense(30, activation='relu'))               #six hidden layers
    classifier.add(Dense(40, activation='relu'))
    classifier.add(Dense(45, activation='relu'))
    classifier.add(Dense(50, activation='relu'))
    classifier.add(Dense(1, activation='relu'))             #one output layer
  # Compile model
    classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
    return classifier

def creating_model3(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='sigmoid')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='sigmoid')) 
  classifier.add(Dense(20, activation='sigmoid'))
  classifier.add(Dense(30, activation='sigmoid'))               #six hidden layers
  classifier.add(Dense(40, activation='sigmoid'))
  classifier.add(Dense(45, activation='sigmoid'))
  classifier.add(Dense(50, activation='sigmoid'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier


  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('relu/sigmoid', KerasClassifier(build_fn=creating_model1, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))
models.append(('relu', KerasClassifier(build_fn=creating_model2, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))
models.append(('sigmoid', KerasClassifier(build_fn=creating_model3, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Accuracy Rate')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('Activation Function')
plt.ylabel('Generalization Accuracy')
pyplot.show()
print('\n')
print('\n')
# again evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
  results.append(cv_results)
  model.fit(pca_embedded_train_features, Y_train)
  y_pred = model.predict(pca_embedded_test_features)
  mse = mean_squared_error(Y_test,  y_pred)
  results.append(mse)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  print(msg)
# boxplot algorithm comparison
fig = pyplot.figure()
fig.suptitle('Mean Squarred Error')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
plt.xlabel('Activation Function')
plt.ylabel('Generalization Error Rate')
pyplot.show()

"""Analysing the f1 score for 6 hidden layers is better as compared to the rest.
That is why I chose 6 hidden layers for the model **bold text**
"""

#Analysing the f1 score as per the number of hidden layers
#f1 score for one hidden layer
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
#1 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  #classifier.add(Dense(20, activation='relu'))
  #classifier.add(Dense(30, activation='relu'))               #one hidden layers
  #classifier.add(Dense(40, activation='relu'))
  #classifier.add(Dense(45, activation='relu'))
  #classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


for name, model in models:
  print('\n')
  print("Model : ", name)
  model_build_time = time.time()
  model.fit(pca_embedded_train_features, Y_train)
  print('\n')
  print("Time to build ANN Model in seconds : %.4f " % round(time.time()-model_build_time,4))
  print('\n')
  start_time = time.time()
  y_pred = model.predict(pca_embedded_test_features)
  print("Time to test the ANN Model in seconds : %.4f " % round(time.time()-start_time,4))
  print('\n')
  matrix = confusion_matrix(Y_test, y_pred)
  print("Time elapsed in seconds: %.4f " % round(time.time()-model_build_time, 4))
  print('\n')
  print(matrix)
  print('\n')
  report = classification_report(Y_test, y_pred)
  print(report)
  print('\n')

#Analysing the f1 score as per the number of hidden layers
#f1 score for two hidden layer
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
#2 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  #classifier.add(Dense(30, activation='relu'))               #two hidden layers
  #classifier.add(Dense(40, activation='relu'))
  #classifier.add(Dense(45, activation='relu'))
  #classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


for name, model in models:
  print('\n')
  print("Model : ", name)
  model_build_time = time.time()
  model.fit(pca_embedded_train_features, Y_train)
  print('\n')
  print("Time to build ANN Model in seconds : %.4f " % round(time.time()-model_build_time,4))
  print('\n')
  start_time = time.time()
  y_pred = model.predict(pca_embedded_test_features)
  print("Time to test the ANN Model in seconds : %.4f " % round(time.time()-start_time,4))
  print('\n')
  matrix = confusion_matrix(Y_test, y_pred)
  print("Time elapsed in seconds: %.4f " % round(time.time()-model_build_time, 4))
  print('\n')
  print(matrix)
  print('\n')
  report = classification_report(Y_test, y_pred)
  print(report)
  print('\n')

#Analysing the f1 score as per the number of hidden layers
#f1 score for three hidden layer
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
#3 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #three hidden layers
  #classifier.add(Dense(40, activation='relu'))
  #classifier.add(Dense(45, activation='relu'))
  #classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


for name, model in models:
  print('\n')
  print("Model : ", name)
  model_build_time = time.time()
  model.fit(pca_embedded_train_features, Y_train)
  print('\n')
  print("Time to build ANN Model in seconds : %.4f " % round(time.time()-model_build_time,4))
  print('\n')
  start_time = time.time()
  y_pred = model.predict(pca_embedded_test_features)
  print("Time to test the ANN Model in seconds : %.4f " % round(time.time()-start_time,4))
  print('\n')
  matrix = confusion_matrix(Y_test, y_pred)
  print("Time elapsed in seconds: %.4f " % round(time.time()-model_build_time, 4))
  print('\n')
  print(matrix)
  print('\n')
  report = classification_report(Y_test, y_pred)
  print(report)
  print('\n')

#Analysing the f1 score as per the number of hidden layers
#f1 score for 4 hidden layer
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
#4 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #four hidden layers
  classifier.add(Dense(40, activation='relu'))
  #classifier.add(Dense(45, activation='relu'))
  #classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


for name, model in models:
  print('\n')
  print("Model : ", name)
  model_build_time = time.time()
  model.fit(pca_embedded_train_features, Y_train)
  print('\n')
  print("Time to build ANN Model in seconds : %.4f " % round(time.time()-model_build_time,4))
  print('\n')
  start_time = time.time()
  y_pred = model.predict(pca_embedded_test_features)
  print("Time to test the ANN Model in seconds : %.4f " % round(time.time()-start_time,4))
  print('\n')
  matrix = confusion_matrix(Y_test, y_pred)
  print("Time elapsed in seconds: %.4f " % round(time.time()-model_build_time, 4))
  print('\n')
  print(matrix)
  print('\n')
  report = classification_report(Y_test, y_pred)
  print(report)
  print('\n')

#Analysing the f1 score as per the number of hidden layers
#f1 score for five hidden layer
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
#5 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #five hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  #classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


for name, model in models:
  print('\n')
  print("Model : ", name)
  model_build_time = time.time()
  model.fit(pca_embedded_train_features, Y_train)
  print('\n')
  print("Time to build ANN Model in seconds : %.4f " % round(time.time()-model_build_time,4))
  print('\n')
  start_time = time.time()
  y_pred = model.predict(pca_embedded_test_features)
  print("Time to test the ANN Model in seconds : %.4f " % round(time.time()-start_time,4))
  print('\n')
  matrix = confusion_matrix(Y_test, y_pred)
  print("Time elapsed in seconds: %.4f " % round(time.time()-model_build_time, 4))
  print('\n')
  print(matrix)
  print('\n')
  report = classification_report(Y_test, y_pred)
  print(report)
  print('\n')

# #Analysing the f1 score for 6 hidden layers have already done in the evaluation section also and in this section as well which is better as compared to the rest.
#That is why I chose 6 hidden layers for the model
#f1 score for six hidden layer
optimizers = ['WAME']
inits = ['uniform', 'glorot_uniform'] 
#6 hidden layer used on pca_Embedded dataset
def creating_model(optimizer=optimizers, init=inits):
  # this functions creates the model 
  classifier = Sequential() #Sequential module Initialises the ANN
  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
  classifier.add(Dense(10, activation='relu')) 
  classifier.add(Dense(20, activation='relu'))
  classifier.add(Dense(30, activation='relu'))               #six hidden layers
  classifier.add(Dense(40, activation='relu'))
  classifier.add(Dense(45, activation='relu'))
  classifier.add(Dense(50, activation='relu'))
  classifier.add(Dense(1, activation='sigmoid'))             #one output layer
  # Compile model
  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
  return classifier

  # Compile model
#using the best hyperparameter for the model
models = []
models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


for name, model in models:
  print('\n')
  print("Model : ", name)
  model_build_time = time.time()
  model.fit(pca_embedded_train_features, Y_train)
  print('\n')
  print("Time to build ANN Model in seconds : %.4f " % round(time.time()-model_build_time,4))
  print('\n')
  start_time = time.time()
  y_pred = model.predict(pca_embedded_test_features)
  print("Time to test the ANN Model in seconds : %.4f " % round(time.time()-start_time,4))
  print('\n')
  matrix = confusion_matrix(Y_test, y_pred)
  print("Time elapsed in seconds: %.4f " % round(time.time()-model_build_time, 4))
  print('\n')
  print(matrix)
  print('\n')
  report = classification_report(Y_test, y_pred)
  print(report)
  print('\n')

# #5 Practice to improve the graph of MSE Graph x axis to align for Activation for our  model to see Accuracy and MSE of the Model

# #6 hidden layer used on pca_Embedded dataset
# def creating_model1(optimizer=optimizers, init=inits):# input and hidden uses relu and output layer uses sigmoid
#   # this functions creates the model 
#   classifier = Sequential() #Sequential module Initialises the ANN
#   classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
#   classifier.add(Dense(10, activation='relu')) 
#   classifier.add(Dense(20, activation='relu'))
#   classifier.add(Dense(30, activation='relu'))               #six hidden layers
#   classifier.add(Dense(40, activation='relu'))
#   classifier.add(Dense(45, activation='relu'))
#   classifier.add(Dense(50, activation='relu'))
#   classifier.add(Dense(1, activation='sigmoid'))             #one output layer
#   # Compile model
#   classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
#   return classifier

#   def creating_model2(optimizer=optimizers, init=inits):
#     classifier = Sequential() #Sequential module Initialises the ANN
#     classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN
#     classifier.add(Dense(10, activation='relu')) 
#     classifier.add(Dense(20, activation='relu'))
#     classifier.add(Dense(30, activation='relu'))               #six hidden layers
#     classifier.add(Dense(40, activation='relu'))
#     classifier.add(Dense(45, activation='relu'))
#     classifier.add(Dense(50, activation='relu'))
#     classifier.add(Dense(1, activation='relu'))             #one output layer
#   # Compile model
#     classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
#     return classifier

# def creating_model3(optimizer=optimizers, init=inits):
#   # this functions creates the model 
#   classifier = Sequential() #Sequential module Initialises the ANN
#   classifier.add(Dense(12, input_dim=6, activation='sigmoid')) # one input layer. Dense module builds the layer for ANN
#   classifier.add(Dense(10, activation='sigmoid')) 
#   classifier.add(Dense(20, activation='sigmoid'))
#   classifier.add(Dense(30, activation='sigmoid'))               #six hidden layers
#   classifier.add(Dense(40, activation='sigmoid'))
#   classifier.add(Dense(45, activation='sigmoid'))
#   classifier.add(Dense(50, activation='sigmoid'))
#   classifier.add(Dense(1, activation='sigmoid'))             #one output layer
#   # Compile model
#   classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=["accuracy"])
#   return classifier


#   # Compile model
# #using the best hyperparameter for the model
# models = []
# models.append(('relu/sigmoid', KerasClassifier(build_fn=creating_model1, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))
# models.append(('relu', KerasClassifier(build_fn=creating_model2, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))
# models.append(('sigmoid', KerasClassifier(build_fn=creating_model3, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))


# # evaluate each model in turn
# results = []
# names = []
# scoring = 'accuracy'
# for name, model in models:
#   kfold = KFold(n_splits=10, random_state=7,shuffle=True)
#   cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)
#   results.append(cv_results)
#   names.append(name)
#   msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
#   print(msg)
# # boxplot algorithm comparison
# fig = pyplot.figure()
# fig.suptitle('Mean Accuracy Rate')
# ax = fig.add_subplot(111)
# pyplot.boxplot(results)
# ax.set_xticklabels(names)
# plt.xlabel('Activation Function')
# plt.ylabel('Generalization Accuracy')
# pyplot.show()
# print('\n')
# print('\n')
# # again evaluate each model in turn
# results = []
# names = []
# scoring = 'accuracy'
# for name, model in models:
#   kfold = KFold(n_splits=10, random_state=7,shuffle=True)
#   cv_results = cross_val_score(model, pca_embedded_train_features, cv=kfold, scoring=scoring)
#   #results.append(cv_results)
#   model= tf.stack(model.fit(cv_results, Y_train))
#   #model.fit(pca_embedded_train_features, Y_train)
#   y_pred = model.predict(pca_embedded_test_features)
#   mse = mean_squared_error(Y_test,  y_pred)
#   results.append(mse)
#   names.append(name)
#   msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
#   print(msg)
# # boxplot algorithm comparison
# fig = pyplot.figure()
# fig.suptitle('Mean Squarred Error')
# ax = fig.add_subplot(111)
# pyplot.boxplot(results)
# ax.set_xticklabels(names)
# plt.xlabel('Activation Function')
# plt.ylabel('Generalization Error Rate')
# pyplot.show()

#Not needed lamba function to convert the target variable 'class'. As label encoder already assigns more than 50K as 1 and less than 50K as 0 and ignores the dot.
# #Step1c converting the target variable class into 2 groups of zeros and ones. More than 50K as 1 and less than 50 K as 0
# dataset['class'] = dataset['class'].apply(lambda x: str(x).replace('<=50K.', '0') if '<=50K.' in str(x) else str(x))
# dataset['class'] = dataset['class'].apply(lambda x: str(x).replace('<=50K', '0') if '<=50K' in str(x) else str(x))
# dataset['class'] = dataset['class'].apply(lambda x: str(x).replace('>50K.', '1') if '>50K.' in str(x) else str(x))
# dataset['class'] = dataset['class'].apply(lambda x: str(x).replace('>50K', '1') if '>50K' in str(x) else str(x))
# dataset['class'] = dataset['class'].apply(lambda x: int(x))
# dataset
#No need to drop anything now
# #Step1d assigning the target variable to Y and then dropping it from training data
# Y_train = dataset['class']
# dataset.drop('class', axis = 1, inplace = True)
# #Step 1e SELECTING ALL ROWS AND COLUMNS IN X and putting it in Z dataset to visualise
# X_train = dataset.iloc[:, :].values
# Z_train = pd.DataFrame(X_train)
# X_train
# #Step 1f --to do label encoding
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# labelencoder_X = LabelEncoder()
# #for i in range(0, 13):
#  # if isinstance(X[:, i], str):
# X_train[:,1] = labelencoder_X.fit_transform(X_train[:,1])
# X_train[:,3] = labelencoder_X.fit_transform(X_train[:,3])
# X_train[:,5] = labelencoder_X.fit_transform(X_train[:,5])
# X_train[:,6] = labelencoder_X.fit_transform(X_train[:,6])
# X_train[:,7] = labelencoder_X.fit_transform(X_train[:,7])
# X_train[:,8] = labelencoder_X.fit_transform(X_train[:,8])
# X_train[:,9] = labelencoder_X.fit_transform(X_train[:,9])
# X_train[:,13] = labelencoder_X.fit_transform(X_train[:,13])
# Z_traindata = pd.DataFrame(X_train)
# Z_traindata#this is the tests data ready for scaling
##-------------------------
#Step 1g the categorical columns are convered to 0s and 1s by using one hot encoder to the 8 categorical columns
# #onehotvector gives a feature for every categorical option. No Need to do Feature extraction and Feature selection when using one hot encoder but Scaling is a must.
# #onehotencoder = OneHotEncoder(categories = [1])
# from sklearn.compose import ColumnTransformer

# ct = ColumnTransformer(
#     [('oh_enc', OneHotEncoder(sparse=False), [1, 3, 5, 6, 7, 8, 9, 13]),],  # the column numbers I want to apply this to
#     remainder='passthrough'  # This leaves the rest of my columns in place
# )
# X_traindata = ct.fit_transform(X_train)
# X_traindata

# #print(ct.fit_transform(X)) # Notice the output is a string
# Z_train = pd.DataFrame(X_traindata)
# Z_train

# #step 1 h
# #this is just to test step 1 h if this works that means after applying the one hot encoding on the categorical variable we then need to scale all the features to prepare it for the model so that ML algorithm can be applied to it
# #Now Scaling the categorical and the continuous variables by using MinMaxScalar
# from sklearn.preprocessing import MinMaxScaler
# from numpy import set_printoptions
# #rescaled data between 0 and 1
# scaler = MinMaxScaler(feature_range=(0, 1))
# rescaledX = scaler.fit_transform(Z_train)
# #summarize transformed data
# set_printoptions(precision=3)
# print(rescaledX)
# M = pd.DataFrame(rescaledX) #M shows that the dataset has been scaled
# M

# #so rescaledX is the final training dataset that I will use

# #Step 1i -- check to see which dataset works Note this dosent work

# # Now dropping the the last six columns from X_traindata as these last 6 columns are continuos data that needs to be scaled so I stored it first in L to scale them and then will combine with X again
# #so dropping the unscaled last 6 col from X
# #dataset.drop('class', axis = 1, inplace = True)
# #all rows, all columns except the last six
# L = X_traindata[:, -6 :]# so L has all the 6 continuous variables
# O = pd.DataFrame(L)# to visualise L that the 6 continuos feature are assigned to L 
# O

# #Now dropping the 6 continuous variable from Ztrain --- Note this dosent work

# #X_traindata.drop[:, -6 :]
# Z_train2 = Z_train.drop([102,103,104,105,106,107], axis=1)

# Z_train2

# #Step 1 j --- Note this dosent work

# #Now Scaling the continuous variables L by using MinMaxScalar
# from sklearn.preprocessing import MinMaxScaler
# from numpy import set_printoptions
# #rescaled data between 0 and 1
# scaler = MinMaxScaler(feature_range=(0, 1))
# rescaledX = scaler.fit_transform(L)
# #summarize transformed data
# set_printoptions(precision=3)
# print(rescaledX)
# M = pd.DataFrame(rescaledX) #M shows that L(6 continuous variables)has been scaled
# M

# #Step 1 k Combine the onehotvector Z_train2(8 categorical features) with the rescaledX(6 continuous features) and get the combined dataset for the model
#  #--Note this dosent work

# combined_dataset = np.hstack((Z_train2 , rescaledX))
# combined_dataset.shape # combined_dataset is the final dataset for the train data but fit is failing for this type of dataset
# CD = pd.DataFrame(combined_dataset)
# CD

# #Buiding neural network
# optimizers = ['WAME1()', 'WAME2()', 'WAME3()', 'WAME4()']#SO WAME IS THE BEST OPTIMISER
# inits = ['uniform', 'glorot_uniform'] 
# epochs = [5, 7, 10]
# batches = [20, 30, 40]


# def create_model(optimizer=optimizers, init=inits):
#   # create model
#   mlp_model = Sequential()
#   mlp_model.add(Dense(12, input_dim=108, activation='relu')) 
#   mlp_model.add(Dense(10, activation='relu')) 
#   mlp_model.add(Dense(20, activation='relu'))
#   mlp_model.add(Dense(30, activation='relu'))
#   mlp_model.add(Dense(1, activation='sigmoid'))

 
#   mlp_model.compile(loss=tf.keras.losses.MeanSquaredError(),  metrics=["accuracy"])
#   return mlp_model

# # create model

# model_keras = KerasClassifier(build_fn=create_model, verbose=0)

# # grid search epochs, batch size and optimizer
# param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits) 
# grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)
# grid_result = grid.fit(Z_train, Y_train)# so i decided after app lying the one hot encoder, do the rescaling of all 108 features and then see if its work
# #print("Best: %f using %s" % (grid_result.bestscore, grid_result.bestparams))
# print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))#checking the best hyperparameter along with the best optimiser

# #Step 3 part 2 hypertuning the model to see which parameter are best for the neural network model. It took 48 minutes to execute 

# from keras.layers import Dense, Dropout,Flatten
# from keras.layers.convolutional import Conv2D, MaxPooling2D
# from keras.models import Sequential
# from tensorflow.keras.models import Sequential
# from keras.layers import Dropout
# from keras.layers.core import Activation
# from keras.optimizers import Optimizer
# from sklearn.preprocessing import LabelBinarizer
# from keras.utils import to_categorical 
# from keras import backend as K
# import keras
# from sklearn.model_selection import train_test_split
# import keras
# from keras.models import Sequential
# from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
# from sklearn.preprocessing import Normalizer
# from numpy import reshape
# from tensorflow.keras import backend
# from keras.models import Model
# from keras.layers import Dense, Input
# from keras.regularizers import l1 
# from keras.optimizers import Adam
# from keras.losses import kullback_leibler_divergence
# from keras.losses import mean_squared_error
# from keras.models import Sequential 
# from keras.layers import Dropout
# from keras.wrappers.scikit_learn import KerasClassifier 
# from sklearn.model_selection import GridSearchCV
# from sklearn.model_selection import RandomizedSearchCV
# from keras.losses import MeanSquaredError
# import tensorflow as tf
# from tensorflow.keras.optimizers import Optimizer, RMSprop

# from keras.wrappers.scikit_learn import KerasClassifier
# from sklearn.model_selection import GridSearchCV
# #Buiding neural network
# optimizers = ['WAME1()', 'WAME2()', 'WAME3()', 'WAME4()']#SO WAME IS THE BEST OPTIMISER
# inits = ['uniform', 'glorot_uniform'] 
# epochs = [5, 7, 10]
# batches = [20, 30, 40]


# def create_model(optimizer=optimizers, init=inits):
#   # create model
#   mlp_model = Sequential()
#   mlp_model.add(Dense(12, input_dim=108, activation='relu')) 
#   mlp_model.add(Dense(10, activation='relu')) 
#   mlp_model.add(Dense(20, activation='relu'))
#   mlp_model.add(Dense(30, activation='relu'))
#   mlp_model.add(Dense(1, activation='sigmoid'))

 
#   mlp_model.compile(loss=tf.keras.losses.MeanSquaredError(),  metrics=["accuracy"])
#   return mlp_model

# # create model

# model_keras = KerasClassifier(build_fn=create_model, verbose=0)

# # grid search epochs, batch size and optimizer
# param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits) 
# grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)
# grid_result = grid.fit(rescaledX, Y_train)# so i decided after app lying the one hot encoder, do the rescaling of all 108 features and then see if its work
# #print("Best: %f using %s" % (grid_result.bestscore, grid_result.bestparams))
# print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))#checking the best hyperparameter along with the best optimiser

# #Step 3 part2 to visualise the graph of WAME 1 accuracy but there is a problem
# from keras.layers import Dense, Dropout,Flatten
# from keras.layers.convolutional import Conv2D, MaxPooling2D
# from keras.models import Sequential
# from tensorflow.keras.models import Sequential
# from keras.layers import Dropout
# from keras.layers.core import Activation
# from keras.optimizers import Optimizer
# from sklearn.preprocessing import LabelBinarizer
# from keras.utils import to_categorical 
# from keras import backend as K
# import keras
# from sklearn.model_selection import train_test_split
# import keras
# from keras.models import Sequential
# from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
# from sklearn.preprocessing import Normalizer
# from numpy import reshape
# from tensorflow.keras import backend
# from keras.models import Model
# from keras.layers import Dense, Input
# from keras.regularizers import l1 
# from keras.optimizers import Adam
# from keras.losses import kullback_leibler_divergence
# from keras.losses import mean_squared_error
# from keras.models import Sequential 
# from keras.layers import Dropout
# from keras.wrappers.scikit_learn import KerasClassifier 
# from sklearn.model_selection import GridSearchCV
# from sklearn.model_selection import RandomizedSearchCV
# from keras.losses import MeanSquaredError
# import tensorflow as tf
# from tensorflow.keras.optimizers import Optimizer, RMSprop

# from keras.wrappers.scikit_learn import KerasClassifier
# from sklearn.model_selection import GridSearchCV
# #Buiding neural network
# optimizers = ['WAME1()']#SO WAME IS THE BEST OPTIMISER
# inits = ['uniform', 'glorot_uniform'] 
# epochs = [5, 7, 10]
# batches = [20, 30, 40]


# def create_model(optimizer=optimizers, init=inits):
#   # create model
#   mlp_model = Sequential()
#   mlp_model.add(Dense(12, input_dim=108, activation='relu')) 
#   mlp_model.add(Dense(10, activation='relu')) 
#   mlp_model.add(Dense(20, activation='relu'))
#   mlp_model.add(Dense(30, activation='relu'))
#   mlp_model.add(Dense(1, activation='sigmoid'))

 
#   mlp_model.compile(loss=tf.keras.losses.MeanSquaredError(),  metrics=["accuracy"])
#   return mlp_model

# # create model

# model_keras = KerasClassifier(build_fn=create_model, verbose=0)

# # grid search epochs, batch size and optimizer
# param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits) 
# grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)
# grid_result = grid.fit(rescaledX, Y_train)# so i decided after app lying the one hot encoder, do the rescaling of all 108 features and then see if its work
# #print("Best: %f using %s" % (grid_result.bestscore, grid_result.bestparams))
# print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
# print()

# plt.plot(grid.grid['accuracy'])
# plt.plot(grid.grid['val_accuracy'])
# plt.title("Accuracy of WAME1")
# plt.xlabel('WAME1')
# plt.ylabel('accuracy')
# plt.legend(['train','test'])

# plt.show()

# #Step 4a preparing the test data
# import pandas as pd
# names = ('age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','class')
# dataset_test = pd.read_csv(filename2, names=names)
# dataset_test

# #Step 4d to visualise B_testdata to see the target variable is dropped
# B_testdata

# #Step 4e SELECTING ALL ROWS AND COLUMNS IN X and putting it in Z dataset to visualise
# X_testdata = B_testdata.iloc[:, :].values
# Z_testdata = pd.DataFrame(X_testdata)
# Z_testdata

# #Step 4g the categorical columns are convered to 0s and 1s by using one hot encoder to the 8 categorical columns
# #onehotvector gives a feature for every categorical option. No Need to do Feature extraction and Feature selection when using one hot encoder but Scaling is a must.
# #onehotencoder = OneHotEncoder(categories = [1])
# from sklearn.compose import ColumnTransformer

# ct = ColumnTransformer(
#     [('oh_enc', OneHotEncoder(sparse=False), [1, 3, 5, 6, 7, 8, 9, 13]),],  # the column numbers I want to apply this to
#     remainder='passthrough'  # This leaves the rest of my columns in place
# )
# X_testdata = ct.fit_transform(X_testdata)
# X_testdata

# #print(ct.fit_transform(X)) # Notice the output is a string
# Z_test = pd.DataFrame(X_testdata)
# Z_test# No idea why the columns is 107 instead of 108

# #step 4 h
# #this is just to test step 1 h if this works that means after applying the one hot encoding on the categorical variable we then need to scale all the features to prepare it for the model so that ML algorithm can be applied to it
# #Now Scaling the categorical and the continuous variables by using MinMaxScalar
# from sklearn.preprocessing import MinMaxScaler
# from numpy import set_printoptions
# #rescaled data between 0 and 1
# scaler = MinMaxScaler(feature_range=(0, 1))
# rescaledY = scaler.fit_transform(Z_test)
# #summarize transformed data
# set_printoptions(precision=3)
# print(rescaledY)
# M = pd.DataFrame(rescaledY) #M shows that the dataset has been scaled
# M

# #so rescaledX is the final training dataset that I will use

# #STEP 8 FirstTesting WITHOUT USING wame Best: 0.852185 using {'batch_size': 30, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME1()'}
# from keras.models import Sequential

# from keras.layers import Dense

# inits = ['uniform'] 
# epochs = [10]
# batches = [30]
# #the SEQUENTIAL MODULE IS REQUIRED TO INITIALIZE THE ANN, AND DENSE MODULE IS REQUIRED TO BUILD THE LAYERS OF OUR ANN
#   # create model
# classifier = Sequential()#initialise the ANN by creating an instance of Sequential
# classifier.add(Dense(12, input_dim=14, activation='relu')) 
# classifier.add(Dense(10, activation='relu')) 
# classifier.add(Dense(20, activation='relu'))
# classifier.add(Dense(30, activation='relu'))
# classifier.add(Dense(1, activation='sigmoid'))



# opt = keras.optimizers.Adam(learning_rate=0.0001)

# classifier.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(),  metrics=["accuracy"])

# classifier.fit(rescaledX, Y_train, batch_size=20, epochs=10) #init = 'global_uniform')
# # create model
# y_pred = classifier.predict(rescaledY)
# y_pred = (y_pred > 0.5)
  
# from sklearn.metrics import confusion_matrix#, classification_report
# #cm = confusion_matrix(Y_test, y_pred)

# #model = LogisticRegression(solver='liblinear') model.fit(X_train, Y_train)
# #predicted = model.predict(X_test)
# report = classification_report(Y_test, y_pred) 
# print(report)

# #predicted = model.predict(X_test)
# matrix = confusion_matrix(Y_test, y_pred) 
# print(matrix)

