{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled28.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB-d4OE2NnHr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from pandas import read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.layers import Dense, Dropout,Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.core import Activation\n",
        "from keras.optimizers import Optimizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.utils import to_categorical \n",
        "from keras import backend as K\n",
        "import keras\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import set_printoptions\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE , chi2\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from numpy import set_printoptions\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import math\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w48uQyYcNy3m"
      },
      "source": [
        "**Data Preparation**\n",
        "\n",
        "1. Step 1 \n",
        "One flaw in the dataset I noticed is that, it contains question marks. So I  tried to remove this.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo23NU2pCs0v"
      },
      "source": [
        "#Step1a Preparing the training data\n",
        "#https://www.geeksforgeeks.org/use-of-na_values-parameter-in-read_csv-function-of-pandas-in-python/\n",
        "sample1 = 'adult.data'\n",
        "sample2 = 'adult.test'\n",
        "names = ('age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','class')\n",
        "#data_train = read_csv(filename1, names=names, index_col=0)\n",
        "#data_test = read_csv(filename2, names=names)\n",
        "question_mark_traindata = pd.read_csv(sample1,names= names, na_values=[' ?','?'])\n",
        "question_mark_testdata = pd.read_csv(sample2, names= names, na_values=[' ?','?'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "aLNMsW3lQQg2",
        "outputId": "05c332d9-acc5-46a1-ab70-f6655f01601d"
      },
      "source": [
        "# To visualise the train data that has question marks in the categorical columns. Hence there are 32561 rows and 15 columns\n",
        "#question_mark_traindata.shape\n",
        "question_mark_traindata\n",
        "#question_mark_traindata.describe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>27</td>\n",
              "      <td>Private</td>\n",
              "      <td>257302</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Tech-support</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>40</td>\n",
              "      <td>Private</td>\n",
              "      <td>154374</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>58</td>\n",
              "      <td>Private</td>\n",
              "      <td>151910</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>22</td>\n",
              "      <td>Private</td>\n",
              "      <td>201490</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>52</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>287927</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>15024</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age          workclass  fnlwgt  ... hours-per-week  native-country   class\n",
              "0       39          State-gov   77516  ...             40   United-States   <=50K\n",
              "1       50   Self-emp-not-inc   83311  ...             13   United-States   <=50K\n",
              "2       38            Private  215646  ...             40   United-States   <=50K\n",
              "3       53            Private  234721  ...             40   United-States   <=50K\n",
              "4       28            Private  338409  ...             40            Cuba   <=50K\n",
              "...    ...                ...     ...  ...            ...             ...     ...\n",
              "32556   27            Private  257302  ...             38   United-States   <=50K\n",
              "32557   40            Private  154374  ...             40   United-States    >50K\n",
              "32558   58            Private  151910  ...             40   United-States   <=50K\n",
              "32559   22            Private  201490  ...             20   United-States   <=50K\n",
              "32560   52       Self-emp-inc  287927  ...             40   United-States    >50K\n",
              "\n",
              "[32561 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "uzzbJGNYQ1Pl",
        "outputId": "91cba2e7-a159-47c6-b4de-f7fecbffa40e"
      },
      "source": [
        "# To visualise the test data that has question marks in the categorical columns. Hence there are 16282 rows and 15 columns\n",
        "#question_mark_testdata.shape\n",
        "question_mark_testdata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>|1x3 Cross validator</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "      <td>Private</td>\n",
              "      <td>226802.0</td>\n",
              "      <td>11th</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>89814.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>336951.0</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>160323.0</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>39</td>\n",
              "      <td>Private</td>\n",
              "      <td>215419.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16278</th>\n",
              "      <td>64</td>\n",
              "      <td>NaN</td>\n",
              "      <td>321403.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Other-relative</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>374983.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>83891.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Asian-Pac-Islander</td>\n",
              "      <td>Male</td>\n",
              "      <td>5455.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>35</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>182148.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16282 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        age      workclass  ...  native-country    class\n",
              "0      |1x3 Cross validator            NaN  ...             NaN      NaN\n",
              "1                        25        Private  ...   United-States   <=50K.\n",
              "2                        38        Private  ...   United-States   <=50K.\n",
              "3                        28      Local-gov  ...   United-States    >50K.\n",
              "4                        44        Private  ...   United-States    >50K.\n",
              "...                     ...            ...  ...             ...      ...\n",
              "16277                    39        Private  ...   United-States   <=50K.\n",
              "16278                    64            NaN  ...   United-States   <=50K.\n",
              "16279                    38        Private  ...   United-States   <=50K.\n",
              "16280                    44        Private  ...   United-States   <=50K.\n",
              "16281                    35   Self-emp-inc  ...   United-States    >50K.\n",
              "\n",
              "[16282 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "B_lpmKwOc6AL",
        "outputId": "085f6d63-c307-44e6-b2fa-74664c5c8893"
      },
      "source": [
        "# So dropping rows which has question_mark in the train dataset. We can see below that from 32561 rows, we have 30162 rows = 2399 rows with question marks are dropped \n",
        "train_data = question_mark_traindata.dropna()\n",
        "train_data # train data is now cleaned"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>27</td>\n",
              "      <td>Private</td>\n",
              "      <td>257302</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Tech-support</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>40</td>\n",
              "      <td>Private</td>\n",
              "      <td>154374</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>58</td>\n",
              "      <td>Private</td>\n",
              "      <td>151910</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>22</td>\n",
              "      <td>Private</td>\n",
              "      <td>201490</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>52</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>287927</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>15024</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30162 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age          workclass  fnlwgt  ... hours-per-week  native-country   class\n",
              "0       39          State-gov   77516  ...             40   United-States   <=50K\n",
              "1       50   Self-emp-not-inc   83311  ...             13   United-States   <=50K\n",
              "2       38            Private  215646  ...             40   United-States   <=50K\n",
              "3       53            Private  234721  ...             40   United-States   <=50K\n",
              "4       28            Private  338409  ...             40            Cuba   <=50K\n",
              "...    ...                ...     ...  ...            ...             ...     ...\n",
              "32556   27            Private  257302  ...             38   United-States   <=50K\n",
              "32557   40            Private  154374  ...             40   United-States    >50K\n",
              "32558   58            Private  151910  ...             40   United-States   <=50K\n",
              "32559   22            Private  201490  ...             20   United-States   <=50K\n",
              "32560   52       Self-emp-inc  287927  ...             40   United-States    >50K\n",
              "\n",
              "[30162 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "mU-n3FrneUlq",
        "outputId": "756e0076-a607-4438-ab57-876864b8a7e4"
      },
      "source": [
        "# So dropping rows which has question_mark in the test dataset. We can see below that from 16282 rows, we have 15060 rows = 1222 rows with question marks are dropped \n",
        "test_data = question_mark_testdata.dropna()\n",
        "test_data # test data is now cleaned"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "      <td>Private</td>\n",
              "      <td>226802.0</td>\n",
              "      <td>11th</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>89814.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>336951.0</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>160323.0</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>34</td>\n",
              "      <td>Private</td>\n",
              "      <td>198693.0</td>\n",
              "      <td>10th</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Other-service</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>33</td>\n",
              "      <td>Private</td>\n",
              "      <td>245211.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>39</td>\n",
              "      <td>Private</td>\n",
              "      <td>215419.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>374983.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>83891.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Asian-Pac-Islander</td>\n",
              "      <td>Male</td>\n",
              "      <td>5455.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>35</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>182148.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15060 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age      workclass    fnlwgt  ... hours-per-week  native-country    class\n",
              "1      25        Private  226802.0  ...           40.0   United-States   <=50K.\n",
              "2      38        Private   89814.0  ...           50.0   United-States   <=50K.\n",
              "3      28      Local-gov  336951.0  ...           40.0   United-States    >50K.\n",
              "4      44        Private  160323.0  ...           40.0   United-States    >50K.\n",
              "6      34        Private  198693.0  ...           30.0   United-States   <=50K.\n",
              "...    ..            ...       ...  ...            ...             ...      ...\n",
              "16276  33        Private  245211.0  ...           40.0   United-States   <=50K.\n",
              "16277  39        Private  215419.0  ...           36.0   United-States   <=50K.\n",
              "16279  38        Private  374983.0  ...           50.0   United-States   <=50K.\n",
              "16280  44        Private   83891.0  ...           40.0   United-States   <=50K.\n",
              "16281  35   Self-emp-inc  182148.0  ...           60.0   United-States    >50K.\n",
              "\n",
              "[15060 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtAalzwtlHZa"
      },
      "source": [
        "Step 2 -- Using Label Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "5LhDJ7JPC28U",
        "outputId": "ab9ad447-ed0c-4f9d-88fc-f5cfb9b5ccc0"
      },
      "source": [
        "#Step1b Now using Label Encoder on train dataset so that the categorical variables are converted into continuous variables. Label Encoder compares the data and assigns labels according to the comparision made.\n",
        "labelencoder_X = LabelEncoder()\n",
        "train_data1 = train_data.apply(labelencoder_X.fit_transform)\n",
        "train_data1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>5</td>\n",
              "      <td>2491</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33</td>\n",
              "      <td>4</td>\n",
              "      <td>2727</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>13188</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>14354</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>18120</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>15471</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>7555</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "      <td>7377</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>12060</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>16689</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30162 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  workclass  fnlwgt  ...  hours-per-week  native-country  class\n",
              "0       22          5    2491  ...              39              38      0\n",
              "1       33          4    2727  ...              12              38      0\n",
              "2       21          2   13188  ...              39              38      0\n",
              "3       36          2   14354  ...              39              38      0\n",
              "4       11          2   18120  ...              39               4      0\n",
              "...    ...        ...     ...  ...             ...             ...    ...\n",
              "32556   10          2   15471  ...              37              38      0\n",
              "32557   23          2    7555  ...              39              38      1\n",
              "32558   41          2    7377  ...              39              38      0\n",
              "32559    5          2   12060  ...              19              38      0\n",
              "32560   35          3   16689  ...              39              38      1\n",
              "\n",
              "[30162 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "lX-oBHLqlepD",
        "outputId": "4b07c65a-e5af-40eb-a18b-b0fdbbffcb45"
      },
      "source": [
        "#Now using Label Encoder on test dataset so that the categorical variables are converted into continuous variables. Label Encoder compares the data and assigns labels according to the comparision made.\n",
        "labelencoder_Y = LabelEncoder()\n",
        "test_data1 = test_data.apply(labelencoder_Y.fit_transform)\n",
        "test_data1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8315</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>1754</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>10750</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>4780</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>7091</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>8927</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>7893</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>11193</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>1593</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>6062</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15060 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  workclass  fnlwgt  ...  hours-per-week  native-country  class\n",
              "1        8          2    8315  ...              39              37      0\n",
              "2       21          2    1754  ...              49              37      0\n",
              "3       11          1   10750  ...              39              37      1\n",
              "4       27          2    4780  ...              39              37      1\n",
              "6       17          2    7091  ...              29              37      0\n",
              "...    ...        ...     ...  ...             ...             ...    ...\n",
              "16276   16          2    8927  ...              39              37      0\n",
              "16277   22          2    7893  ...              35              37      0\n",
              "16279   21          2   11193  ...              49              37      0\n",
              "16280   27          2    1593  ...              39              37      0\n",
              "16281   18          3    6062  ...              59              37      1\n",
              "\n",
              "[15060 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHGM6dTjnO9E"
      },
      "source": [
        "Step 3- Using One hot Encoding. Concluded not to use at too many features and will lead to the dimensionality problem.\n",
        "https://stats.stackexchange.com/questions/371750/using-categorical-feature-as-both-a-continuous-feature-and-also-doing-one-hot-e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "kDooWPi8nkRs",
        "outputId": "2dc47c5a-4d4a-4402-da3c-8d4f3cd385ee"
      },
      "source": [
        "# #Step 3 the columns are convered to 0s and 1s by using one hot encoder. With One hot encoding we cannot remove any features. So no filter, Wrapper or Embedded method needed. Also no need to even Scale the data.\n",
        "# #onehotvector gives a feature for every categorical option. No Need to do Feature extraction and Feature selection when using one hot encoder.\n",
        "# array = train_data1.values #first get the train data into arrays\n",
        "# X_train = array[:, 0:14] #all rows of all columns except the last column which is a target variable\n",
        "# Y_train = array[:, 14] # the last columns\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "# ct = ColumnTransformer(\n",
        "#     [('oh_enc', OneHotEncoder(sparse=False), [1, 3, 5, 6, 7, 8, 9, 13]),],  # the column numbers I want to apply this to\n",
        "#     remainder='passthrough'  # This leaves the rest of my columns in place\n",
        "# )\n",
        "# X_traindata = ct.fit_transform(X_train)\n",
        "# X_traindata\n",
        "\n",
        "# #print(ct.fit_transform(X)) # Notice the output is a string\n",
        "# Z_train = pd.DataFrame(X_traindata)\n",
        "# Z_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2491.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>2727.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>13188.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>14354.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>18120.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30157</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15471.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30158</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>7555.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30159</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>7377.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30160</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>12060.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30161</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>16689.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30162 rows × 104 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2    3    4    5    ...   98       99    100    101  102   103\n",
              "0      0.0  0.0  0.0  0.0  0.0  1.0  ...  22.0   2491.0  12.0   24.0  0.0  39.0\n",
              "1      0.0  0.0  0.0  0.0  1.0  0.0  ...  33.0   2727.0  12.0    0.0  0.0  12.0\n",
              "2      0.0  0.0  1.0  0.0  0.0  0.0  ...  21.0  13188.0   8.0    0.0  0.0  39.0\n",
              "3      0.0  0.0  1.0  0.0  0.0  0.0  ...  36.0  14354.0   6.0    0.0  0.0  39.0\n",
              "4      0.0  0.0  1.0  0.0  0.0  0.0  ...  11.0  18120.0  12.0    0.0  0.0  39.0\n",
              "...    ...  ...  ...  ...  ...  ...  ...   ...      ...   ...    ...  ...   ...\n",
              "30157  0.0  0.0  1.0  0.0  0.0  0.0  ...  10.0  15471.0  11.0    0.0  0.0  37.0\n",
              "30158  0.0  0.0  1.0  0.0  0.0  0.0  ...  23.0   7555.0   8.0    0.0  0.0  39.0\n",
              "30159  0.0  0.0  1.0  0.0  0.0  0.0  ...  41.0   7377.0   8.0    0.0  0.0  39.0\n",
              "30160  0.0  0.0  1.0  0.0  0.0  0.0  ...   5.0  12060.0   8.0    0.0  0.0  19.0\n",
              "30161  0.0  0.0  0.0  1.0  0.0  0.0  ...  35.0  16689.0   8.0  107.0  0.0  39.0\n",
              "\n",
              "[30162 rows x 104 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDLeuLeRvpbD"
      },
      "source": [
        "Step 4 Using Minmax Scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "aBud63L8vmf7",
        "outputId": "c8e7b47d-767f-49e8-f7e7-24ce711a5c45"
      },
      "source": [
        "#Step 4a MinmaxScalar on Training dataset\n",
        "X_train = train_data1.values[:, 0:14] #all rows of all columns except the last column which is a target variable\n",
        "Y_train = train_data1.values[:, 14]\n",
        "#rescaled data between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaledX = scaler.fit_transform(X_train)\n",
        "#summarize transformed data\n",
        "set_printoptions(precision=3)\n",
        "print(rescaledX)\n",
        "M = pd.DataFrame(rescaledX) #M shows that train data has been scaled\n",
        "M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.31  0.833 0.123 ... 0.    0.419 0.95 ]\n",
            " [0.465 0.667 0.135 ... 0.    0.129 0.95 ]\n",
            " [0.296 0.333 0.651 ... 0.    0.419 0.95 ]\n",
            " ...\n",
            " [0.577 0.333 0.364 ... 0.    0.419 0.95 ]\n",
            " [0.07  0.333 0.595 ... 0.    0.204 0.95 ]\n",
            " [0.493 0.5   0.824 ... 0.    0.419 0.95 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.309859</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.122939</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.205128</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.464789</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.134587</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.295775</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.650874</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.507042</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.708420</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.154930</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.894285</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30157</th>\n",
              "      <td>0.140845</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.763548</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397849</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30158</th>\n",
              "      <td>0.323944</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.372865</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30159</th>\n",
              "      <td>0.577465</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.364081</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30160</th>\n",
              "      <td>0.070423</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.595203</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.204301</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30161</th>\n",
              "      <td>0.492958</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.823660</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.914530</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30162 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2         3   ...        10   11        12    13\n",
              "0      0.309859  0.833333  0.122939  0.600000  ...  0.205128  0.0  0.419355  0.95\n",
              "1      0.464789  0.666667  0.134587  0.600000  ...  0.000000  0.0  0.129032  0.95\n",
              "2      0.295775  0.333333  0.650874  0.733333  ...  0.000000  0.0  0.419355  0.95\n",
              "3      0.507042  0.333333  0.708420  0.066667  ...  0.000000  0.0  0.419355  0.95\n",
              "4      0.154930  0.333333  0.894285  0.600000  ...  0.000000  0.0  0.419355  0.10\n",
              "...         ...       ...       ...       ...  ...       ...  ...       ...   ...\n",
              "30157  0.140845  0.333333  0.763548  0.466667  ...  0.000000  0.0  0.397849  0.95\n",
              "30158  0.323944  0.333333  0.372865  0.733333  ...  0.000000  0.0  0.419355  0.95\n",
              "30159  0.577465  0.333333  0.364081  0.733333  ...  0.000000  0.0  0.419355  0.95\n",
              "30160  0.070423  0.333333  0.595203  0.733333  ...  0.000000  0.0  0.204301  0.95\n",
              "30161  0.492958  0.500000  0.823660  0.733333  ...  0.914530  0.0  0.419355  0.95\n",
              "\n",
              "[30162 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "HRR3iW9GwrdT",
        "outputId": "d4fdeb49-8b7e-4f90-9d26-5c7f582e3cb9"
      },
      "source": [
        "#Step 4b MinmaxScalar on Testing dataset\n",
        "X_test = test_data1.values[:, 0:14] #all rows of all columns except the last column which is a target variable\n",
        "Y_test = test_data1.values[:, 14]\n",
        "#rescaled data between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaledY = scaler.fit_transform(X_test)\n",
        "#summarize transformed data\n",
        "set_printoptions(precision=3)\n",
        "print(rescaledY)\n",
        "N = pd.DataFrame(rescaledY) #M shows that test data has been scaled\n",
        "N"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.111 0.333 0.698 ... 0.    0.443 0.949]\n",
            " [0.292 0.333 0.147 ... 0.    0.557 0.949]\n",
            " [0.153 0.167 0.902 ... 0.    0.443 0.949]\n",
            " ...\n",
            " [0.292 0.333 0.94  ... 0.    0.557 0.949]\n",
            " [0.375 0.333 0.134 ... 0.    0.443 0.949]\n",
            " [0.25  0.5   0.509 ... 0.    0.67  0.949]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.698036</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.291667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.147246</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.556818</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.152778</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.902451</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.401276</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.798165</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.236111</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.595282</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.329545</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15055</th>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.749412</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15056</th>\n",
              "      <td>0.305556</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.662609</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15057</th>\n",
              "      <td>0.291667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.939641</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.556818</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15058</th>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.133731</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.669725</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15059</th>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.508899</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.670455</td>\n",
              "      <td>0.948718</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15060 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2   ...   11        12        13\n",
              "0      0.111111  0.333333  0.698036  ...  0.0  0.443182  0.948718\n",
              "1      0.291667  0.333333  0.147246  ...  0.0  0.556818  0.948718\n",
              "2      0.152778  0.166667  0.902451  ...  0.0  0.443182  0.948718\n",
              "3      0.375000  0.333333  0.401276  ...  0.0  0.443182  0.948718\n",
              "4      0.236111  0.333333  0.595282  ...  0.0  0.329545  0.948718\n",
              "...         ...       ...       ...  ...  ...       ...       ...\n",
              "15055  0.222222  0.333333  0.749412  ...  0.0  0.443182  0.948718\n",
              "15056  0.305556  0.333333  0.662609  ...  0.0  0.397727  0.948718\n",
              "15057  0.291667  0.333333  0.939641  ...  0.0  0.556818  0.948718\n",
              "15058  0.375000  0.333333  0.133731  ...  0.0  0.443182  0.948718\n",
              "15059  0.250000  0.500000  0.508899  ...  0.0  0.670455  0.948718\n",
              "\n",
              "[15060 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnn71hds0ffH"
      },
      "source": [
        "Step 5 Using the Feature Selection(FS) Methods\n",
        "Step 5a FS - Wraper Method - RFE i.e Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pFw88P0xs_X",
        "outputId": "a837f44c-2a1c-4b4f-94e8-23a83d910bc3"
      },
      "source": [
        "#Step 5a Wrapper Method Applied on the training dataset\n",
        "print('Recursive Feature Elimination')\n",
        "print()\n",
        "wrapper_model = LogisticRegression(solver='liblinear') \n",
        "wrapper_model = RFE(wrapper_model, 10) \n",
        "wrapper_model_train = wrapper_model.fit(rescaledX,Y_train)\n",
        "wrapper_model_train_features = wrapper_model_train.transform(rescaledX)\n",
        "print(wrapper_model_train_features)\n",
        "print()\n",
        "print(wrapper_model_train_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recursive Feature Elimination\n",
            "\n",
            "[[0.31  0.833 0.8   ... 0.205 0.    0.419]\n",
            " [0.465 0.667 0.8   ... 0.    0.    0.129]\n",
            " [0.296 0.333 0.533 ... 0.    0.    0.419]\n",
            " ...\n",
            " [0.577 0.333 0.533 ... 0.    0.    0.419]\n",
            " [0.07  0.333 0.533 ... 0.    0.    0.204]\n",
            " [0.493 0.5   0.533 ... 0.915 0.    0.419]]\n",
            "\n",
            "(30162, 10)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abji6m4H34ua",
        "outputId": "efa29c66-b1e7-496f-a858-9b1dfea17ad5"
      },
      "source": [
        "#Step 5a Wrapper Method Applied on the testing dataset\n",
        "print('Recursive Feature Elimination')\n",
        "print()\n",
        "wrapper_model = LogisticRegression(solver='liblinear') \n",
        "wrapper_model = RFE(wrapper_model, 10) \n",
        "wrapper_model_test = wrapper_model.fit(rescaledY,Y_test)\n",
        "wrapper_model_test_features = wrapper_model_test.transform(rescaledY)\n",
        "print(wrapper_model_test_features)\n",
        "print()\n",
        "print(wrapper_model_test_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recursive Feature Elimination\n",
            "\n",
            "[[0.111 0.333 0.4   ... 0.    0.    0.443]\n",
            " [0.292 0.333 0.533 ... 0.    0.    0.557]\n",
            " [0.153 0.167 0.733 ... 0.    0.    0.443]\n",
            " ...\n",
            " [0.292 0.333 0.8   ... 0.    0.    0.557]\n",
            " [0.375 0.333 0.8   ... 0.67  0.    0.443]\n",
            " [0.25  0.5   0.8   ... 0.    0.    0.67 ]]\n",
            "\n",
            "(15060, 10)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5NF8u6W6jk7"
      },
      "source": [
        "Step 5b Using Feature Selection - Embedded Method i.e ExtraTreesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTEUTgEq61xw",
        "outputId": "aa8e49d9-565b-4367-db8a-4ed540afab47"
      },
      "source": [
        "#Step 5b Embedded Method Applied on the training dataset\n",
        "print('ExtraTreesClassifier')\n",
        "print()\n",
        "embedded_model = ExtraTreesClassifier(n_estimators=10)\n",
        "embedded_model_train = embedded_model.fit(rescaledX,Y_train)\n",
        "clf_emb_model = SelectFromModel(embedded_model_train , prefit=True)\n",
        "embedded_model_train_features = clf_emb_model.transform(rescaledX)\n",
        "print(embedded_model_train_features)\n",
        "print()\n",
        "print(embedded_model_train_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ExtraTreesClassifier\n",
            "\n",
            "[[0.31  0.123 0.8   ... 0.2   0.205 0.419]\n",
            " [0.465 0.135 0.8   ... 0.    0.    0.129]\n",
            " [0.296 0.651 0.533 ... 0.2   0.    0.419]\n",
            " ...\n",
            " [0.577 0.364 0.533 ... 0.8   0.    0.419]\n",
            " [0.07  0.595 0.533 ... 0.6   0.    0.204]\n",
            " [0.493 0.824 0.533 ... 1.    0.915 0.419]]\n",
            "\n",
            "(30162, 7)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-yR-VPL9sKp",
        "outputId": "799d0e39-4820-4a3b-b3ec-d3b601dff62d"
      },
      "source": [
        "#Step 5b Embedded Method Applied on the testing dataset\n",
        "print('ExtraTreesClassifier')\n",
        "print()\n",
        "embedded_model = ExtraTreesClassifier(n_estimators=10)\n",
        "embedded_model_test = embedded_model.fit(rescaledY,Y_test)\n",
        "clf_emb_model = SelectFromModel(embedded_model_test , prefit=True)\n",
        "embedded_model_test_features = clf_emb_model.transform(rescaledY)\n",
        "print(embedded_model_test_features)\n",
        "print()\n",
        "print(embedded_model_test_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ExtraTreesClassifier\n",
            "\n",
            "[[0.111 0.698 0.4   ... 0.462 0.    0.443]\n",
            " [0.292 0.147 0.533 ... 0.308 0.    0.557]\n",
            " [0.153 0.902 0.733 ... 0.769 0.    0.443]\n",
            " ...\n",
            " [0.292 0.94  0.8   ... 0.692 0.    0.557]\n",
            " [0.375 0.134 0.8   ... 0.    0.67  0.443]\n",
            " [0.25  0.509 0.8   ... 0.231 0.    0.67 ]]\n",
            "\n",
            "(15060, 7)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFvJZVeq_Yrt"
      },
      "source": [
        "Step 5c Using Filter Method - SelectKbest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgSeSFqs_Xpy",
        "outputId": "d4b5f4ec-c105-4ebe-ca48-6b9088f7f15c"
      },
      "source": [
        "#Step 5c Filter Method Applied on the training dataset\n",
        "print('selectKBest')\n",
        "print()\n",
        "selectKbest_model = SelectKBest(score_func=chi2, k=10)\n",
        "fs = selectKbest_model.fit(rescaledX, Y_train)\n",
        "# summarize scores\n",
        "set_printoptions(precision=3)\n",
        "#print(fit.score_)\n",
        "selectKbest_train_features = fs.transform(rescaledX)\n",
        "# summarize selected features\n",
        "print(selectKbest_train_features)\n",
        "print()\n",
        "print(selectKbest_train_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "selectKBest\n",
            "\n",
            "[[0.31  0.6   0.8   ... 0.205 0.    0.419]\n",
            " [0.465 0.6   0.8   ... 0.    0.    0.129]\n",
            " [0.296 0.733 0.533 ... 0.    0.    0.419]\n",
            " ...\n",
            " [0.577 0.733 0.533 ... 0.    0.    0.419]\n",
            " [0.07  0.733 0.533 ... 0.    0.    0.204]\n",
            " [0.493 0.733 0.533 ... 0.915 0.    0.419]]\n",
            "\n",
            "(30162, 10)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHLQ5hN3DZ-9",
        "outputId": "777e9f0c-4b6f-49ee-84e6-a25f7b4feaec"
      },
      "source": [
        "#Step 5c Filter Method Applied on the testing dataset\n",
        "print('selectKBest')\n",
        "print()\n",
        "selectKbest_model = SelectKBest(score_func=chi2, k=10)\n",
        "fs = selectKbest_model.fit(rescaledY, Y_test)\n",
        "# summarize scores\n",
        "set_printoptions(precision=3)\n",
        "#print(fit.score_)\n",
        "selectKbest_test_features = fs.transform(rescaledY)\n",
        "# summarize selected features\n",
        "print(selectKbest_test_features)\n",
        "print()\n",
        "print(selectKbest_test_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "selectKBest\n",
            "\n",
            "[[0.111 0.067 0.4   ... 0.    0.    0.443]\n",
            " [0.292 0.733 0.533 ... 0.    0.    0.557]\n",
            " [0.153 0.467 0.733 ... 0.    0.    0.443]\n",
            " ...\n",
            " [0.292 0.6   0.8   ... 0.    0.    0.557]\n",
            " [0.375 0.6   0.8   ... 0.67  0.    0.443]\n",
            " [0.25  0.6   0.8   ... 0.    0.    0.67 ]]\n",
            "\n",
            "(15060, 10)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn4S0raxD8nr"
      },
      "source": [
        "Step 6 Applying Feature Extraction Principal Component Analysis (PCA) Technique each on all the feature selection inorder to prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE-HX5FBD3uP",
        "outputId": "cdd48719-0dc1-456b-935a-afa60bdc4e8b"
      },
      "source": [
        "#Step 6a PCA on Wrapper Method Applied on the training dataset\n",
        "print('PCA on Wrapper Method (Recursive Feature Elimination)')\n",
        "print()\n",
        "pca = PCA(n_components=6, random_state = 7) \n",
        "pca_train = pca.fit(wrapper_model_train_features) \n",
        "pca_wrapper_train_features = pca_train.transform(wrapper_model_train_features)\n",
        "print(pca_wrapper_train_features)\n",
        "print()\n",
        "print(pca_wrapper_train_features.shape)\n",
        "print('\\n')\n",
        "\n",
        "#Step 6a PCA on Wrapper Method Applied on the testing dataset\n",
        "pca_test = pca.fit(wrapper_model_test_features) \n",
        "pca_wrapper_test_features = pca_test.transform(wrapper_model_test_features)\n",
        "print(pca_wrapper_test_features)\n",
        "print()\n",
        "print(pca_wrapper_test_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA on Wrapper Method (Recursive Feature Elimination)\n",
            "\n",
            "[[-0.324 -0.174 -0.125 -0.1    0.26   0.051]\n",
            " [-0.427  0.135 -0.064 -0.049 -0.031  0.061]\n",
            " [-0.361  0.211  0.312 -0.064 -0.145 -0.041]\n",
            " ...\n",
            " [ 0.87  -0.288 -0.21  -0.107  0.097  0.377]\n",
            " [-0.077 -0.53   0.19  -0.156  0.01  -0.002]\n",
            " [ 0.86   0.222  0.327 -0.105  0.853  0.331]]\n",
            "\n",
            "(30162, 6)\n",
            "\n",
            "\n",
            "[[-0.07  -0.575  0.238  0.341 -0.037  0.038]\n",
            " [-0.432  0.067 -0.03  -0.052 -0.115 -0.032]\n",
            " [-0.413  0.025 -0.036 -0.06  -0.072 -0.276]\n",
            " ...\n",
            " [-0.435  0.101 -0.049 -0.051 -0.027 -0.174]\n",
            " [-0.174  0.113  0.613  0.692  0.594 -0.076]\n",
            " [-0.445  0.105 -0.05  -0.067 -0.014 -0.171]]\n",
            "\n",
            "(15060, 6)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXcAul-HHjj7",
        "outputId": "5e938d1b-b998-4205-96c8-e0c9d59b26b5"
      },
      "source": [
        "#Step 6b PCA on Embedded Method Applied on the training dataset\n",
        "print('PCA on Embedded Method (ExtraTreesClassifier)')\n",
        "print()\n",
        "pca_train = pca.fit(embedded_model_train_features) \n",
        "pca_embedded_train_features = pca_train.transform(embedded_model_train_features)\n",
        "print(pca_embedded_train_features)\n",
        "print()\n",
        "print(pca_embedded_train_features.shape)\n",
        "print('\\n')\n",
        "\n",
        "#Step 6b PCA on Embedded Method Applied on the testing dataset\n",
        "pca_test = pca.fit(embedded_model_test_features) \n",
        "pca_embedded_test_features = pca_test.transform(embedded_model_test_features)\n",
        "print(pca_embedded_test_features)\n",
        "print()\n",
        "print(pca_embedded_test_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA on Embedded Method (ExtraTreesClassifier)\n",
            "\n",
            "[[ 0.045 -0.47  -0.35   0.174 -0.134 -0.099]\n",
            " [-0.193 -0.325 -0.335 -0.035  0.041 -0.155]\n",
            " [-0.024 -0.09   0.178 -0.081  0.024  0.039]\n",
            " ...\n",
            " [ 0.588 -0.273 -0.166  0.096  0.332 -0.083]\n",
            " [ 0.538 -0.275  0.114 -0.073 -0.131  0.074]\n",
            " [ 0.623  0.017  0.266  0.943  0.172  0.282]]\n",
            "\n",
            "(30162, 6)\n",
            "\n",
            "\n",
            "[[-0.03  -0.298 -0.232 -0.073  0.008  0.165]\n",
            " [-0.142  0.355 -0.021 -0.095 -0.023  0.046]\n",
            " [ 0.296 -0.403  0.159 -0.064 -0.179 -0.047]\n",
            " ...\n",
            " [ 0.226 -0.406  0.235 -0.006 -0.105 -0.175]\n",
            " [-0.405  0.527  0.422  0.502 -0.191  0.156]\n",
            " [-0.213  0.026  0.114  0.01  -0.183 -0.181]]\n",
            "\n",
            "(15060, 6)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvbod5pCLgaG",
        "outputId": "9fa2daf4-070d-4241-f7f1-d5751df1ed0f"
      },
      "source": [
        "#Step 6c PCA on Filter Method Applied on the training dataset\n",
        "print('PCA on Filter Method (selectKBest)')\n",
        "print()\n",
        "pca_train = pca.fit(selectKbest_train_features) \n",
        "pca_selectKbest_train_features = pca_train.transform(selectKbest_train_features)\n",
        "print(pca_selectKbest_train_features)\n",
        "print()\n",
        "print(pca_selectKbest_train_features.shape)\n",
        "print('\\n')\n",
        "\n",
        "#Step 6c PCA on Filter Method Applied on the testing dataset\n",
        "pca_test = pca.fit(selectKbest_test_features) \n",
        "pca_selectKbest_test_features = pca_test.transform(selectKbest_test_features)\n",
        "print(pca_selectKbest_test_features)\n",
        "print()\n",
        "print(pca_selectKbest_test_features.shape)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA on Filter Method (selectKBest)\n",
            "\n",
            "[[-0.281 -0.429  0.204 -0.147 -0.126  0.241]\n",
            " [-0.401 -0.244 -0.036  0.103 -0.077 -0.036]\n",
            " [-0.353 -0.137 -0.147  0.151  0.301 -0.141]\n",
            " ...\n",
            " [ 0.9   -0.372  0.259 -0.222 -0.213  0.126]\n",
            " [-0.047 -0.426  0.448 -0.36   0.191  0.005]\n",
            " [ 0.882 -0.197 -0.166  0.082  0.343  0.883]]\n",
            "\n",
            "(30162, 6)\n",
            "\n",
            "\n",
            "[[-0.099 -0.008  0.805  0.156  0.182  0.075]\n",
            " [-0.421 -0.179 -0.065  0.028 -0.037 -0.107]\n",
            " [-0.429  0.302  0.087  0.137 -0.03  -0.078]\n",
            " ...\n",
            " [-0.443  0.225 -0.058  0.069 -0.037 -0.058]\n",
            " [-0.189 -0.463 -0.153  0.121  0.554  0.644]\n",
            " [-0.428 -0.234 -0.062  0.058 -0.052 -0.044]]\n",
            "\n",
            "(15060, 6)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDWooQWVO_6k"
      },
      "source": [
        "Step 7 Class Wame is used as an optimiser inplace of stochastic gradient descent(adam)-- We will see which dataset performs best on the Neural Network that uses Wame -- This is done to hypertune the parameters for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRPIYWBeO_Gk"
      },
      "source": [
        "#Step 7a using WAME\n",
        "from tensorflow.keras.optimizers import Optimizer, RMSprop\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "if K.backend() == 'tensorflow':\n",
        "  import tensorflow as tf\n",
        "\n",
        "class WAME(Optimizer):#Creating a custom optimiser WAME\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, alpha_1 = 0.9, alpha_2 = 0.999,                  # this is the constructor of class WAME. It creates the optimiser.\n",
        "                 epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                 zeta_min=1e-2, zeta_max=1e2, alpha_a=0.9,\n",
        "                 **kwargs):#3 alpha\n",
        "        \n",
        "        super(WAME, self).__init__(**kwargs)\n",
        "        self.__dict__.update(locals())\n",
        "        self.iterations = K.variable(0)\n",
        "        self.learning_rate = K.variable(learning_rate, name='learning_rate')\n",
        "        self.alpha_1 = K.variable(alpha_1)#3 alpha\n",
        "        self.alpha_2 = K.variable(alpha_2)#2\n",
        "        self.alpha_a = K.variable(alpha_a)#1\n",
        "        #self.zeta = K.variable(zeta)\n",
        "        self.decay = K.variable(decay)\n",
        "        self.eta_plus = K.variable(eta_plus)\n",
        "        self.eta_minus = K.variable(eta_minus)\n",
        "        self.zeta_min = zeta_min\n",
        "        self.zeta_max = zeta_max\n",
        "        self.inital_decay = decay\n",
        "    #@K.symbolic\n",
        "    def get_updates(self, params, constraints, loss): # This update method has everything betweem line 4 and 12 of the algorithm i.e the for loop given in the paper WAME\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.inital_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "        t = self.iterations + 1\n",
        "        lr_t = lr * K.sqrt(1. - K.pow(self.alpha_2, t)) / (1. - K.pow(self.alpha_1, t))\n",
        "\n",
        "        shapes = [K.get_variable_shape(p) for p in params]\n",
        "        prev_grads = [K.zeros(shape) for shape in shapes] \n",
        "        prev_param = [K.zeros(shape) for shape in shapes]\n",
        "        ms = [K.zeros(shape) for shape in shapes]\n",
        "        vs = [K.zeros(shape) for shape in shapes]\n",
        "        accs = [K.ones(shape) for shape in shapes]\n",
        "        acc_ms = [K.ones(shape) for shape in shapes]\n",
        "        acc_vs = [K.ones(shape) for shape in shapes]\n",
        "        self.weights = [self.iterations] + ms + vs\n",
        "\n",
        "\n",
        "        for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "                acc_ms, acc_vs, prev_grads, prev_param):\n",
        "\n",
        "            change = pg * g  # This is line four and six\n",
        "            change_below_zero = K.less(change,0.) #boolean true or false\n",
        "            change_above_zero = K.greater(change,0.) #boolean true or false\n",
        "            a_t = K.switch(\n",
        "                change_below_zero, # switch statement to jump to line 6\n",
        "                a * self.eta_minus, # otherwise use line 7\n",
        "                K.switch(change_above_zero, a * self.eta_plus, a) # if line 4 is true go to line 5, otheriwse leave a as it was\n",
        "            )\n",
        "            a_clipped = K.clip(a_t, self.eta_min, self.eta_max) #end of line 5 and 7 i.e min and max value of eta\n",
        "            v_t = (self.alpha_2 * v) + (1. - self.alpha_2) * K.square(g) # this is line 9\n",
        "            am_t = (self.alpha_a * am) + (1. - self.alpha_a) * a_clipped  # line 10 \n",
        "            a_rate = a_clipped / am_t                                     #line 11 i.e am_t == θij(t) as given in the paper\n",
        "            p_t = p - lr_t * a_rate * g / (K.sqrt(v_t) + self.epsilon)    # line 12\n",
        "\n",
        "            new_p = p_t \n",
        "            #apply constraints\n",
        "            if p in constraints:\n",
        "              c = constraints[p]\n",
        "              new_p = c(new_p)\n",
        "\n",
        "            self.updates.append(K.update(v, v_t))\n",
        "            self.updates.append(K.update(p, new_p))\n",
        "            self.updates.append(K.update(pg, p))\n",
        "            self.updates.append(K.update(a, a_t))\n",
        "            self.updates.append(K.update(am, am_t))\n",
        "            self.updates.append(K.update(pp, p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'lr': float(K.get_value(self.lr)),\n",
        "                  'alpha_1': float(K.get_value(self.alpha_1)),\n",
        "                  'alpha_2': float(K.get_value(self.alpha_2)),\n",
        "                  'alpha_a': float(K.get_value(self.alpha_a)),\n",
        "                  'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                  'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                  'zeta_min': float(self.zeta_min),\n",
        "                  'zeta_max': float(self.zeta_max),\n",
        "                  'epsilon': self.epsilon}\n",
        "        base_config = super(WAME, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "opt = WAME(name=\"WAME\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaKpaAwhGFv9"
      },
      "source": [
        "Step 7b -- Just checking the model is able to use the WAME class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1B-57VoTV8r",
        "outputId": "c499788c-0c06-4e0a-d7fb-826a8420d43a"
      },
      "source": [
        "#pca with Wrapper dataset\n",
        "#i will see which dataset gives better result on this neural network\n",
        "from tensorflow.python.keras import regularizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "#regularizers_choice =  [ regularizers.l2(0.01), regularizers.l2(0.001), regularizers.l2(0.0001), regularizers.l1(0.01), regularizers.l1(0.001), regularizers.l1(0.0001)]\n",
        "\n",
        "#One hidden layer used on Wrapper dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  #mlp_model.add(Dense(20, activation='relu'))\n",
        "  #mlp_model.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  #mlp_model.add(Dense(40, activation='relu'))\n",
        "  #mlp_model.add(Dense(45, activation='relu'))\n",
        "  #mlp_model.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=creating_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_wrapper_train_features, Y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.838804 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcsQaMIoc3GV",
        "outputId": "e94d8c0f-1054-4805-c09b-162123a173e0"
      },
      "source": [
        "# pca with Embedded dataset. This dataset is chosen by K fold analysis in step 8 with 6 hidden layers\n",
        "#i will see which dataset gives better result on this neural network\n",
        "from tensorflow.python.keras import regularizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "#regularizers_choice =  [ regularizers.l2(0.01), regularizers.l2(0.001), regularizers.l2(0.0001), regularizers.l1(0.01), regularizers.l1(0.001), regularizers.l1(0.0001)]\n",
        "\n",
        "#One hidden layer used on Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  #mlp_model.add(Dense(20, activation='relu'))\n",
        "  #mlp_model.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  #mlp_model.add(Dense(40, activation='relu'))\n",
        "  #mlp_model.add(Dense(45, activation='relu'))\n",
        "  #mlp_model.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=creating_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_embedded_train_features, Y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.839832 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3oDh1HlfxAb",
        "outputId": "acddab03-1257-4fe2-bd3c-f2470f146097"
      },
      "source": [
        "# # pca with Filter dataset\n",
        "#i will see which dataset gives better result on this neural network\n",
        "from tensorflow.python.keras import regularizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "#regularizers_choice =  [ regularizers.l2(0.01), regularizers.l2(0.001), regularizers.l2(0.0001), regularizers.l1(0.01), regularizers.l1(0.001), regularizers.l1(0.0001)]\n",
        "\n",
        "#One hidden layer used on Filter dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  #mlp_model.add(Dense(20, activation='relu'))\n",
        "  #mlp_model.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  #mlp_model.add(Dense(40, activation='relu'))\n",
        "  #mlp_model.add(Dense(45, activation='relu'))\n",
        "  #mlp_model.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=creating_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_selectKbest_train_features, Y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.827233 using {'batch_size': 20, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJvB1l_xy9PJ"
      },
      "source": [
        "roughly estimate without using K fold to analyse which dataset to use for the model\n",
        "\n",
        "pca_wrapper_train_features ---- 0.838804 \n",
        "\n",
        "pca_embedded_train_features ----- 0.839832   ---- so i chose pca with embedded method dataset for slightly better performance\n",
        "\n",
        "pca_selectKbest_train_features ---- 0.827233"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFjcEvB63xmg"
      },
      "source": [
        "STEP 8 KFOLD IS USED TO COMPARE WHICH MODEL FITS BEST ON WHICH DATASET\n",
        "Comparing which dataset(i.e the one obtained from pca with filter, wrapper or embedded method)performs best on which model, how many hidden layers to be used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dKx7B-D7yK4V",
        "outputId": "1b4df290-be6f-4349-8925-9b46e5bbbb87"
      },
      "source": [
        "#step8a for pca_Wrapper dataset \n",
        "#checking which algorithm to use for the pca data\n",
        "# Compare Algorithms\n",
        "\n",
        "#1 hidden layer used on pca_Wrapper dataset\n",
        "def creating_model1(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #one hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #2 hidden layer used on pca_Wrapper dataset\n",
        "def creating_model2(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))               #two hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #3 hidden layer used on pca_Wrapper dataset\n",
        "def creating_model3(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(30, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "\n",
        "  #4 hidden layer used on pca_Wrapper dataset\n",
        "def creating_model4(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #four hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #5 hidden layer used on pca_Wrapper dataset\n",
        "def creating_model5(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #five hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #6 hidden layer used on pca_Wrapper dataset\n",
        "def creating_model6(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_wrapper_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of HL(Hidden Layer)')\n",
        "plt.ylabel('Model Accuracy')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 812us/step - loss: 0.1706 - accuracy: 0.7687\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 813us/step - loss: 0.1236 - accuracy: 0.8223\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 835us/step - loss: 0.1210 - accuracy: 0.8235\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 804us/step - loss: 0.1189 - accuracy: 0.8239\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 852us/step - loss: 0.1149 - accuracy: 0.8286\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 853us/step - loss: 0.1142 - accuracy: 0.8329\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1132 - accuracy: 0.8378\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1125 - accuracy: 0.8361\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 821us/step - loss: 0.1107 - accuracy: 0.8389\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 909us/step - loss: 0.1107 - accuracy: 0.8386\n",
            "Epoch 1/10"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1358/1358 [==============================] - 2s 799us/step - loss: 0.1707 - accuracy: 0.7568\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 776us/step - loss: 0.1237 - accuracy: 0.8227\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 833us/step - loss: 0.1175 - accuracy: 0.8310\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 827us/step - loss: 0.1157 - accuracy: 0.8313\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 790us/step - loss: 0.1148 - accuracy: 0.8347\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 818us/step - loss: 0.1122 - accuracy: 0.8367\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 841us/step - loss: 0.1125 - accuracy: 0.8353\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1135 - accuracy: 0.8357\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 807us/step - loss: 0.1131 - accuracy: 0.8370\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 804us/step - loss: 0.1117 - accuracy: 0.8381\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 756us/step - loss: 0.1737 - accuracy: 0.7643\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 772us/step - loss: 0.1236 - accuracy: 0.8231\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 775us/step - loss: 0.1176 - accuracy: 0.8266\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 795us/step - loss: 0.1150 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 786us/step - loss: 0.1156 - accuracy: 0.8336\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1136 - accuracy: 0.8367\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 802us/step - loss: 0.1138 - accuracy: 0.8350\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1152 - accuracy: 0.8347\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1139 - accuracy: 0.8346\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 836us/step - loss: 0.1135 - accuracy: 0.8363\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 755us/step - loss: 0.1621 - accuracy: 0.7810\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 772us/step - loss: 0.1222 - accuracy: 0.8256\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1192 - accuracy: 0.8249\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 874us/step - loss: 0.1144 - accuracy: 0.8330\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 887us/step - loss: 0.1134 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 852us/step - loss: 0.1110 - accuracy: 0.8405\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 871us/step - loss: 0.1107 - accuracy: 0.8415\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 813us/step - loss: 0.1105 - accuracy: 0.8408\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 817us/step - loss: 0.1090 - accuracy: 0.8439\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 867us/step - loss: 0.1113 - accuracy: 0.8407\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 815us/step - loss: 0.1641 - accuracy: 0.7719\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 812us/step - loss: 0.1227 - accuracy: 0.8231\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1178 - accuracy: 0.8268\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 835us/step - loss: 0.1186 - accuracy: 0.8287\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 810us/step - loss: 0.1152 - accuracy: 0.8358\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 816us/step - loss: 0.1148 - accuracy: 0.8307\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 800us/step - loss: 0.1140 - accuracy: 0.8370\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1157 - accuracy: 0.8310\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 853us/step - loss: 0.1137 - accuracy: 0.8356\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 832us/step - loss: 0.1138 - accuracy: 0.8359\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 794us/step - loss: 0.1760 - accuracy: 0.7482\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 792us/step - loss: 0.1223 - accuracy: 0.8285\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 809us/step - loss: 0.1169 - accuracy: 0.8297\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 839us/step - loss: 0.1137 - accuracy: 0.8364\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1141 - accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1143 - accuracy: 0.8369\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 846us/step - loss: 0.1135 - accuracy: 0.8363\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 819us/step - loss: 0.1137 - accuracy: 0.8346\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1111 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 807us/step - loss: 0.1149 - accuracy: 0.8328\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 798us/step - loss: 0.1704 - accuracy: 0.7755\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 789us/step - loss: 0.1227 - accuracy: 0.8252\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 775us/step - loss: 0.1180 - accuracy: 0.8276\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 804us/step - loss: 0.1153 - accuracy: 0.8306\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1166 - accuracy: 0.8321\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1151 - accuracy: 0.8330\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 827us/step - loss: 0.1130 - accuracy: 0.8339\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1117 - accuracy: 0.8385\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 805us/step - loss: 0.1144 - accuracy: 0.8345\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1134 - accuracy: 0.8332\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 810us/step - loss: 0.1661 - accuracy: 0.7883\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 812us/step - loss: 0.1206 - accuracy: 0.8311\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 818us/step - loss: 0.1154 - accuracy: 0.8300\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1146 - accuracy: 0.8338\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 860us/step - loss: 0.1151 - accuracy: 0.8331\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1124 - accuracy: 0.8358\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1125 - accuracy: 0.8366\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1103 - accuracy: 0.8418\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 828us/step - loss: 0.1117 - accuracy: 0.8357\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1104 - accuracy: 0.8388\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 865us/step - loss: 0.1620 - accuracy: 0.7778\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 836us/step - loss: 0.1243 - accuracy: 0.8215\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1203 - accuracy: 0.8262\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1178 - accuracy: 0.8271\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 871us/step - loss: 0.1156 - accuracy: 0.8285\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 864us/step - loss: 0.1139 - accuracy: 0.8302\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1138 - accuracy: 0.8316\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1131 - accuracy: 0.8352\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 816us/step - loss: 0.1148 - accuracy: 0.8323\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 882us/step - loss: 0.1102 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 836us/step - loss: 0.1605 - accuracy: 0.7751\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 966us/step - loss: 0.1218 - accuracy: 0.8251\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 879us/step - loss: 0.1186 - accuracy: 0.8241\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 867us/step - loss: 0.1167 - accuracy: 0.8280\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1153 - accuracy: 0.8316\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1146 - accuracy: 0.8336\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 852us/step - loss: 0.1120 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 846us/step - loss: 0.1121 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 796us/step - loss: 0.1108 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 852us/step - loss: 0.1095 - accuracy: 0.8443\n",
            "1 HL: 0.837677 (0.006737)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 835us/step - loss: 0.1588 - accuracy: 0.7803\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 869us/step - loss: 0.1193 - accuracy: 0.8278\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 853us/step - loss: 0.1154 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 875us/step - loss: 0.1132 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 899us/step - loss: 0.1135 - accuracy: 0.8343\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 929us/step - loss: 0.1113 - accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 915us/step - loss: 0.1128 - accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 928us/step - loss: 0.1112 - accuracy: 0.8380\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 876us/step - loss: 0.1123 - accuracy: 0.8358\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 934us/step - loss: 0.1111 - accuracy: 0.8383\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 806us/step - loss: 0.1515 - accuracy: 0.7841\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 868us/step - loss: 0.1196 - accuracy: 0.8264\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 868us/step - loss: 0.1142 - accuracy: 0.8330\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1157 - accuracy: 0.8319\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 853us/step - loss: 0.1139 - accuracy: 0.8379\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 935us/step - loss: 0.1143 - accuracy: 0.8356\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 924us/step - loss: 0.1117 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 909us/step - loss: 0.1119 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 844us/step - loss: 0.1112 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 937us/step - loss: 0.1090 - accuracy: 0.8435\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 850us/step - loss: 0.1546 - accuracy: 0.7925\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1163 - accuracy: 0.8286\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 898us/step - loss: 0.1164 - accuracy: 0.8295\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 932us/step - loss: 0.1149 - accuracy: 0.8346\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 937us/step - loss: 0.1104 - accuracy: 0.8392\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1103 - accuracy: 0.8423\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 900us/step - loss: 0.1083 - accuracy: 0.8429\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 901us/step - loss: 0.1108 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 890us/step - loss: 0.1113 - accuracy: 0.8386\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 950us/step - loss: 0.1070 - accuracy: 0.8460\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 907us/step - loss: 0.1606 - accuracy: 0.7768\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 883us/step - loss: 0.1168 - accuracy: 0.8311\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 914us/step - loss: 0.1136 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 882us/step - loss: 0.1145 - accuracy: 0.8355\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1112 - accuracy: 0.8404\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1125 - accuracy: 0.8406\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 960us/step - loss: 0.1129 - accuracy: 0.8385\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8383\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8381\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 866us/step - loss: 0.1569 - accuracy: 0.7846\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1202 - accuracy: 0.8281\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1162 - accuracy: 0.8296\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 958us/step - loss: 0.1153 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1141 - accuracy: 0.8330\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 887us/step - loss: 0.1106 - accuracy: 0.8368\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 891us/step - loss: 0.1120 - accuracy: 0.8382\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 874us/step - loss: 0.1115 - accuracy: 0.8405\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 911us/step - loss: 0.1099 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 898us/step - loss: 0.1101 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 831us/step - loss: 0.1548 - accuracy: 0.7844\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1149 - accuracy: 0.8341\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1137 - accuracy: 0.8332\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1121 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 886us/step - loss: 0.1140 - accuracy: 0.8343\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 894us/step - loss: 0.1100 - accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1109 - accuracy: 0.8384\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 887us/step - loss: 0.1100 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 908us/step - loss: 0.1100 - accuracy: 0.8445\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 933us/step - loss: 0.1107 - accuracy: 0.8419\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 884us/step - loss: 0.1675 - accuracy: 0.7400\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 914us/step - loss: 0.1155 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 882us/step - loss: 0.1120 - accuracy: 0.8399\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 917us/step - loss: 0.1125 - accuracy: 0.8379\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 883us/step - loss: 0.1116 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 887us/step - loss: 0.1092 - accuracy: 0.8423\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 890us/step - loss: 0.1104 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 951us/step - loss: 0.1067 - accuracy: 0.8460\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 971us/step - loss: 0.1074 - accuracy: 0.8445\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1097 - accuracy: 0.8377\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 854us/step - loss: 0.1593 - accuracy: 0.7814\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1157 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1144 - accuracy: 0.8317\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 907us/step - loss: 0.1149 - accuracy: 0.8302\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1118 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 882us/step - loss: 0.1097 - accuracy: 0.8406\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 928us/step - loss: 0.1103 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1109 - accuracy: 0.8392\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 926us/step - loss: 0.1103 - accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1087 - accuracy: 0.8388\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 883us/step - loss: 0.1574 - accuracy: 0.7760\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 899us/step - loss: 0.1196 - accuracy: 0.8248\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 871us/step - loss: 0.1159 - accuracy: 0.8301\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 951us/step - loss: 0.1135 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 945us/step - loss: 0.1140 - accuracy: 0.8362\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 920us/step - loss: 0.1110 - accuracy: 0.8407\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1124 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 889us/step - loss: 0.1123 - accuracy: 0.8387\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 912us/step - loss: 0.1101 - accuracy: 0.8402\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 864us/step - loss: 0.1101 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 844us/step - loss: 0.1535 - accuracy: 0.7886\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1185 - accuracy: 0.8264\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 924us/step - loss: 0.1166 - accuracy: 0.8314\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1147 - accuracy: 0.8352\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 886us/step - loss: 0.1138 - accuracy: 0.8362\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1140 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 912us/step - loss: 0.1110 - accuracy: 0.8415\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 892us/step - loss: 0.1126 - accuracy: 0.8353\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1104 - accuracy: 0.8437\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1107 - accuracy: 0.8390\n",
            "2 HL: 0.841191 (0.005993)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 942us/step - loss: 0.1537 - accuracy: 0.7836\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1161 - accuracy: 0.8299\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1123 - accuracy: 0.8368\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8417\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8357\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1121 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1095 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 937us/step - loss: 0.1080 - accuracy: 0.8438\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1106 - accuracy: 0.8398\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 907us/step - loss: 0.1085 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 905us/step - loss: 0.1510 - accuracy: 0.7904\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 942us/step - loss: 0.1160 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 916us/step - loss: 0.1169 - accuracy: 0.8316\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 935us/step - loss: 0.1148 - accuracy: 0.8349\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 923us/step - loss: 0.1143 - accuracy: 0.8340\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1167 - accuracy: 0.8291\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 936us/step - loss: 0.1147 - accuracy: 0.8338\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 949us/step - loss: 0.1121 - accuracy: 0.8351\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8313\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 994us/step - loss: 0.1127 - accuracy: 0.8364\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 905us/step - loss: 0.1454 - accuracy: 0.7997\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 954us/step - loss: 0.1142 - accuracy: 0.8348\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 960us/step - loss: 0.1154 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 925us/step - loss: 0.1119 - accuracy: 0.8379\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 951us/step - loss: 0.1113 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 945us/step - loss: 0.1100 - accuracy: 0.8420\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 950us/step - loss: 0.1076 - accuracy: 0.8456\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 925us/step - loss: 0.1100 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8417\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8420\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 960us/step - loss: 0.1542 - accuracy: 0.7876\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1199 - accuracy: 0.8240\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1153 - accuracy: 0.8323\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8330\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 949us/step - loss: 0.1120 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1132 - accuracy: 0.8359\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 978us/step - loss: 0.1108 - accuracy: 0.8398\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1105 - accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1084 - accuracy: 0.8437\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 945us/step - loss: 0.1081 - accuracy: 0.8446\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 919us/step - loss: 0.1492 - accuracy: 0.7933\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 951us/step - loss: 0.1163 - accuracy: 0.8314\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 979us/step - loss: 0.1141 - accuracy: 0.8339\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 981us/step - loss: 0.1134 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 990us/step - loss: 0.1120 - accuracy: 0.8399\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 929us/step - loss: 0.1126 - accuracy: 0.8341\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 966us/step - loss: 0.1123 - accuracy: 0.8355\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 976us/step - loss: 0.1119 - accuracy: 0.8380\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 974us/step - loss: 0.1097 - accuracy: 0.8392\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8420\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 937us/step - loss: 0.1537 - accuracy: 0.7863\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 924us/step - loss: 0.1175 - accuracy: 0.8321\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 982us/step - loss: 0.1126 - accuracy: 0.8362\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 977us/step - loss: 0.1145 - accuracy: 0.8364\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1106 - accuracy: 0.8441\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1111 - accuracy: 0.8388\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 902us/step - loss: 0.1084 - accuracy: 0.8438\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 999us/step - loss: 0.1112 - accuracy: 0.8428\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1100 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1089 - accuracy: 0.8417\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 948us/step - loss: 0.1488 - accuracy: 0.7879\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 912us/step - loss: 0.1161 - accuracy: 0.8334\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 943us/step - loss: 0.1168 - accuracy: 0.8317\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1152 - accuracy: 0.8339\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 932us/step - loss: 0.1135 - accuracy: 0.8354\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 963us/step - loss: 0.1123 - accuracy: 0.8371\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 976us/step - loss: 0.1107 - accuracy: 0.8430\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 922us/step - loss: 0.1124 - accuracy: 0.8356\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 961us/step - loss: 0.1114 - accuracy: 0.8384\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1101 - accuracy: 0.8407\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 946us/step - loss: 0.1450 - accuracy: 0.7950\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 966us/step - loss: 0.1144 - accuracy: 0.8349\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 954us/step - loss: 0.1148 - accuracy: 0.8324\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1150 - accuracy: 0.8314\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 958us/step - loss: 0.1097 - accuracy: 0.8409\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1128 - accuracy: 0.8336\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1110 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 913us/step - loss: 0.1124 - accuracy: 0.8348\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 910us/step - loss: 0.1122 - accuracy: 0.8379\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 928us/step - loss: 0.1115 - accuracy: 0.8385\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 914us/step - loss: 0.1549 - accuracy: 0.7841\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8332\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1147 - accuracy: 0.8353\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 973us/step - loss: 0.1149 - accuracy: 0.8359\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1102 - accuracy: 0.8414\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1104 - accuracy: 0.8413\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 917us/step - loss: 0.1099 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1100 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 977us/step - loss: 0.1076 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1091 - accuracy: 0.8410\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 920us/step - loss: 0.1541 - accuracy: 0.7784\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 935us/step - loss: 0.1172 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1155 - accuracy: 0.8318\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 978us/step - loss: 0.1121 - accuracy: 0.8407\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 943us/step - loss: 0.1099 - accuracy: 0.8437\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 923us/step - loss: 0.1098 - accuracy: 0.8439\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 979us/step - loss: 0.1095 - accuracy: 0.8425\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1096 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 974us/step - loss: 0.1116 - accuracy: 0.8374\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 951us/step - loss: 0.1094 - accuracy: 0.8432\n",
            "3 HL: 0.839036 (0.007487)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 973us/step - loss: 0.1428 - accuracy: 0.8009\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1165 - accuracy: 0.8294\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 994us/step - loss: 0.1138 - accuracy: 0.8358\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8374\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8413\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 998us/step - loss: 0.1136 - accuracy: 0.8349\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 981us/step - loss: 0.1107 - accuracy: 0.8376\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8378\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8478\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1443 - accuracy: 0.7970\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1180 - accuracy: 0.8275\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 975us/step - loss: 0.1138 - accuracy: 0.8365\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 968us/step - loss: 0.1137 - accuracy: 0.8352\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8380\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8377\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8409\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 974us/step - loss: 0.1378 - accuracy: 0.8004\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8379\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8386\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8364\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8419\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8388\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8399\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 975us/step - loss: 0.1085 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1475 - accuracy: 0.7932\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.8347\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8358\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8388\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8386\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 999us/step - loss: 0.1097 - accuracy: 0.8443\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8391\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8409\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 989us/step - loss: 0.1428 - accuracy: 0.8061\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8294\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8404\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 996us/step - loss: 0.1124 - accuracy: 0.8360\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8410\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8428\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1429 - accuracy: 0.7984\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8356\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8392\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8451\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8436\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8445\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1056 - accuracy: 0.8483\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8439\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1409 - accuracy: 0.8024\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.8339\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8352\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8348\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8356\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8331\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8408\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8397\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.7889\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1193 - accuracy: 0.8275\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 997us/step - loss: 0.1145 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8312\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8353\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8418\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8425\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8381\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1423 - accuracy: 0.8006\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8343\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8344\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8378\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8444\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8373\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1443 - accuracy: 0.7984\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8335\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8428\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8354\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8378\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8449\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8420\n",
            "4 HL: 0.840329 (0.005699)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1444 - accuracy: 0.7998\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1189 - accuracy: 0.8301\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8344\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8396\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8361\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8433\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8437\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1482 - accuracy: 0.7893\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1201 - accuracy: 0.8234\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8287\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1178 - accuracy: 0.8292\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1160 - accuracy: 0.8310\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8365\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8359\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8378\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8370\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8401\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1416 - accuracy: 0.7997\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1176 - accuracy: 0.8310\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8355\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1145 - accuracy: 0.8348\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8362\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8329\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8399\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1439 - accuracy: 0.8012\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.8293\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8343\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8393\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8378\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8447\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8386\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1436 - accuracy: 0.7983\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8347\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8417\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8426\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8387\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1072 - accuracy: 0.8453\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1394 - accuracy: 0.8062\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8365\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8395\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8414\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8409\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8442\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1056 - accuracy: 0.8476\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8391\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1429 - accuracy: 0.7993\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1197 - accuracy: 0.8255\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1177 - accuracy: 0.8272\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1138 - accuracy: 0.8369\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8343\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1152 - accuracy: 0.8354\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8424\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1335 - accuracy: 0.8071\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1170 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8385\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8349\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8356\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8376\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8416\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1433 - accuracy: 0.7976\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8273\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8359\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8361\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8441\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8405\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8379\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1418 - accuracy: 0.7979\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1188 - accuracy: 0.8224\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8320\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8354\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8363\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8359\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8450\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8403\n",
            "5 HL: 0.838472 (0.003648)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1403 - accuracy: 0.7945\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8334\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1147 - accuracy: 0.8354\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8421\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8359\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8376\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8395\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8408\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8390\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1415 - accuracy: 0.7987\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1204 - accuracy: 0.8271\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1164 - accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1165 - accuracy: 0.8332\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8353\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8378\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8342\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8421\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8368\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8377\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1467 - accuracy: 0.7943\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1165 - accuracy: 0.8345\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1162 - accuracy: 0.8310\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8338\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8375\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8411\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8434\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8345\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8397\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1483 - accuracy: 0.7848\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1147 - accuracy: 0.8351\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1167 - accuracy: 0.8328\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8388\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8378\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8425\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8409\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8361\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8430\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8404\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1395 - accuracy: 0.8002\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8365\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1155 - accuracy: 0.8325\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8419\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8345\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8438\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8397\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8399\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8410\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1432 - accuracy: 0.7982\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1167 - accuracy: 0.8337\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1137 - accuracy: 0.8365\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8429\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8429\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8445\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8418\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1451 - accuracy: 0.7999\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1170 - accuracy: 0.8300\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1147 - accuracy: 0.8351\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8396\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8425\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8447\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8375\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1463 - accuracy: 0.7877\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1180 - accuracy: 0.8305\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1145 - accuracy: 0.8337\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8352\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8398\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8356\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8380\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8439\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1470 - accuracy: 0.7959\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1173 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8332\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8346\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8415\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8408\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1403 - accuracy: 0.8083\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1178 - accuracy: 0.8308\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8384\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8384\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1145 - accuracy: 0.8321\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8361\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8414\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8397\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8420\n",
            "6 HL: 0.839666 (0.004287)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEjCAYAAAAlhuZMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xdZX3v8c+XcAlFbmlSRUIIHlFDURCHixKwkYqIHvDCsaBUoWk5tBooYi0esAYs9tRWUQvSomCUSzDa0hOP3LwEMUoxE7mGSIuRSwBLECwEBBL49o+1BhfjzJ69hlmz9575vl+v/cpe99+zZ7J/8zzPWs8j20RERLRrk04HEBERvSWJIyIiakniiIiIWpI4IiKiliSOiIioJYkjIiJqSeKIjpK0SNJfN3Tu90i6usX235O0tolr9zpJ/0fSFzsdR3SnJI4YF5KukfSwpC3G65q2L7Z9cCUGS3rpeF1fhRMk3SrpMUlrJX1N0ivHK4bRsv0J23/c6TiiOyVxROMkzQYOAAwcNk7X3HQ8rjOCzwInAicA04CXAf8KvKWTQY2kSz676GJJHDEe3gv8G7AIeF+rHSV9WNL9ku6T9MfVWoKkbSV9RdI6SXdJOk3SJuW2YyT9QNJZkn4BLCzXLS+3X1te4iZJ6yX9QeWaJ0t6oLzusZX1iyR9XtIV5TE/kPQiSZ8pa08/kfTqYcqxK/B+4Cjb37X9pO3Hy1rQ/61Znl9KWiPpdeX6e8p43zco1n+U9C1Jj0r6nqSdK9s/Wx73iKSVkg6obFso6euSLpL0CHBMue6icvvUctsvylhWSHphue3FkpZKekjSHZL+ZNB5l5RlfFTSKkl9rX7+0RuSOGI8vBe4uHy9aeBLZzBJhwAfBH4feCnwe4N2+QdgW+AlwOvL8x5b2b4vsAZ4IXBm9UDbB5Zv97D9AttfLZdfVJ5zR2A+cI6k7SuHvgs4DZgOPAlcB/y4XP468OlhynwQsNb2j4bZ3m55bgZ+G7gEuBTYm+KzORo4W9ILKvu/B/h4GduNFJ/3gBXAnhQ1n0uAr0maWtl+eFme7QYdB0Wy3xbYqYzleOBX5bZLgbXAi4EjgE9IekPl2MPKfbYDlgJnt/g8okckcUSjJM0FdgaW2F4J/BR49zC7vwv4ku1Vth8HFlbOMwU4EviI7Udt3wl8CvjDyvH32f4H2xtt/4r2bADOsL3B9uXAeuDlle2X2V5p+wngMuAJ21+x/TTwVWDIGgfFF+z9w120zfL8zPaXKtfaqYz1SdtXA09RJJEB37R9re0ngVOB10raCcD2RbZ/UX42nwK2GFTO62z/q+1nhvjsNpTleantp8vP45Hy3PsDf2n7Cds3Al+kSIADltu+vCzDhcAew30m0TuSOKJp7wOutv1guXwJwzdXvRi4p7JcfT8d2Ay4q7LuLoqawlD7t+sXtjdWlh8Hqn/F/2fl/a+GWK7u+5zzAju0uG475Rl8LWy3uv6z5be9HniI4jNF0ockrZb0X5J+SVGDmD7UsUO4ELgKuLRsQvykpM3Kcz9k+9EWZfh55f3jwNT0ofS+JI5ojKQtKWoRr5f0c0k/B04C9pA01F+e9wMzK8s7Vd4/SPGX786VdbOAeyvL3TTU83eAmS3a9NspT13Pfl5lE9Y04L6yP+PDFD+L7W1vB/wXoMqxw352ZW3sdNu7Aa8D3kpRq7gPmCZp6zEsQ/SAJI5o0tuAp4HdKNrX9wTmAN/nuc0ZA5YAx0qaI+m3gI8ObCibOpYAZ0rauuz4/SBwUY14/pOiP6Fxtv8D+DywWMXzIpuXncxHSjpljMoz2KGS5kranKKv499s3wNsDWwE1gGbSvorYJt2TyppnqRXls1rj1AkvGfKc/8Q+JuybK+i6Cd6PmWIHpDEEU16H0Wfxd22fz7wouggfc/gJgvbVwCfA5YBd1DciQVFpzTAAuAxig7w5RTNXhfUiGch8OXyzqB3jbJMdZxAUdZzgF9S9O+8HfhGuf35lmewS4CPUTRRvYaiAx2KZqYrgX+naEp6gnrNei+i6Dh/BFgNfI+i+QrgKGA2Re3jMuBjtr/9PMoQPUCZyCm6laQ5wK3AFoP6IWIQSYso7uI6rdOxxMSXGkd0FUlvl7RFeUvs3wLfSNKI6C5JHNFt/jfwAEWzztPAn3Y2nIgYLE1VERFRS2ocERFRSxJHRETUksQRERG1JHFEREQtSRwREVFLEkdERNSSxBEREbUkcURERC1JHBERUUsSR0RE1JLEERERtSRxRERELUkcERFRSxJHRETUsunIu/S+6dOne/bs2Z0OIyKiZ6xcufJB2zOG2jYpEsfs2bPp7+/vdBgRET1D0l3DbUtTVURE1JLEERERtSRxRERELUkcERFRSxJHRETUksQRERG1JHFEREQtSRwREVHLpHgAMKJK0qiPtT2GkUT0piSOmHRafflLSnKIGEGaqiIiopYkjoiIqCWJIyIiakniiIiIWpI4IiKiliSOiIioJYkjIiJqSeKIiIhakjgiIqKWJI6IiKgliSMiImpJ4oiIiFoaTRySDpF0u6Q7JJ0yxPZZkpZJukHSzZIOLdfPlvQrSTeWr3+sHPMaSbeU5/ycns9QpzEkSaN+RcTE19jouJKmAOcAbwTWAiskLbV9W2W304Alts+VtBtwOTC73PZT23sOcepzgT8Bri/3PwS4oplSTE4ZPTYiWmmyxrEPcIftNbafAi4FDh+0j4FtyvfbAve1OqGkHYBtbP+bi2+vrwBvG9uwIyKilSYTx47APZXlteW6qoXA0ZLWUtQeFlS27VI2YX1P0gGVc64d4ZwREdGgTneOHwUssj0TOBS4UNImwP3ALNuvBj4IXCJpmxbn+Q2SjpPUL6l/3bp1Yx54RMRk1WTiuBfYqbI8s1xXNR9YAmD7OmAqMN32k7Z/Ua5fCfwUeFl5/MwRzkl53Hm2+2z3zZgxYwyKE9EbcnNDNK3JxLEC2FXSLpI2B44Elg7a527gIABJcygSxzpJM8rOdSS9BNgVWGP7fuARSfuVd1O9F/h/DZYhoufYHvbVzvaIkTR2V5XtjZI+AFwFTAEusL1K0hlAv+2lwMnAFySdRNFRfoxtSzoQOEPSBuAZ4HjbD5Wn/jNgEbAlxd1UuaMqImIcaTL8ldHX1+f+/v5OhzEhTPTbcVO+iIKklbb7htrW6c7xiIjoMUkcERFRSxJHRETUksQRERG1JHFEREQtSRwREVFLEkdERNSSxBEREbUkcURERC1JHBERUUsSR0RE1JLEERERtSRxRERELUkcERFRSxJHRETUksQRERG1JHFEREQtSRwREVFLEkdERNSSxBEREbUkcURERC1JHBERUUsSR0RE1JLEERERtSRxRERELUkcERFRSxJHRETUksQRERG1JHFEREQtm3Y6gIiI+DVJozrO9hhHMrwkjoiILjJcApA0rsmhlTRVRURELalxjFIvVCcjIprQaI1D0iGSbpd0h6RThtg+S9IySTdIulnSoUNsXy/pQ5V1d0q6RdKNkvqbjL8V20O+Wm1L0oiIiaCxGoekKcA5wBuBtcAKSUtt31bZ7TRgie1zJe0GXA7Mrmz/NHDFEKefZ/vBZiKPiIhWmqxx7APcYXuN7aeAS4HDB+1jYJvy/bbAfQMbJL0N+BmwqsEYIyKipiYTx47APZXlteW6qoXA0ZLWUtQ2FgBIegHwl8DpQ5zXwNWSVko6briLSzpOUr+k/nXr1o2+FBER8RydvqvqKGCR7ZnAocCFkjahSChn2V4/xDFzbe8FvBl4v6QDhzqx7fNs99numzFjRkPhR0RMPiP2cUhaAFxk++Ga574X2KmyPLNcVzUfOATA9nWSpgLTgX2BIyR9EtgOeEbSE7bPtn1vuf8Dki6jaBK7tmZsERExSu3UOF5I0bG9pLxLqt37UFcAu0raRdLmwJHA0kH73A0cBCBpDjAVWGf7ANuzbc8GPgN8wvbZkraStHW5/1bAwcCtbcYTERFjYMTEYfs0YFfgfOAY4D8kfULS/xjhuI3AB4CrgNUUd0+tknSGpMPK3U4G/kTSTcBi4Bi3vmf1hcDycv8fAd+0feVIZRitadOmIanWC6h9jCSmTZvWVDEiIsaU2n22QNIewLEUTUvLgP2Ab9n+cHPhjY2+vj7399d/5GM8H/HvpuEEWumVOEcr5YtuNd4/O0krbfcNtW3EGoekEyWtBD4J/AB4pe0/BV4DvHNMI41xM5raVGpUEQHtPQA4DXiH7buqK20/I+mtzYQVTXv44YfH+6+XcbvWZDBt2jQefrju/SqF0fwstt9+ex566KFRXS8mnnYSxxXAs78xkrYB5ti+3vbqxiKLiGEl8UcntXNX1blA9XmK9eW6iIhxN5qm0mpTazx/7dQ4VL3TqWyiyqi6EdERrWpa6fwfH+3UONZIOkHSZuXrRGBN04FFRExUvX6rfzuJ43jgdRRPfa+leKp72DGiIiKitYE+qvF4jfYmilZGbHKy/QDFU98RERFtjVU1lWJMqd+lGBIEANt/1GBcERHRpdppqroQeBHwJuB7FIMVPtpkUBER0b3aSRwvtf1R4DHbXwbeQtHPERERk1A7iWND+e8vJe1OMVPf7zQXUkREdLN2Esd5kranmB98KXAb8LeNRhXxPGUsrojmtOwcVzEb3yPlJE7XAi8Zl6ginqcMyRHRnJY1DtvPAF0/bHpERIyfdoYO+bakDwFfBR4bWGk7Q2X2MH9sG1i47fheLyImhHYSxx+U/76/ss6k2aqn6fRHxr0pxwvH7XIR0aB2nhzfZTwCiYiI3tDOk+PvHWq97a+MfTgREdHt2mmq2rvyfipwEPBjIIkjImISaqepakF1WdJ2wKWNRRQREV2tnQcAB3sMSL9HRMQk1U4fxzco7qKCItHsBixpMqiIiOhe7fRx/H3l/UbgLttrG4onIiK6XDuJ427gfttPAEjaUtJs23c2GllERHSldvo4vgY8U1l+ulwXERGTUDuJY1PbTw0slO83by6kiIjoZu0kjnWSDhtYkHQ48GBzIUXEZJdh8btbO30cxwMXSzq7XF4LDPk0eUTEWMiw+N2tnQcAfwrsJ+kF5fL6xqOKiIiuNWJTlaRPSNrO9nrb6yVtL+mvxyO4iIjoPu30cbzZ9i8HFsrZAA9tLqSIiOhm7SSOKZK2GFiQtCWwRYv9nyXpEEm3S7pD0ilDbJ8laZmkGyTdLOnQIbavLyeSauucERHRrHY6xy8GviPpS+XysbQxMq6kKcA5wBspOtRXSFpq+7bKbqcBS2yfK2k34HJgdmX7p4Erap4zIiIa1E7n+N9Kugn4/XLVx21f1ca59wHusL0GQNKlwOFA9UvewMCcotsC9w1skPQ24GdUpqtt85wREdGgdmoc2L4SuFLSVsA7JH3T9ltGOGxH4J7K8lpg30H7LASulrQA2IoyOZV3cP0lRc3iQ5X92zlnxISfU32ily+6Wzuj424OvAV4N/Am4J+Bfxyj6x8FLLL9KUmvBS6UtDtFQjmrvItrVCeWdBxwHMCsWbPGKNzoFRN9TvWJXr7obsMmDkkHU3yxHwwso+jX2Nv2sW2e+15gp8ryzHJd1XzgEADb10maCkynqEUcIemTwHbAM5KeAFa2cU7K850HnAfQ19c3fv/DIiJGMJ41xiZqi61qHFcC3wfm2v4ZgKTP1jj3CmBXSbtQfLkfSVFrqbqbYiraRZLmUExNu872AQM7SFoIrLd9tqRN2zhntGk8n5bdfvvtx+1aEd1uPGuMTdQWWyWOvSi+mL8taQ3FdLFT2j2x7Y2SPgBcVR53ge1Vks4A+m0vBU4GviDpJIqO8mPc4tMc7pztxhS/NtpfWknj2kQSEd1H7XwJSHodRbPVO4GbgMvKpqCe0NfX5/7+/trHjeeXZK98ISfOXC/X663rjfZaklba7htqW1tzjtv+oe0FFH0KZwH71Y4iIiImhLZuxx1g+xng6vIVE9RIfR+ttvdCbSQinp9aiSMmh3z5R0QrSRwR0XXygGN3a/UcR8spsWw/NPbhRETkAcdu16rGsZLiFtmhGrQNvKSRiCIioqsNmzhs7zKegURERG9oZwZASTpa0kfL5VmS9mk+tIiI6EbtPMfxeeC1/Hpoj0cp5sSIiIhJqJ27qva1vZekG6CYOrYcMTciIiahdhLHhnLmPQNImgE802hUXaLXR7CMiGhCO4njc8BlwO9IOhM4gmLK1wmv10ewjIhoQjtTx14saSXF8OcC3mZ7deORRUREV2r3AcAHgMXVbXkAMCJicmr3AcBZwMPl++0oJmDKcx4REZPQsLfj2t7F9kuAbwP/0/Z0278NvJWMjhsRMWm18xzHfrYvH1iwfQXwuuZCioiIbtbOXVX3SToNuKhcfg9wX3MhRUQ7Mmd8dEo7ieMo4GMUt+QCXFuui4gOyZzx0Unt3I77EHCipK2LRa9vPqyIiOhW7Qxy+MpyuJFbgVWSVkravfnQIiKiG7XTOf5PwAdt72x7Z+Bk4Lxmw4qIiG7VTuLYyvaygQXb1wBbNRZRRER0tXY6x9eUc3FcWC4fDaxpLqSIiOhm7dQ4/giYAfxL+ZpRrouIiEmonbuqHgZOGIdYIiKiB7Qa5HBpqwNtHzb24URERLdrVeN4LXAPxai411MMcBgREZNcq8TxIuCNFE+Jvxv4JrDY9qrxCCwiYiIbryFjmhguZtjEYftp4ErgSklbUCSQaySdbvvsMY8kImKSGM2wL900XEzLzvEyYbyFImnM5tfTyEZExCTVqnP8K8DuwOXA6bZvHbeoImLSy+i/3atVjeNo4DHgROCEyg9RFIMdbtNwbBExSWX03+7WagbATWxvXb62qby2bjdpSDpE0u2S7pB0yhDbZ0laJukGSTdLOrRcv4+kG8vXTZLeXjnmTkm3lNv6R1PoiIgYvXaGHBkVSVOAcyjuzFoLrJC01PZtld1OA5bYPlfSbhTNYrMpRuLts71R0g7ATZK+YXtjedw82w82FXtERAyvnSFHRmsf4A7ba2w/BVwKHD5oHwMDtZdtKWcWtP14JUlMLfeLiIgu0GTi2JHiAcIBa8t1VQuBoyWtpahtLBjYIGlfSauAW4DjK4nEwNXlvCDHDXdxScdJ6pfUv27duudfmoiIAJpNHO04ClhkeyZwKHChpE0AbF9v+3eBvYGPSJpaHjPX9l7Am4H3SzpwqBPbPs92n+2+GTNmNF+SiIhJosnEcS+wU2V5Zrmuaj6wBMD2dRTNUtOrO9heDaynuDUY2/eW/z5A8UzJPg3EHhERw2gycawAdpW0i6TNgSOBwQMn3g0cBCBpDkXiWFces2m5fmfgFcCdkrYq5z5H0lbAwRQd6RERMU4au6uqvCPqA8BVwBTgAturJJ0B9NteSjEN7RcknUTRd3GMbUuaC5wiaQPwDPBnth+U9BLgsvKZkk2BS2xf2VQZIiLiN2kyPCzT19fn/v76j3yM58NEeXBpbI3359krP79eiXO0JnL5OvA7vdJ231DbOt05HhERPSaJIyIiakniiIiIWpI4IiKiliSOiIioJYkjIiJqSeKIiIhakjgiIqKWxp4cj+i0TD0a0YwkjpiQMvVo9KpWf/C02jaev7dJHBERXaQX/nBJH0dERNSSxBEREbUkcURERC3p44iInjLS3XLd0oE8kSVxRERPyZd/56WpKtqyePFidt99d6ZMmcLuu+/O4sWLOx1SRHRIahwxosWLF3Pqqady/vnnM3fuXJYvX878+fMBOOqoozocXUSMt9Q4YkRnnnkm559/PvPmzWOzzTZj3rx5nH/++Zx55pmdDi0iOiBzjreQOccLU6ZM4YknnmCzzTZ7dt2GDRuYOnUqTz/9dAcjG3vd/HMYCxO9fDF2Mud4PC9z5sxh+fLlz1m3fPly5syZ06GIIqKTkjhiRKeeeirz589n2bJlbNiwgWXLljF//nxOPfXUTocWER2QzvEY0UAH+IIFC1i9ejVz5szhzDPPTMd4xCSVPo4W0scx+Uz0n8NEL1+MnfRxRETEmEniiIiIWpI4IiKiliSOiIioJXdVjWC85q3OnNUR0SuSOFoYzd0nuWslIia6NFVFREQtSRwREVFLo4lD0iGSbpd0h6RThtg+S9IySTdIulnSoeX6fSTdWL5ukvT2ds8ZERHNaqyPQ9IU4BzgjcBaYIWkpbZvq+x2GrDE9rmSdgMuB2YDtwJ9tjdK2gG4SdI3ALdxzoiIaFCTNY59gDtsr7H9FHApcPigfQxsU77fFrgPwPbjtjeW66eW+7V7zoiIaFCTiWNH4J7K8tpyXdVC4GhJaylqGwsGNkjaV9Iq4Bbg+DKRtHPOgeOPk9QvqX/dunXPtywREVHqdOf4UcAi2zOBQ4ELJW0CYPt6278L7A18RNLUOie2fZ7tPtt9M2bMGPPAJ5vMOR4RA5p8juNeYKfK8sxyXdV84BAA29eVyWE68MDADrZXS1oP7N7mOWOMZc7xiKhqssaxAthV0i6SNgeOBJYO2udu4CAASXMo+jPWlcdsWq7fGXgFcGeb54wxljnHI6KqsRpHeUfUB4CrgCnABbZXSToD6Le9FDgZ+IKkkyg6wI+xbUlzgVMkbQCeAf7M9oMAQ52zqTJEYfXq1cydO/c56+bOncvq1as7FFFEdFKjQ47Yvpyi07u67q8q728D9h/iuAuBC9s9ZzRrYM7xefPmPbsuc45HTF6d7hyPHpA5xyOiKoMcjlKrUXNbbevFARAz53hEVGXO8YiKiT668UQvX4ydVnOOp8YRk85Ic6xMtBpjxFhL4ohJJ1/+Ec9POscjIqKWJI6IiKgliSMiImpJ4oiIiFrSOR4xweSusWhaEkfEBJMv/2hamqoiIqKWJI6IiKgliSMiImpJ4oiIiFqSOCIiopYkjoiIqCWJIyIiakniiIiIWibFRE6S1gF3jdPlpgMPjtO1OiHl620pX+8a77LtbHvGUBsmReIYT5L6h5s1ayJI+Xpbyte7uqlsaaqKiIhakjgiIqKWJI6xd16nA2hYytfbUr7e1TVlSx9HRETUkhpHRETUksTRJkkXSHpA0q0t9lko6UOD1t0paXr5fn3TcY6GpJ0kLZN0m6RVkk4cZr9eLd9UST+SdFNZvtOH2W+RpCMGrVtf/ju71c++G0iaIukGSf9/mO09Wb7yd+wWSTdK6h9mn5783QSQtJ2kr0v6iaTVkl47xD5dVb4kjvYtAg7pdBAN2QicbHs3YD/g/ZJ263BMY+lJ4A229wD2BA6RtF+HY2rCicDqTgfRkHm29+yW21HH2GeBK22/AtiDHvgZJnG0yfa1wEOdjqMJtu+3/ePy/aMUv7g7djaqsePCwF9km5WvCdW5J2km8Bbgi52OJdonaVvgQOB8ANtP2f5lZ6MaWaaOHXsnSTq6svzijkUyCpJmA68Grh9ml54sn6QpwErgpcA5tocr399JOm38IhsznwE+DGw9wn69WD4DV0sy8E+2h7u7qBd/N3cB1gFfkrQHxe/oibYfG2Lfrilfahxj76yySr2n7T2B+zodULskvQD4Z+DPbT8yzG49WT7bT5fxzgT2kbT7MLv+xaDydT1JbwUesL2yjd17rnzAXNt7AW+maEY9cJj9evF3c1NgL+Bc268GHgNOGWbfrilfEkcAIGkziqRxse1/6XQ8TSmbAZYxsfqr9gcOk3QncCnwBkkXdTaksWP73vLfB4DLgH06G9GYWgusrdSAv06RSLpaEkcgSRRtrKttf7rT8Yw1STMkbVe+3xJ4I/CTzkY1dmx/xPZM27OBI4Hv2j56hMN6gqStJG098B44GOjKu79Gw/bPgXskvbxcdRBwWwdDaksSR5skLQauA14uaa2k+aM4zW+Vxw68PjjGYY7W/sAfUvylemP5OnQU5+nW8u0ALJN0M7AC+JbtIW9ZHcHLB5Xvf41tmB3XjeV7IbBc0k3Aj4Bv2r5yFOfp1t9NgAXAxeXv557AJ0ZxjnEtX54cj4iIWlLjiIiIWpI4IiKiliSOiIioJYkjIiJqSeKIiIhakjii60iypE9Vlj8kaWED11ks6WZJJw1aP6qRSCX9uaT3lu9bjUT7YklfH+Yc10j6jYH8JB0j6ew65RtOtSxNK5+hGc3ts9HFkjiiGz0JvKPJLzdJLwL2tv0q22eNwfk2Bf4IuGSkfW3fZ/uIkfbrdZI2tb0OuF/S/p2OJ8ZOEkd0o40U02SeNHhDOW/Ed8uawnckzWp1IhVzcXypnM/hBknzyk1XAzuWDzseMAYxvwH4se2NI+1YnftC0paSLi3nYbgM2LKy37GS/l3Sjyge0hxYP0PSP0taUb72L9cvVDFvzDWS1kg6od3gJe0j6bryM/rhwJPMkq6VtGdlv+WS9iif6L5AxTwnN0g6vNx+jKSlkr4LfKc87F+B97QbS3S/JI7oVucA71Ex7HTVPwBftv0q4GLgcyOc5/0UI6u/EjgK+LKkqcBhwE/LAeO+P8RxJ1Weor+RkUci3Z9iZNOqvxt0jqH8KfC47TnAx4DXAEjaATi9PO9coDo/ymcpBrzbG3gnzx1K/RXAmyjGc/qYijHI2vET4IByoL2/4tdPL58PHFPG9DJgqu2bgFMphjbZB5hXlnWr8pi9gCNsv75c7gfGIjlHl8iw6tGVbD8i6SvACcCvKpteC7yjfH8h8MkRTjWXItlg+yeS7gJeBgw3+u+As2z//cCCigEEW9mB35yA5y9sP9uXMUzfyIGUyc/2zeWwEwD7AteUTT1I+moZN8DvA7tJGjjHNipGNoZiSI4ngSclPUAxZMfaEWIH2JYiqe5KMYz5QML5GvBRSX9B0RS3qFx/MMXAigN9QVOBgdrft2xX5655gN4Y4jzalMQR3ewzwI+BL3U6kDb8iuLLczxsAuxn+4nqyjKRPFlZ9TTt/x//OLDM9ttVzMlyDYDtxyV9CzgceBdljQgQ8E7btw+KYV+KocGrpvLc5B89Lk1V0bXKv1qXANUBJX9IMQIsFO3mQzUzVX2/3G+gqWUWcHvLI0ZnNcUkUXVdC7wbQMUcIa8q118PvF7Sb5fNTdUBB6+mGBiP8rixmFdjW+De8v0xg7Z9kaJWtML2w+W6q4AFKrOVpFe3OPfLmEAj2kYSR3S/TwQ1kRIAAADpSURBVAHVu6sWAMeWTTp/SDHPNpKOl3T8EMd/HthE0i3AV4Fjyqac52OokUivoGh2qutc4AWSVgNnUPaT2L4fWEgxIvMPeG4z2AlAX3mDwG3AUOUeyc2V+D9N0eT3N5JuYFAtpZwg6hGeW/P7OEVz1s2SVpXLw5kHfHMUMUaXyui4EWOkvCvqw7b/o9OxjCVJL6ZounqF7WdGcfy1wOGV2kr0uNQ4IsbOKRSd5BNG+UDj9cCpo0waM4BPJ2lMLKlxRERELalxRERELUkcERFRSxJHRETUksQRERG1JHFEREQtSRwREVHLfwN9/wfeAs8ukgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iDH8GQEABN6u",
        "outputId": "eb5550c9-a1be-4206-eb1d-18d4daa5529f"
      },
      "source": [
        "#step8b for pca_Embedded dataset \n",
        "#checking which algorithm to use for the pca data\n",
        "# Compare Algorithms\n",
        "\n",
        "#1 hidden layer used on pca_Embedded dataset\n",
        "def creating_model1(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #one hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #2 hidden layer used on pca_Embedded dataset\n",
        "def creating_model2(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))              #two hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #3 hidden layer used on pca_Embedded dataset\n",
        "def creating_model3(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(30, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "\n",
        "  #4 hidden layer used on pca_Embedded dataset\n",
        "def creating_model4(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #four hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #5 hidden layer used on pca_Embedded dataset\n",
        "def creating_model5(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #five hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model6(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Model Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of HL(Hidden Layer)')\n",
        "plt.ylabel('Model Accuracy')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 799us/step - loss: 0.1601 - accuracy: 0.7798\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 828us/step - loss: 0.1169 - accuracy: 0.8290\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1143 - accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 829us/step - loss: 0.1109 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 814us/step - loss: 0.1127 - accuracy: 0.8342\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1114 - accuracy: 0.8360\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 830us/step - loss: 0.1115 - accuracy: 0.8381\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1094 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 805us/step - loss: 0.1105 - accuracy: 0.8363\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 813us/step - loss: 0.1110 - accuracy: 0.8383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 823us/step - loss: 0.1545 - accuracy: 0.7840\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 769us/step - loss: 0.1131 - accuracy: 0.8359\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 798us/step - loss: 0.1088 - accuracy: 0.8419\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1098 - accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 844us/step - loss: 0.1119 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 852us/step - loss: 0.1080 - accuracy: 0.8418\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 817us/step - loss: 0.1086 - accuracy: 0.8397\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 795us/step - loss: 0.1076 - accuracy: 0.8439\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1108 - accuracy: 0.8379\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 833us/step - loss: 0.1101 - accuracy: 0.8394\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 795us/step - loss: 0.1583 - accuracy: 0.7714\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 793us/step - loss: 0.1144 - accuracy: 0.8358\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 792us/step - loss: 0.1136 - accuracy: 0.8338\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 791us/step - loss: 0.1145 - accuracy: 0.8321\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 813us/step - loss: 0.1119 - accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 809us/step - loss: 0.1105 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 842us/step - loss: 0.1118 - accuracy: 0.8350\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1096 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 841us/step - loss: 0.1102 - accuracy: 0.8383\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 776us/step - loss: 0.1106 - accuracy: 0.8374\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 797us/step - loss: 0.1761 - accuracy: 0.7440\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 795us/step - loss: 0.1163 - accuracy: 0.8299\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 793us/step - loss: 0.1155 - accuracy: 0.8310\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 795us/step - loss: 0.1131 - accuracy: 0.8326\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1150 - accuracy: 0.8318\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 823us/step - loss: 0.1125 - accuracy: 0.8349\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 814us/step - loss: 0.1122 - accuracy: 0.8344\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 807us/step - loss: 0.1116 - accuracy: 0.8343\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1110 - accuracy: 0.8362\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 814us/step - loss: 0.1114 - accuracy: 0.8379\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 809us/step - loss: 0.1730 - accuracy: 0.7616\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 787us/step - loss: 0.1161 - accuracy: 0.8326\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 800us/step - loss: 0.1135 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 812us/step - loss: 0.1120 - accuracy: 0.8369\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 803us/step - loss: 0.1106 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1105 - accuracy: 0.8395\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 836us/step - loss: 0.1104 - accuracy: 0.8398\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 838us/step - loss: 0.1116 - accuracy: 0.8368\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 796us/step - loss: 0.1097 - accuracy: 0.8395\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 832us/step - loss: 0.1103 - accuracy: 0.8379\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 811us/step - loss: 0.1715 - accuracy: 0.7570\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 814us/step - loss: 0.1166 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 838us/step - loss: 0.1125 - accuracy: 0.8337\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 812us/step - loss: 0.1127 - accuracy: 0.8338\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 856us/step - loss: 0.1110 - accuracy: 0.8349\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1130 - accuracy: 0.8335\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1090 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 809us/step - loss: 0.1101 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1078 - accuracy: 0.8425\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1084 - accuracy: 0.8432\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 773us/step - loss: 0.1708 - accuracy: 0.7715\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 800us/step - loss: 0.1118 - accuracy: 0.8384\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 804us/step - loss: 0.1126 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 842us/step - loss: 0.1119 - accuracy: 0.8345\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1127 - accuracy: 0.8357\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 809us/step - loss: 0.1127 - accuracy: 0.8332\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 811us/step - loss: 0.1093 - accuracy: 0.8398\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 807us/step - loss: 0.1139 - accuracy: 0.8348\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 833us/step - loss: 0.1109 - accuracy: 0.8383\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 848us/step - loss: 0.1128 - accuracy: 0.8328\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 772us/step - loss: 0.1739 - accuracy: 0.7589\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 806us/step - loss: 0.1140 - accuracy: 0.8352\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 836us/step - loss: 0.1146 - accuracy: 0.8315\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 841us/step - loss: 0.1108 - accuracy: 0.8363\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 807us/step - loss: 0.1104 - accuracy: 0.8348\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1114 - accuracy: 0.8345\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 860us/step - loss: 0.1120 - accuracy: 0.8353\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 819us/step - loss: 0.1115 - accuracy: 0.8370\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 820us/step - loss: 0.1120 - accuracy: 0.8365\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 812us/step - loss: 0.1103 - accuracy: 0.8391\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 817us/step - loss: 0.1751 - accuracy: 0.7528\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 827us/step - loss: 0.1162 - accuracy: 0.8314\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1129 - accuracy: 0.8350\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 807us/step - loss: 0.1115 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 803us/step - loss: 0.1113 - accuracy: 0.8365\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1124 - accuracy: 0.8355\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1131 - accuracy: 0.8328\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 811us/step - loss: 0.1118 - accuracy: 0.8355\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 805us/step - loss: 0.1099 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 845us/step - loss: 0.1122 - accuracy: 0.8374\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 763us/step - loss: 0.1754 - accuracy: 0.7380\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1192 - accuracy: 0.8291\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 818us/step - loss: 0.1144 - accuracy: 0.8321\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1123 - accuracy: 0.8369\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 838us/step - loss: 0.1125 - accuracy: 0.8345\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 813us/step - loss: 0.1095 - accuracy: 0.8411\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 808us/step - loss: 0.1118 - accuracy: 0.8383\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1085 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 850us/step - loss: 0.1112 - accuracy: 0.8375\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 808us/step - loss: 0.1076 - accuracy: 0.8434\n",
            "1 HL: 0.837013 (0.006086)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 826us/step - loss: 0.1582 - accuracy: 0.7864\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 846us/step - loss: 0.1150 - accuracy: 0.8319\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 839us/step - loss: 0.1124 - accuracy: 0.8342\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 850us/step - loss: 0.1137 - accuracy: 0.8320\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 842us/step - loss: 0.1111 - accuracy: 0.8392\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 870us/step - loss: 0.1119 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 899us/step - loss: 0.1111 - accuracy: 0.8384\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 900us/step - loss: 0.1094 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1092 - accuracy: 0.8373\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1099 - accuracy: 0.8406\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 875us/step - loss: 0.1647 - accuracy: 0.7645\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1138 - accuracy: 0.8347\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1145 - accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 881us/step - loss: 0.1114 - accuracy: 0.8392\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1101 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 869us/step - loss: 0.1109 - accuracy: 0.8380\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1081 - accuracy: 0.8391\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1085 - accuracy: 0.8399\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 870us/step - loss: 0.1072 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 882us/step - loss: 0.1082 - accuracy: 0.8411\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 821us/step - loss: 0.1642 - accuracy: 0.7723\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1165 - accuracy: 0.8310\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 881us/step - loss: 0.1130 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 828us/step - loss: 0.1150 - accuracy: 0.8311\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1140 - accuracy: 0.8333\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1132 - accuracy: 0.8346\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 842us/step - loss: 0.1118 - accuracy: 0.8360\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 842us/step - loss: 0.1145 - accuracy: 0.8340\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 863us/step - loss: 0.1100 - accuracy: 0.8409\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1121 - accuracy: 0.8368\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 847us/step - loss: 0.1574 - accuracy: 0.7802\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 858us/step - loss: 0.1139 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1132 - accuracy: 0.8348\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 850us/step - loss: 0.1110 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 835us/step - loss: 0.1126 - accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 875us/step - loss: 0.1128 - accuracy: 0.8367\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 835us/step - loss: 0.1124 - accuracy: 0.8355\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 875us/step - loss: 0.1099 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 829us/step - loss: 0.1115 - accuracy: 0.8354\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1129 - accuracy: 0.8335\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 839us/step - loss: 0.1553 - accuracy: 0.7870\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 846us/step - loss: 0.1112 - accuracy: 0.8400\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 910us/step - loss: 0.1102 - accuracy: 0.8401\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 880us/step - loss: 0.1087 - accuracy: 0.8409\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 922us/step - loss: 0.1082 - accuracy: 0.8423\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 889us/step - loss: 0.1100 - accuracy: 0.8398\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1075 - accuracy: 0.8421\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 881us/step - loss: 0.1076 - accuracy: 0.8435\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 863us/step - loss: 0.1079 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 876us/step - loss: 0.1094 - accuracy: 0.8372\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 847us/step - loss: 0.1622 - accuracy: 0.7767\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 897us/step - loss: 0.1130 - accuracy: 0.8378\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 915us/step - loss: 0.1116 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 881us/step - loss: 0.1127 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1119 - accuracy: 0.8359\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 860us/step - loss: 0.1105 - accuracy: 0.8379\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1106 - accuracy: 0.8384\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 907us/step - loss: 0.1098 - accuracy: 0.8423\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 867us/step - loss: 0.1097 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1090 - accuracy: 0.8432\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 833us/step - loss: 0.1625 - accuracy: 0.7766\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1148 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1098 - accuracy: 0.8413\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 915us/step - loss: 0.1111 - accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 899us/step - loss: 0.1091 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 936us/step - loss: 0.1075 - accuracy: 0.8434\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1089 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1104 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 985us/step - loss: 0.1084 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1104 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 905us/step - loss: 0.1562 - accuracy: 0.7888\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 902us/step - loss: 0.1148 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8371\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 955us/step - loss: 0.1103 - accuracy: 0.8396\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8413\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 899us/step - loss: 0.1101 - accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1111 - accuracy: 0.8387\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 887us/step - loss: 0.1080 - accuracy: 0.8426\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 874us/step - loss: 0.1106 - accuracy: 0.8375\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 890us/step - loss: 0.1079 - accuracy: 0.8412\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 854us/step - loss: 0.1638 - accuracy: 0.7802\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1152 - accuracy: 0.8296\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1108 - accuracy: 0.8375\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 914us/step - loss: 0.1115 - accuracy: 0.8359\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1109 - accuracy: 0.8372\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1123 - accuracy: 0.8360\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 906us/step - loss: 0.1127 - accuracy: 0.8355\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 898us/step - loss: 0.1114 - accuracy: 0.8368\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1100 - accuracy: 0.8386\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 880us/step - loss: 0.1147 - accuracy: 0.8303\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 859us/step - loss: 0.1614 - accuracy: 0.7758\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1162 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 895us/step - loss: 0.1132 - accuracy: 0.8338\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1141 - accuracy: 0.8313\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 872us/step - loss: 0.1122 - accuracy: 0.8355\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 903us/step - loss: 0.1158 - accuracy: 0.8296\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1131 - accuracy: 0.8344\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 892us/step - loss: 0.1125 - accuracy: 0.8349\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 890us/step - loss: 0.1123 - accuracy: 0.8373\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 929us/step - loss: 0.1120 - accuracy: 0.8350\n",
            "2 HL: 0.839202 (0.004050)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 909us/step - loss: 0.1526 - accuracy: 0.7816\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 945us/step - loss: 0.1138 - accuracy: 0.8369\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 878us/step - loss: 0.1110 - accuracy: 0.8390\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1114 - accuracy: 0.8351\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 968us/step - loss: 0.1090 - accuracy: 0.8426\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 929us/step - loss: 0.1100 - accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 926us/step - loss: 0.1093 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 921us/step - loss: 0.1080 - accuracy: 0.8439\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 997us/step - loss: 0.1069 - accuracy: 0.8470\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1087 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 918us/step - loss: 0.1507 - accuracy: 0.7855\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1151 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1128 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 989us/step - loss: 0.1112 - accuracy: 0.8376\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 960us/step - loss: 0.1147 - accuracy: 0.8321\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1110 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 945us/step - loss: 0.1087 - accuracy: 0.8405\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 956us/step - loss: 0.1118 - accuracy: 0.8350\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 944us/step - loss: 0.1092 - accuracy: 0.8401\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8394\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 892us/step - loss: 0.1530 - accuracy: 0.7862\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 923us/step - loss: 0.1116 - accuracy: 0.8380\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 944us/step - loss: 0.1121 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 938us/step - loss: 0.1123 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 985us/step - loss: 0.1113 - accuracy: 0.8379\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1098 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1104 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 942us/step - loss: 0.1073 - accuracy: 0.8443\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1072 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 926us/step - loss: 0.1086 - accuracy: 0.8418\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 909us/step - loss: 0.1496 - accuracy: 0.7887\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 936us/step - loss: 0.1135 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 950us/step - loss: 0.1140 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8399\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1104 - accuracy: 0.8378\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8368\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 963us/step - loss: 0.1093 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 944us/step - loss: 0.1086 - accuracy: 0.8404\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 962us/step - loss: 0.1088 - accuracy: 0.8432\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 947us/step - loss: 0.1094 - accuracy: 0.8387\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 906us/step - loss: 0.1477 - accuracy: 0.7927\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 910us/step - loss: 0.1144 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 926us/step - loss: 0.1099 - accuracy: 0.8423\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 965us/step - loss: 0.1083 - accuracy: 0.8430\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 952us/step - loss: 0.1084 - accuracy: 0.8432\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 912us/step - loss: 0.1084 - accuracy: 0.8424\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 948us/step - loss: 0.1077 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 954us/step - loss: 0.1059 - accuracy: 0.8462\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 942us/step - loss: 0.1091 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1083 - accuracy: 0.8401\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 920us/step - loss: 0.1525 - accuracy: 0.7906\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 954us/step - loss: 0.1132 - accuracy: 0.8373\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 968us/step - loss: 0.1123 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1108 - accuracy: 0.8411\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 958us/step - loss: 0.1103 - accuracy: 0.8413\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1101 - accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 965us/step - loss: 0.1094 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 990us/step - loss: 0.1083 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 990us/step - loss: 0.1090 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 948us/step - loss: 0.1095 - accuracy: 0.8406\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 925us/step - loss: 0.1425 - accuracy: 0.8021\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 920us/step - loss: 0.1149 - accuracy: 0.8306\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1145 - accuracy: 0.8337\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 992us/step - loss: 0.1127 - accuracy: 0.8357\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1109 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1102 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 996us/step - loss: 0.1111 - accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1089 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1076 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 977us/step - loss: 0.1087 - accuracy: 0.8416\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 884us/step - loss: 0.1462 - accuracy: 0.7951\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1134 - accuracy: 0.8358\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1087 - accuracy: 0.8393\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 949us/step - loss: 0.1106 - accuracy: 0.8389\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1098 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 947us/step - loss: 0.1087 - accuracy: 0.8423\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 947us/step - loss: 0.1085 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 942us/step - loss: 0.1075 - accuracy: 0.8428\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 970us/step - loss: 0.1094 - accuracy: 0.8379\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1077 - accuracy: 0.8415\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 972us/step - loss: 0.1574 - accuracy: 0.7779\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1149 - accuracy: 0.8317\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8358\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 999us/step - loss: 0.1120 - accuracy: 0.8380\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 984us/step - loss: 0.1128 - accuracy: 0.8337\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1113 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 965us/step - loss: 0.1066 - accuracy: 0.8441\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8390\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 943us/step - loss: 0.1087 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 928us/step - loss: 0.1114 - accuracy: 0.8377\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 925us/step - loss: 0.1581 - accuracy: 0.7833\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 928us/step - loss: 0.1120 - accuracy: 0.8384\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1129 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1119 - accuracy: 0.8382\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1107 - accuracy: 0.8390\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 967us/step - loss: 0.1092 - accuracy: 0.8406\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 944us/step - loss: 0.1104 - accuracy: 0.8386\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 930us/step - loss: 0.1070 - accuracy: 0.8442\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1095 - accuracy: 0.8389\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 962us/step - loss: 0.1092 - accuracy: 0.8428\n",
            "3 HL: 0.839235 (0.008830)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 975us/step - loss: 0.1512 - accuracy: 0.7918\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 954us/step - loss: 0.1165 - accuracy: 0.8271\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 970us/step - loss: 0.1132 - accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8389\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8367\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1096 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1090 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 955us/step - loss: 0.1069 - accuracy: 0.8398\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1091 - accuracy: 0.8405\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 991us/step - loss: 0.1508 - accuracy: 0.7879\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1155 - accuracy: 0.8323\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 969us/step - loss: 0.1130 - accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8402\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1099 - accuracy: 0.8425\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8366\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8364\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 946us/step - loss: 0.1515 - accuracy: 0.7856\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 978us/step - loss: 0.1142 - accuracy: 0.8351\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1148 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8381\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8435\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8429\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8435\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8456\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 993us/step - loss: 0.1490 - accuracy: 0.7871\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.8290\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8422\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 994us/step - loss: 0.1112 - accuracy: 0.8394\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1118 - accuracy: 0.8366\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8384\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8447\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1442 - accuracy: 0.7944\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8351\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8330\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8396\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8411\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8399\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8441\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8442\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8389\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1428 - accuracy: 0.7981\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8356\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1136 - accuracy: 0.8332\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8402\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8379\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8361\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8388\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8401\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 999us/step - loss: 0.1450 - accuracy: 0.7949\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8345\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 961us/step - loss: 0.1124 - accuracy: 0.8345\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8399\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 999us/step - loss: 0.1091 - accuracy: 0.8420\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8463\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8404\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8432\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8417\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1493 - accuracy: 0.7929\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8327\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8337\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8347\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8391\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8407\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8441\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8391\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1463 - accuracy: 0.7890\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8353\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8365\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8386\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8435\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8410\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8407\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8430\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1460 - accuracy: 0.7926\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8335\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8380\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8348\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8402\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8411\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8423\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8443\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8372\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8400\n",
            "4 HL: 0.841390 (0.004275)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1435 - accuracy: 0.7980\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8328\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8371\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8435\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8408\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8445\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8402\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8420\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1067 - accuracy: 0.8448\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1416 - accuracy: 0.7937\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1147 - accuracy: 0.8316\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8348\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8407\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8418\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8380\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8392\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1449 - accuracy: 0.7959\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8323\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8346\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8386\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8398\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8366\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8366\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8407\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1388 - accuracy: 0.8014\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8325\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8411\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8393\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8433\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8395\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8401\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1459 - accuracy: 0.7935\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8358\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8383\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8349\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8415\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8440\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1428 - accuracy: 0.7988\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8379\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8414\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8396\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8431\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1074 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8375\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8425\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1459 - accuracy: 0.7963\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8355\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8351\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8354\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8388\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8383\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8399\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8370\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1069 - accuracy: 0.8430\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1519 - accuracy: 0.7920\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.8291\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8365\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8360\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8424\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8383\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8401\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8377\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1459 - accuracy: 0.7938\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8317\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8311\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8338\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8328\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8373\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8360\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8404\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8380\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8389\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1396 - accuracy: 0.8018\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8355\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8388\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8397\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8349\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8372\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8447\n",
            "5 HL: 0.840959 (0.005570)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1433 - accuracy: 0.7993\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8331\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8430\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8389\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1067 - accuracy: 0.8468\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8405\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1411 - accuracy: 0.8037\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8323\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8375\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8399\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8362\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8371\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8386\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8398\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8407\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1481 - accuracy: 0.7944\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1164 - accuracy: 0.8316\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8355\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8326\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8344\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8374\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8351\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8351\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1445 - accuracy: 0.7932\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8349\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8355\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8408\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8372\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8431\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1065 - accuracy: 0.8455\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1064 - accuracy: 0.8464\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8437\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1512 - accuracy: 0.7874\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8333\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8397\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8392\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8353\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8421\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1064 - accuracy: 0.8463\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8436\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8367\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1442 - accuracy: 0.7960\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1150 - accuracy: 0.8349\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8410\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8426\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8435\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8426\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1447 - accuracy: 0.7956\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8367\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8428\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8406\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8429\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8445\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8370\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1075 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1387 - accuracy: 0.8075\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1163 - accuracy: 0.8300\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8370\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8401\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8388\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8437\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8399\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8421\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1427 - accuracy: 0.7940\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8373\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8348\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8394\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8401\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8427\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8435\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8450\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8428\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1451 - accuracy: 0.7940\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1168 - accuracy: 0.8295\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8391\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8374\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8350\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8421\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8442\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8427\n",
            "6 HL: 0.841257 (0.005902)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5659rgsANwE0",
        "outputId": "92ace917-3c88-4a1f-bf59-44ac1ba3ed62"
      },
      "source": [
        "#step8c for pca_Filter dataset \n",
        "#checking which algorithm to use for the pca data\n",
        "# Compare Algorithms\n",
        "\n",
        "#1 hidden layer used on pca_Filter dataset\n",
        "def creating_model1(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #one hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #2 hidden layer used on pca_Filter dataset\n",
        "def creating_model2(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))              #two hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #3 hidden layer used on pca_Filter dataset\n",
        "def creating_model3(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(30, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "\n",
        "  #4 hidden layer used on pca_Filter dataset\n",
        "def creating_model4(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #four hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #5 hidden layer used on pca_Filter dataset\n",
        "def creating_model5(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #five hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #6 hidden layer used on pca_Filter dataset\n",
        "def creating_model6(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_selectKbest_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Model Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of HL(Hidden Layer)')\n",
        "plt.ylabel('Model Accuracy')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 774us/step - loss: 0.1650 - accuracy: 0.7674\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 793us/step - loss: 0.1310 - accuracy: 0.8059\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1262 - accuracy: 0.8142\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1237 - accuracy: 0.8154\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 828us/step - loss: 0.1232 - accuracy: 0.8146\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 814us/step - loss: 0.1206 - accuracy: 0.8205\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 806us/step - loss: 0.1188 - accuracy: 0.8257\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 797us/step - loss: 0.1177 - accuracy: 0.8288\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 803us/step - loss: 0.1181 - accuracy: 0.8301\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1177 - accuracy: 0.8292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 767us/step - loss: 0.1681 - accuracy: 0.7614\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1287 - accuracy: 0.8092\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 797us/step - loss: 0.1214 - accuracy: 0.8188\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 804us/step - loss: 0.1215 - accuracy: 0.8171\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1182 - accuracy: 0.8225\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 810us/step - loss: 0.1161 - accuracy: 0.8264\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1155 - accuracy: 0.8273\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 835us/step - loss: 0.1201 - accuracy: 0.8241\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1179 - accuracy: 0.8235\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 839us/step - loss: 0.1159 - accuracy: 0.8297\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 805us/step - loss: 0.1689 - accuracy: 0.7551\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 829us/step - loss: 0.1263 - accuracy: 0.8158\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1207 - accuracy: 0.8205\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 814us/step - loss: 0.1188 - accuracy: 0.8224\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1175 - accuracy: 0.8295\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1179 - accuracy: 0.8264\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 806us/step - loss: 0.1155 - accuracy: 0.8305\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 806us/step - loss: 0.1190 - accuracy: 0.8234\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 803us/step - loss: 0.1168 - accuracy: 0.8273\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1170 - accuracy: 0.8294\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 781us/step - loss: 0.1701 - accuracy: 0.7543\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 792us/step - loss: 0.1286 - accuracy: 0.8064\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1260 - accuracy: 0.8070\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 816us/step - loss: 0.1234 - accuracy: 0.8116\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 818us/step - loss: 0.1234 - accuracy: 0.8124\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 772us/step - loss: 0.1198 - accuracy: 0.8178\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 813us/step - loss: 0.1217 - accuracy: 0.8159\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1194 - accuracy: 0.8233\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 838us/step - loss: 0.1200 - accuracy: 0.8232\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 812us/step - loss: 0.1173 - accuracy: 0.8251\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 761us/step - loss: 0.1591 - accuracy: 0.7655\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 802us/step - loss: 0.1300 - accuracy: 0.8063\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 808us/step - loss: 0.1237 - accuracy: 0.8189\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1230 - accuracy: 0.8242\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 818us/step - loss: 0.1214 - accuracy: 0.8231\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 822us/step - loss: 0.1207 - accuracy: 0.8252\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1192 - accuracy: 0.8253\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1202 - accuracy: 0.8221\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 808us/step - loss: 0.1207 - accuracy: 0.8225\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 805us/step - loss: 0.1214 - accuracy: 0.8219\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 804us/step - loss: 0.1774 - accuracy: 0.7438\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 819us/step - loss: 0.1296 - accuracy: 0.8049\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 841us/step - loss: 0.1257 - accuracy: 0.8096\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1216 - accuracy: 0.8158\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 811us/step - loss: 0.1192 - accuracy: 0.8196\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1184 - accuracy: 0.8232\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1196 - accuracy: 0.8244\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 817us/step - loss: 0.1173 - accuracy: 0.8256\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 871us/step - loss: 0.1187 - accuracy: 0.8256\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 870us/step - loss: 0.1188 - accuracy: 0.8252\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 870us/step - loss: 0.1611 - accuracy: 0.7770\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 846us/step - loss: 0.1288 - accuracy: 0.8114\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 806us/step - loss: 0.1250 - accuracy: 0.8122\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 817us/step - loss: 0.1211 - accuracy: 0.8175\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1208 - accuracy: 0.8211\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1190 - accuracy: 0.8259\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 856us/step - loss: 0.1159 - accuracy: 0.8281\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1200 - accuracy: 0.8243\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 812us/step - loss: 0.1169 - accuracy: 0.8287\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 796us/step - loss: 0.1182 - accuracy: 0.8277\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 772us/step - loss: 0.1613 - accuracy: 0.7684\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 769us/step - loss: 0.1254 - accuracy: 0.8129\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 799us/step - loss: 0.1202 - accuracy: 0.8205\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 773us/step - loss: 0.1202 - accuracy: 0.8198\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 779us/step - loss: 0.1169 - accuracy: 0.8271\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 802us/step - loss: 0.1176 - accuracy: 0.8259\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 794us/step - loss: 0.1169 - accuracy: 0.8294\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 824us/step - loss: 0.1192 - accuracy: 0.8232\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 802us/step - loss: 0.1168 - accuracy: 0.8276\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 813us/step - loss: 0.1155 - accuracy: 0.8272\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 793us/step - loss: 0.1617 - accuracy: 0.7707\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 848us/step - loss: 0.1294 - accuracy: 0.8091\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 794us/step - loss: 0.1244 - accuracy: 0.8140\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 795us/step - loss: 0.1217 - accuracy: 0.8192\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1213 - accuracy: 0.8161\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 832us/step - loss: 0.1200 - accuracy: 0.8220\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 836us/step - loss: 0.1185 - accuracy: 0.8230\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 800us/step - loss: 0.1180 - accuracy: 0.8262\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 783us/step - loss: 0.1162 - accuracy: 0.8247\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1175 - accuracy: 0.8265\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 819us/step - loss: 0.1645 - accuracy: 0.7654\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 795us/step - loss: 0.1270 - accuracy: 0.8066\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 800us/step - loss: 0.1219 - accuracy: 0.8129\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 844us/step - loss: 0.1207 - accuracy: 0.8205\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 809us/step - loss: 0.1208 - accuracy: 0.8207\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1188 - accuracy: 0.8219\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 814us/step - loss: 0.1186 - accuracy: 0.8290\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 799us/step - loss: 0.1191 - accuracy: 0.8242\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 852us/step - loss: 0.1184 - accuracy: 0.8255\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 869us/step - loss: 0.1156 - accuracy: 0.8295\n",
            "1 HL: 0.824183 (0.006899)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 841us/step - loss: 0.1589 - accuracy: 0.7786\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1247 - accuracy: 0.8107\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1205 - accuracy: 0.8179\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 901us/step - loss: 0.1189 - accuracy: 0.8243\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 922us/step - loss: 0.1170 - accuracy: 0.8270\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1187 - accuracy: 0.8211\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 858us/step - loss: 0.1163 - accuracy: 0.8294\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 874us/step - loss: 0.1162 - accuracy: 0.8289\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 842us/step - loss: 0.1155 - accuracy: 0.8332\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 805us/step - loss: 0.1138 - accuracy: 0.8333\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 812us/step - loss: 0.1598 - accuracy: 0.7778\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 794us/step - loss: 0.1225 - accuracy: 0.8182\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1196 - accuracy: 0.8189\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1205 - accuracy: 0.8191\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 850us/step - loss: 0.1193 - accuracy: 0.8213\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 833us/step - loss: 0.1178 - accuracy: 0.8247\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1192 - accuracy: 0.8241\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 833us/step - loss: 0.1160 - accuracy: 0.8291\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 864us/step - loss: 0.1154 - accuracy: 0.8322\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 869us/step - loss: 0.1147 - accuracy: 0.8316\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 825us/step - loss: 0.1589 - accuracy: 0.7777\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 811us/step - loss: 0.1229 - accuracy: 0.8159\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 797us/step - loss: 0.1206 - accuracy: 0.8184\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 828us/step - loss: 0.1199 - accuracy: 0.8233\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1168 - accuracy: 0.8287\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 878us/step - loss: 0.1173 - accuracy: 0.8285\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1168 - accuracy: 0.8301\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 878us/step - loss: 0.1180 - accuracy: 0.8273\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 846us/step - loss: 0.1152 - accuracy: 0.8298\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1137 - accuracy: 0.8330\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 843us/step - loss: 0.1677 - accuracy: 0.7648\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 832us/step - loss: 0.1273 - accuracy: 0.8039\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 864us/step - loss: 0.1221 - accuracy: 0.8111\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 841us/step - loss: 0.1194 - accuracy: 0.8174\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 889us/step - loss: 0.1180 - accuracy: 0.8224\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 876us/step - loss: 0.1198 - accuracy: 0.8174\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 839us/step - loss: 0.1178 - accuracy: 0.8240\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1190 - accuracy: 0.8193\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 880us/step - loss: 0.1145 - accuracy: 0.8312\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 868us/step - loss: 0.1155 - accuracy: 0.8282\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 839us/step - loss: 0.1567 - accuracy: 0.7813\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1255 - accuracy: 0.8105\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 900us/step - loss: 0.1210 - accuracy: 0.8142\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1207 - accuracy: 0.8193\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1216 - accuracy: 0.8209\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1173 - accuracy: 0.8224\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 874us/step - loss: 0.1179 - accuracy: 0.8247\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 908us/step - loss: 0.1159 - accuracy: 0.8303\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 891us/step - loss: 0.1186 - accuracy: 0.8253\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 879us/step - loss: 0.1170 - accuracy: 0.8253\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 826us/step - loss: 0.1674 - accuracy: 0.7654\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 868us/step - loss: 0.1223 - accuracy: 0.8149\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 850us/step - loss: 0.1177 - accuracy: 0.8226\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1194 - accuracy: 0.8235\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1186 - accuracy: 0.8276\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 881us/step - loss: 0.1155 - accuracy: 0.8295\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1168 - accuracy: 0.8281\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 879us/step - loss: 0.1141 - accuracy: 0.8308\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 891us/step - loss: 0.1148 - accuracy: 0.8319\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 889us/step - loss: 0.1132 - accuracy: 0.8332\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 866us/step - loss: 0.1618 - accuracy: 0.7718\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 856us/step - loss: 0.1250 - accuracy: 0.8106\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 863us/step - loss: 0.1199 - accuracy: 0.8183\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 867us/step - loss: 0.1185 - accuracy: 0.8238\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1173 - accuracy: 0.8242\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 857us/step - loss: 0.1174 - accuracy: 0.8251\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1148 - accuracy: 0.8304\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 872us/step - loss: 0.1151 - accuracy: 0.8294\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1139 - accuracy: 0.8314\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 868us/step - loss: 0.1128 - accuracy: 0.8335\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 832us/step - loss: 0.1549 - accuracy: 0.7739\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1215 - accuracy: 0.8170\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 880us/step - loss: 0.1215 - accuracy: 0.8160\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1188 - accuracy: 0.8208\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1186 - accuracy: 0.8223\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1152 - accuracy: 0.8283\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 875us/step - loss: 0.1193 - accuracy: 0.8213\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1168 - accuracy: 0.8261\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 839us/step - loss: 0.1149 - accuracy: 0.8285\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1163 - accuracy: 0.8243\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 850us/step - loss: 0.1531 - accuracy: 0.7852\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 857us/step - loss: 0.1239 - accuracy: 0.8124\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 907us/step - loss: 0.1214 - accuracy: 0.8181\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 875us/step - loss: 0.1200 - accuracy: 0.8200\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 882us/step - loss: 0.1205 - accuracy: 0.8189\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 906us/step - loss: 0.1186 - accuracy: 0.8251\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1156 - accuracy: 0.8323\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 950us/step - loss: 0.1140 - accuracy: 0.8343\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1141 - accuracy: 0.8345\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1162 - accuracy: 0.8325\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 878us/step - loss: 0.1681 - accuracy: 0.7559\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 881us/step - loss: 0.1275 - accuracy: 0.8093\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1214 - accuracy: 0.8158\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 860us/step - loss: 0.1207 - accuracy: 0.8175\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1202 - accuracy: 0.8194\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1183 - accuracy: 0.8258\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1164 - accuracy: 0.8266\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 874us/step - loss: 0.1178 - accuracy: 0.8254\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 869us/step - loss: 0.1159 - accuracy: 0.8289\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 867us/step - loss: 0.1131 - accuracy: 0.8331\n",
            "2 HL: 0.832306 (0.005817)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 901us/step - loss: 0.1611 - accuracy: 0.7731\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1245 - accuracy: 0.8129\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 902us/step - loss: 0.1205 - accuracy: 0.8197\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1179 - accuracy: 0.8275\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 917us/step - loss: 0.1178 - accuracy: 0.8240\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1176 - accuracy: 0.8275\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 981us/step - loss: 0.1170 - accuracy: 0.8272\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 903us/step - loss: 0.1149 - accuracy: 0.8322\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1147 - accuracy: 0.8302\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 936us/step - loss: 0.1142 - accuracy: 0.8289\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 873us/step - loss: 0.1580 - accuracy: 0.7783\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 911us/step - loss: 0.1222 - accuracy: 0.8192\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 958us/step - loss: 0.1217 - accuracy: 0.8182\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 984us/step - loss: 0.1173 - accuracy: 0.8275\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 911us/step - loss: 0.1189 - accuracy: 0.8280\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 962us/step - loss: 0.1168 - accuracy: 0.8309\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 939us/step - loss: 0.1178 - accuracy: 0.8310\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 939us/step - loss: 0.1185 - accuracy: 0.8273\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 939us/step - loss: 0.1165 - accuracy: 0.8300\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1155 - accuracy: 0.8313\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 918us/step - loss: 0.1496 - accuracy: 0.7836\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 913us/step - loss: 0.1202 - accuracy: 0.8151\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 948us/step - loss: 0.1175 - accuracy: 0.8235\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1165 - accuracy: 0.8261\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 935us/step - loss: 0.1167 - accuracy: 0.8285\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 954us/step - loss: 0.1143 - accuracy: 0.8310\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 910us/step - loss: 0.1133 - accuracy: 0.8336\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1131 - accuracy: 0.8348\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1126 - accuracy: 0.8335\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 947us/step - loss: 0.1120 - accuracy: 0.8337\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 886us/step - loss: 0.1609 - accuracy: 0.7656\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1228 - accuracy: 0.8139\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1208 - accuracy: 0.8149\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 950us/step - loss: 0.1179 - accuracy: 0.8234\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1163 - accuracy: 0.8227\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.8270\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 947us/step - loss: 0.1155 - accuracy: 0.8265\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 910us/step - loss: 0.1130 - accuracy: 0.8315\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1131 - accuracy: 0.8336\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 891us/step - loss: 0.1140 - accuracy: 0.8325\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 928us/step - loss: 0.1564 - accuracy: 0.7763\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 937us/step - loss: 0.1205 - accuracy: 0.8185\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1186 - accuracy: 0.8242\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 950us/step - loss: 0.1175 - accuracy: 0.8286\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 935us/step - loss: 0.1180 - accuracy: 0.8256\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 950us/step - loss: 0.1181 - accuracy: 0.8215\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 944us/step - loss: 0.1179 - accuracy: 0.8251\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1165 - accuracy: 0.8251\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 900us/step - loss: 0.1164 - accuracy: 0.8279\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 942us/step - loss: 0.1154 - accuracy: 0.8299\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 886us/step - loss: 0.1550 - accuracy: 0.7785\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 921us/step - loss: 0.1211 - accuracy: 0.8155\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1185 - accuracy: 0.8201\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 903us/step - loss: 0.1177 - accuracy: 0.8259\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 939us/step - loss: 0.1155 - accuracy: 0.8261\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 913us/step - loss: 0.1138 - accuracy: 0.8337\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 922us/step - loss: 0.1141 - accuracy: 0.8307\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1142 - accuracy: 0.8325\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1135 - accuracy: 0.8340\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1139 - accuracy: 0.8300\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 906us/step - loss: 0.1490 - accuracy: 0.7833\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1237 - accuracy: 0.8163\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 910us/step - loss: 0.1218 - accuracy: 0.8178\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 921us/step - loss: 0.1205 - accuracy: 0.8209\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 879us/step - loss: 0.1191 - accuracy: 0.8245\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 920us/step - loss: 0.1171 - accuracy: 0.8296\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1170 - accuracy: 0.8275\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 909us/step - loss: 0.1152 - accuracy: 0.8326\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1137 - accuracy: 0.8340\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 923us/step - loss: 0.1118 - accuracy: 0.8372\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 878us/step - loss: 0.1539 - accuracy: 0.7796\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1229 - accuracy: 0.8198\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 908us/step - loss: 0.1200 - accuracy: 0.8232\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 943us/step - loss: 0.1206 - accuracy: 0.8211\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 947us/step - loss: 0.1169 - accuracy: 0.8283\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1166 - accuracy: 0.8324\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 921us/step - loss: 0.1150 - accuracy: 0.8314\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 949us/step - loss: 0.1152 - accuracy: 0.8306\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 953us/step - loss: 0.1159 - accuracy: 0.8321\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 974us/step - loss: 0.1133 - accuracy: 0.8349\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 907us/step - loss: 0.1564 - accuracy: 0.7831\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1257 - accuracy: 0.8141\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 983us/step - loss: 0.1231 - accuracy: 0.8132\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 938us/step - loss: 0.1205 - accuracy: 0.8213\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 965us/step - loss: 0.1154 - accuracy: 0.8286\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 983us/step - loss: 0.1159 - accuracy: 0.8300\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 970us/step - loss: 0.1147 - accuracy: 0.8304\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 935us/step - loss: 0.1143 - accuracy: 0.8318\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1147 - accuracy: 0.8341\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 976us/step - loss: 0.1147 - accuracy: 0.8325\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 912us/step - loss: 0.1497 - accuracy: 0.7842\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 915us/step - loss: 0.1226 - accuracy: 0.8150\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 943us/step - loss: 0.1205 - accuracy: 0.8202\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 948us/step - loss: 0.1184 - accuracy: 0.8219\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 949us/step - loss: 0.1201 - accuracy: 0.8221\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 967us/step - loss: 0.1162 - accuracy: 0.8298\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 924us/step - loss: 0.1139 - accuracy: 0.8355\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 980us/step - loss: 0.1131 - accuracy: 0.8354\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 975us/step - loss: 0.1137 - accuracy: 0.8340\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8346\n",
            "3 HL: 0.832405 (0.006754)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 992us/step - loss: 0.1475 - accuracy: 0.7864\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1223 - accuracy: 0.8119\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.8208\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8238\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8261\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8271\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 988us/step - loss: 0.1151 - accuracy: 0.8304\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 978us/step - loss: 0.1131 - accuracy: 0.8341\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8358\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8373\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1546 - accuracy: 0.7782\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1223 - accuracy: 0.8096\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8216\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8296\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8352\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8339\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 947us/step - loss: 0.1122 - accuracy: 0.8351\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 998us/step - loss: 0.1148 - accuracy: 0.8322\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 967us/step - loss: 0.1125 - accuracy: 0.8353\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 988us/step - loss: 0.1126 - accuracy: 0.8349\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 944us/step - loss: 0.1556 - accuracy: 0.7784\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1233 - accuracy: 0.8120\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 989us/step - loss: 0.1208 - accuracy: 0.8149\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 978us/step - loss: 0.1177 - accuracy: 0.8211\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.8263\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1184 - accuracy: 0.8212\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8273\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 996us/step - loss: 0.1151 - accuracy: 0.8317\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8362\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8366\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 942us/step - loss: 0.1482 - accuracy: 0.7880\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.8166\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.8121\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 981us/step - loss: 0.1173 - accuracy: 0.8260\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8258\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8264\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.8283\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8328\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 988us/step - loss: 0.1152 - accuracy: 0.8302\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8321\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 988us/step - loss: 0.1495 - accuracy: 0.7801\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1208 - accuracy: 0.8171\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1189 - accuracy: 0.8220\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 999us/step - loss: 0.1175 - accuracy: 0.8228\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8287\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 987us/step - loss: 0.1167 - accuracy: 0.8245\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8330\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 956us/step - loss: 0.1142 - accuracy: 0.8290\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8323\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8304\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 958us/step - loss: 0.1497 - accuracy: 0.7769\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1212 - accuracy: 0.8195\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1211 - accuracy: 0.8168\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1201 - accuracy: 0.8194\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 996us/step - loss: 0.1168 - accuracy: 0.8259\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1164 - accuracy: 0.8251\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1000us/step - loss: 0.1142 - accuracy: 0.8335\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8327\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 990us/step - loss: 0.1139 - accuracy: 0.8334\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8386\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 948us/step - loss: 0.1479 - accuracy: 0.7873\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1251 - accuracy: 0.8127\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 996us/step - loss: 0.1198 - accuracy: 0.8223\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 962us/step - loss: 0.1172 - accuracy: 0.8275\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 977us/step - loss: 0.1147 - accuracy: 0.8333\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 981us/step - loss: 0.1149 - accuracy: 0.8341\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 981us/step - loss: 0.1115 - accuracy: 0.8343\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8404\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 996us/step - loss: 0.1123 - accuracy: 0.8377\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8357\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 962us/step - loss: 0.1446 - accuracy: 0.7957\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.8169\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 975us/step - loss: 0.1219 - accuracy: 0.8167\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.8210\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.8248\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 968us/step - loss: 0.1149 - accuracy: 0.8277\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 930us/step - loss: 0.1124 - accuracy: 0.8328\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1162 - accuracy: 0.8295\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 991us/step - loss: 0.1145 - accuracy: 0.8333\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8330\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 943us/step - loss: 0.1483 - accuracy: 0.7830\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1250 - accuracy: 0.8100\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1207 - accuracy: 0.8191\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1206 - accuracy: 0.8209\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.8183\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8271\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8281\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 990us/step - loss: 0.1145 - accuracy: 0.8296\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1147 - accuracy: 0.8333\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.8309\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 981us/step - loss: 0.1554 - accuracy: 0.7761\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 999us/step - loss: 0.1240 - accuracy: 0.8115\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 988us/step - loss: 0.1197 - accuracy: 0.8212\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1199 - accuracy: 0.8203\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8264\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 992us/step - loss: 0.1167 - accuracy: 0.8253\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8345\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 984us/step - loss: 0.1129 - accuracy: 0.8350\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8357\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8421\n",
            "4 HL: 0.834727 (0.011118)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1496 - accuracy: 0.7862\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1265 - accuracy: 0.8116\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1179 - accuracy: 0.8215\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.8237\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8273\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8315\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8302\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1157 - accuracy: 0.8305\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8302\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8305\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1487 - accuracy: 0.7849\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1221 - accuracy: 0.8128\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1202 - accuracy: 0.8130\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1201 - accuracy: 0.8141\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1179 - accuracy: 0.8192\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8259\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.8282\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8254\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8262\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8292\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1464 - accuracy: 0.7844\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1252 - accuracy: 0.8112\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1174 - accuracy: 0.8216\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1184 - accuracy: 0.8189\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1172 - accuracy: 0.8267\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1147 - accuracy: 0.8324\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8352\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.8276\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8352\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8343\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1453 - accuracy: 0.7904\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1198 - accuracy: 0.8217\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8268\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8263\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8277\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8333\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8314\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8302\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8364\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8340\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1526 - accuracy: 0.7816\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.8144\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1200 - accuracy: 0.8208\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1154 - accuracy: 0.8278\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8307\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8329\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8295\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8344\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8347\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8314\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1442 - accuracy: 0.7888\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.8155\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.8259\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8333\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8360\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8294\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8352\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8350\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8319\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8357\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 996us/step - loss: 0.1490 - accuracy: 0.7852\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1213 - accuracy: 0.8159\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1200 - accuracy: 0.8201\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8252\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8287\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8338\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8305\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8328\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8321\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8361\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1451 - accuracy: 0.7879\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1206 - accuracy: 0.8210\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1180 - accuracy: 0.8232\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.8293\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1152 - accuracy: 0.8297\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8357\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8351\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.8299\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1124 - accuracy: 0.8376\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8349\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1502 - accuracy: 0.7752\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1236 - accuracy: 0.8076\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1179 - accuracy: 0.8196\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.8214\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.8234\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8311\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8315\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1155 - accuracy: 0.8293\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8315\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8327\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1513 - accuracy: 0.7863\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1237 - accuracy: 0.8146\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1185 - accuracy: 0.8229\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8274\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8318\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8326\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8275\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8339\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8358\n",
            "5 HL: 0.836947 (0.004482)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1424 - accuracy: 0.7957\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1237 - accuracy: 0.8106\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1210 - accuracy: 0.8149\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1185 - accuracy: 0.8177\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1202 - accuracy: 0.8181\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.8264\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8339\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8337\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8323\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8317\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1472 - accuracy: 0.7878\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1265 - accuracy: 0.8132\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1175 - accuracy: 0.8253\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1177 - accuracy: 0.8252\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1150 - accuracy: 0.8317\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8372\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.8327\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8370\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1512 - accuracy: 0.7818\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1204 - accuracy: 0.8182\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1183 - accuracy: 0.8213\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1162 - accuracy: 0.8242\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1160 - accuracy: 0.8276\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1155 - accuracy: 0.8280\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8319\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8362\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1147 - accuracy: 0.8295\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8316\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1495 - accuracy: 0.7845\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1224 - accuracy: 0.8133\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1215 - accuracy: 0.8117\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1199 - accuracy: 0.8172\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1174 - accuracy: 0.8245\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1169 - accuracy: 0.8274\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1137 - accuracy: 0.8345\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8271\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.8322\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8355\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1470 - accuracy: 0.7882\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1232 - accuracy: 0.8141\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1201 - accuracy: 0.8180\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1171 - accuracy: 0.8198\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1176 - accuracy: 0.8207\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8301\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8254\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1154 - accuracy: 0.8282\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8321\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8304\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1449 - accuracy: 0.7802\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1213 - accuracy: 0.8199\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1183 - accuracy: 0.8250\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.8329\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1137 - accuracy: 0.8353\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8334\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8360\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8327\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8380\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8355\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1414 - accuracy: 0.7948\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1211 - accuracy: 0.8165\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1175 - accuracy: 0.8232\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1164 - accuracy: 0.8267\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8339\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8323\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8362\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1138 - accuracy: 0.8375\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8364\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8351\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1447 - accuracy: 0.7937\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1223 - accuracy: 0.8155\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1171 - accuracy: 0.8214\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1162 - accuracy: 0.8287\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8333\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1156 - accuracy: 0.8287\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8359\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8360\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8368\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8351\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1439 - accuracy: 0.7907\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1247 - accuracy: 0.8049\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1187 - accuracy: 0.8167\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1215 - accuracy: 0.8096\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1184 - accuracy: 0.8197\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1185 - accuracy: 0.8200\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1156 - accuracy: 0.8275\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.8271\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8286\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8314\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1507 - accuracy: 0.7768\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1243 - accuracy: 0.8140\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1212 - accuracy: 0.8176\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1196 - accuracy: 0.8190\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1196 - accuracy: 0.8180\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8278\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1164 - accuracy: 0.8276\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8301\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.8343\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8324\n",
            "6 HL: 0.829786 (0.008251)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-D11XVvQCom"
      },
      "source": [
        "Step 9 Hypertuning the parameters Now\n",
        "\n",
        "I chose pca_embedded dataset as it has high accuracy(around 85.0 to 85.5 % ) as shown in the graph with 6 hidden layer neural network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdxmNEKJRJz9",
        "outputId": "8d1bccfa-16a4-4af6-d9bc-d316bc1d5681"
      },
      "source": [
        "# Step 9a Hypertuning the model parameter now\n",
        "# pca with Embedded dataset. This dataset is chosen by K fold analysis with 6 hidden layers\n",
        "#i will see which dataset gives better result on this neural network\n",
        "from tensorflow.python.keras import regularizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimizers = ['WAME']#learning_rate=0.001\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "\n",
        "\n",
        "#6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=creating_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_embedded_train_features, Y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.842484 using {'batch_size': 40, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1cppHU-Y916",
        "outputId": "c4159eeb-327b-4d22-d62a-15b8835847b4"
      },
      "source": [
        "# Step 9B Hypertuning the LEARNING RATES AS WELL To see the best learning rate for the model\n",
        "# pca with Embedded dataset. This dataset is chosen by K fold analysis with 6 hidden layers\n",
        "#i will see which dataset gives better result on this neural network\n",
        "from tensorflow.python.keras import regularizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimizers = ['WAME', 'WAME(learning_rate=0.01)', 'WAME(learning_rate=0.0001)', 'WAME(learning_rate=0.00001)'] # By default the learning rate is 0.001 in class WAME\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "\n",
        "\n",
        "#6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=creating_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_embedded_train_features, Y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.841490 using {'batch_size': 40, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME(learning_rate=0.0001)'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vArl8tN_jdcJ"
      },
      "source": [
        "Step 10 Evaluating and Testing the result on Testing dataset now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw3IpQK2ivAc",
        "outputId": "ac7e49d4-f045-4afa-acf7-065c3f62b0ce"
      },
      "source": [
        "# the accuracy for the default learning rates 0.001 is 83%\n",
        "#STEP 10 Testing \n",
        "\n",
        "\n",
        "#6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('ANN WITH 6 HIDDEN LAYERS', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print('\\n')\n",
        "  print(\"Model : \", name)\n",
        "  model_build_time = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  print('\\n')\n",
        "  print(\"Time to build ANN Model in seconds : %.4f \" % round(time.time()-model_build_time,4))\n",
        "  print('\\n')\n",
        "  start_time = time.time()\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test the ANN Model in seconds : %.4f \" % round(time.time()-start_time,4))\n",
        "  print('\\n')\n",
        "  matrix = confusion_matrix(Y_test, y_pred)\n",
        "  print(\"Time elapsed in seconds: %.4f \" % round(time.time()-model_build_time, 4))\n",
        "  print('\\n')\n",
        "  print(matrix)\n",
        "  print('\\n')\n",
        "  report = classification_report(Y_test, y_pred)\n",
        "  print(report)\n",
        "  print('\\n')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model :  ANN WITH 6 HIDDEN LAYERS\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1554 - accuracy: 0.7889\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1224 - accuracy: 0.8228\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8345\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8449\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8398\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1062 - accuracy: 0.8457\n",
            "\n",
            "\n",
            "Time to build ANN Model in seconds : 11.7983 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test the ANN Model in seconds : 0.3474 \n",
            "\n",
            "\n",
            "Time elapsed in seconds: 12.1596 \n",
            "\n",
            "\n",
            "[[10162  1198]\n",
            " [ 1398  2302]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.89      0.89     11360\n",
            "           1       0.66      0.62      0.64      3700\n",
            "\n",
            "    accuracy                           0.83     15060\n",
            "   macro avg       0.77      0.76      0.76     15060\n",
            "weighted avg       0.82      0.83      0.83     15060\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJygUm5HqD52",
        "outputId": "052f1cbe-ce87-483b-8db2-23118d169b9b"
      },
      "source": [
        "# the accuracy for the best hyperparameters with best learning rates 0.0001 is 83%. Hence when using the best hyperparameter the accuracy for the model is increased to 72% from 69%\n",
        "#STEP 10 Testing \n",
        "#aCCURACY IS 83%\n",
        "\n",
        "#6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('ANN WITH 6 HIDDEN LAYERS', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print('\\n')\n",
        "  print(\"Model : \", name)\n",
        "  model_build_time = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  print('\\n')\n",
        "  print(\"Time to build ANN Model in seconds : %.4f \" % round(time.time()-model_build_time,4))\n",
        "  print('\\n')\n",
        "  start_time = time.time()\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test the ANN Model in seconds : %.4f \" % round(time.time()-start_time,4))\n",
        "  print('\\n')\n",
        "  matrix = confusion_matrix(Y_test, y_pred)\n",
        "  print(\"Time elapsed in seconds: %.4f \" % round(time.time()-model_build_time, 4))\n",
        "  print('\\n')\n",
        "  print(matrix)\n",
        "  print('\\n')\n",
        "  report = classification_report(Y_test, y_pred)\n",
        "  print(report)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model :  ANN WITH 6 HIDDEN LAYERS\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1525 - accuracy: 0.7871\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.8312\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8329\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8381\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8387\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8385\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8445\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8424\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8385\n",
            "\n",
            "\n",
            "Time to build ANN Model in seconds : 11.8461 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test the ANN Model in seconds : 0.3523 \n",
            "\n",
            "\n",
            "Time elapsed in seconds: 12.2120 \n",
            "\n",
            "\n",
            "[[10637   723]\n",
            " [ 1843  1857]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.89     11360\n",
            "           1       0.72      0.50      0.59      3700\n",
            "\n",
            "    accuracy                           0.83     15060\n",
            "   macro avg       0.79      0.72      0.74     15060\n",
            "weighted avg       0.82      0.83      0.82     15060\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM8hp--0bNL-"
      },
      "source": [
        "Step 11 Some of the graphs to analyse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qe3lC6OibUM7",
        "outputId": "65c04ce8-92de-4e68-d170-04da6821b2c9"
      },
      "source": [
        "#1 Graph of No of hidden layers vs Accuracy ans MSE of the Model\n",
        "#hidden layer used on pca_Embedded dataset\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "def creating_model1(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #one hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #2 hidden layer used on pca_Embedded dataset\n",
        "def creating_model2(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))              #two hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #3 hidden layer used on pca_Embedded dataset\n",
        "def creating_model3(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(30, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "\n",
        "  #4 hidden layer used on pca_Embedded dataset\n",
        "def creating_model4(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #four hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #5 hidden layer used on pca_Embedded dataset\n",
        "def creating_model5(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #five hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model6(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Accuracy Rate')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of Hidden Layer(HL)')\n",
        "plt.ylabel('Generalization Accuracy')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "\n",
        "#https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/\n",
        "#Graphs of Mean Square Error\n",
        "# def calc_train_error(pca_embedded_train_features, Y_train, model):\n",
        "#     '''returns in-sample error for already fit model.'''\n",
        "#     predictions = model.predict(pca_embedded_train_features)\n",
        "#     mse = mean_squared_error(Y_train, predictions)\n",
        "#     rmse = np.sqrt(mse)\n",
        "#     return mse\n",
        "    \n",
        "# def calc_validation_error(pca_embedded_test_features, Y_test, model):\n",
        "#     '''returns out-of-sample error for already fit model.'''\n",
        "#     predictions = models.predict(pca_embedded_test_features)\n",
        "#     mse = mean_squared_error(Y_test, predictions)\n",
        "#     rmse = np.sqrt(mse)\n",
        "#     return mse\n",
        "    \n",
        "# def calc_metrics(pca_embedded_train_features, Y_train, pca_embedded_test_features, Y_test, model):\n",
        "#     '''fits model and returns the RMSE for in-sample error and out-of-sample error'''\n",
        "#     model.fit(pca_embedded_train_features, Y_train)\n",
        "#     train_error = calc_train_error(pca_embedded_train_features, Y_train, model)\n",
        "#     validation_error = calc_validation_error(pca_embedded_test_features, Y_test, model)\n",
        "#     return train_error, validation_error\n",
        "\n",
        "\n",
        "\n",
        "# # evaluate each model in turn\n",
        "# results = []\n",
        "# names = []\n",
        "# scoring = calc_validation_error(pca_embedded_test_features, Y_test, models)\n",
        "# for name, model in models:\n",
        "#   kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "#   cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "#   results.append(cv_results)\n",
        "#   names.append(name)\n",
        "#   msg = \"%s: %f (%f)\" % (name, cv_results.calc_validation_error())\n",
        "#   print(msg)\n",
        "# # boxplot algorithm comparison\n",
        "# fig = pyplot.figure()\n",
        "# fig.suptitle('Mean Squared Error')\n",
        "# ax = fig.add_subplot(111)\n",
        "# pyplot.boxplot(results)\n",
        "# ax.set_xticklabels(names)\n",
        "# plt.xlabel('No. of Hidden Layer(HL)')\n",
        "# plt.ylabel('Generalization Error Rate')\n",
        "# pyplot.show()\n",
        "# print('\\n')\n",
        "# print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1654 - accuracy: 0.7877\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1197 - accuracy: 0.8298\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8325\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8384\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8386\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8379\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8385\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1644 - accuracy: 0.7745\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1250 - accuracy: 0.8212\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1188 - accuracy: 0.8294\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8342\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8364\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8345\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8357\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8390\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8339\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8373\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1615 - accuracy: 0.7765\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1230 - accuracy: 0.8264\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1152 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8355\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8361\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1139 - accuracy: 0.8336\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8439\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8355\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8445\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8436\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1668 - accuracy: 0.7796\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1235 - accuracy: 0.8242\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8334\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8378\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8367\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8378\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8415\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1693 - accuracy: 0.7620\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1212 - accuracy: 0.8279\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8334\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1124 - accuracy: 0.8363\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8338\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8359\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8406\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1658 - accuracy: 0.7872\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1220 - accuracy: 0.8270\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8334\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8412\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8372\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8448\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8430\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8468\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1723 - accuracy: 0.7458\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1238 - accuracy: 0.8290\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1160 - accuracy: 0.8324\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8342\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8408\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8383\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8393\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8418\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1710 - accuracy: 0.7629\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1268 - accuracy: 0.8220\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1188 - accuracy: 0.8303\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8350\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8349\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8346\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8404\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8372\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8393\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1729 - accuracy: 0.7557\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1258 - accuracy: 0.8224\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1187 - accuracy: 0.8329\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1157 - accuracy: 0.8333\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8331\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8362\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8360\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8374\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8399\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1721 - accuracy: 0.7559\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1187 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8394\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8421\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8375\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8417\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8393\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1075 - accuracy: 0.8424\n",
            "1 HL: 0.839533 (0.004786)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1649 - accuracy: 0.7674\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1189 - accuracy: 0.8298\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8345\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8400\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8366\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8421\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8433\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1593 - accuracy: 0.7888\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8356\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8383\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8412\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8341\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8404\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1064 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1532 - accuracy: 0.7890\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1244 - accuracy: 0.8254\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1157 - accuracy: 0.8313\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8409\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1075 - accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8392\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8385\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8427\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1574 - accuracy: 0.7889\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1226 - accuracy: 0.8229\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1154 - accuracy: 0.8326\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8334\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8364\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8395\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8403\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8434\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1542 - accuracy: 0.7872\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1167 - accuracy: 0.8342\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8423\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8386\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8422\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8422\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8427\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1574 - accuracy: 0.7859\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1157 - accuracy: 0.8287\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8382\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8408\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8407\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8403\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8420\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1553 - accuracy: 0.7918\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1218 - accuracy: 0.8260\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1156 - accuracy: 0.8299\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8391\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8401\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8432\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8427\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8364\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8406\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1596 - accuracy: 0.7812\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1187 - accuracy: 0.8285\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8305\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8357\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8405\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8430\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8416\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8391\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1614 - accuracy: 0.7824\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1219 - accuracy: 0.8206\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8384\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8421\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8375\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8387\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8392\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8361\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1599 - accuracy: 0.7901\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1261 - accuracy: 0.8237\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.8297\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8353\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1138 - accuracy: 0.8348\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8387\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8404\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8363\n",
            "2 HL: 0.841224 (0.005957)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1574 - accuracy: 0.7866\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1216 - accuracy: 0.8265\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1155 - accuracy: 0.8336\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1139 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8390\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8425\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1060 - accuracy: 0.8459\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1567 - accuracy: 0.7832\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1170 - accuracy: 0.8326\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8374\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8393\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8356\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8439\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8385\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8424\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8400\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1519 - accuracy: 0.7888\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8425\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8377\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8427\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8448\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1068 - accuracy: 0.8432\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1073 - accuracy: 0.8430\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1073 - accuracy: 0.8426\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1074 - accuracy: 0.8424\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1583 - accuracy: 0.7748\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1137 - accuracy: 0.8363\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8381\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8393\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8401\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1068 - accuracy: 0.8465\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1070 - accuracy: 0.8440\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1520 - accuracy: 0.7947\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1147 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8363\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1070 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8402\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1563 - accuracy: 0.7878\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1163 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8378\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8423\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8401\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8408\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1511 - accuracy: 0.7933\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8344\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8368\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8410\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8389\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1074 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8424\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1597 - accuracy: 0.7812\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1241 - accuracy: 0.8258\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8359\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8393\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8379\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8386\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8384\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8404\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1554 - accuracy: 0.7878\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8353\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8401\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8384\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8345\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1069 - accuracy: 0.8439\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8373\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1533 - accuracy: 0.7894\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1207 - accuracy: 0.8275\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8370\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8361\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8401\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8412\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8431\n",
            "3 HL: 0.839666 (0.004196)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1430 - accuracy: 0.8062\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1195 - accuracy: 0.8252\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1155 - accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1170 - accuracy: 0.8280\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8436\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8445\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8395\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1435 - accuracy: 0.8031\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1161 - accuracy: 0.8328\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8370\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8361\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8392\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8376\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1072 - accuracy: 0.8444\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8393\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8407\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1528 - accuracy: 0.7888\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1161 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8336\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8395\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8461\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8397\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1475 - accuracy: 0.7951\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1171 - accuracy: 0.8280\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8297\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8381\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8424\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8405\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1062 - accuracy: 0.8469\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8387\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1552 - accuracy: 0.7885\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1145 - accuracy: 0.8374\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8418\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8431\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1072 - accuracy: 0.8436\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8434\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8417\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1543 - accuracy: 0.7916\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1217 - accuracy: 0.8251\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8368\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1138 - accuracy: 0.8338\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8374\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8370\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8395\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1074 - accuracy: 0.8442\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8378\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1554 - accuracy: 0.7897\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1175 - accuracy: 0.8314\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8322\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8427\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8414\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8384\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8416\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8381\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1551 - accuracy: 0.7853\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1197 - accuracy: 0.8300\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8400\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8389\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8336\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8380\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8412\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8384\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8425\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8408\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1497 - accuracy: 0.7879\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8312\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8429\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8387\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1524 - accuracy: 0.7905\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1139 - accuracy: 0.8344\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8350\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8378\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8441\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8352\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1065 - accuracy: 0.8453\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8406\n",
            "4 HL: 0.842318 (0.004637)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1414 - accuracy: 0.8048\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1138 - accuracy: 0.8361\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1107 - accuracy: 0.8393\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8405\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.8371\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1062 - accuracy: 0.8458\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8383\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8393\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1076 - accuracy: 0.8430\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1429 - accuracy: 0.8028\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1157 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1144 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1114 - accuracy: 0.8393\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8412\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1106 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1094 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1090 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1093 - accuracy: 0.8410\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1475 - accuracy: 0.8007\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1168 - accuracy: 0.8296\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1135 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1107 - accuracy: 0.8407\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1106 - accuracy: 0.8420\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1091 - accuracy: 0.8410\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1097 - accuracy: 0.8389\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1090 - accuracy: 0.8442\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8367\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1492 - accuracy: 0.7908\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1153 - accuracy: 0.8342\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1132 - accuracy: 0.8335\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1130 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1130 - accuracy: 0.8356\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8388\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.8385\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1083 - accuracy: 0.8428\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1085 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1090 - accuracy: 0.8407\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1497 - accuracy: 0.7934\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1141 - accuracy: 0.8381\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8372\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1098 - accuracy: 0.8394\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1070 - accuracy: 0.8450\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1072 - accuracy: 0.8444\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1098 - accuracy: 0.8404\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1073 - accuracy: 0.8442\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1083 - accuracy: 0.8446\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1066 - accuracy: 0.8455\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1522 - accuracy: 0.7934\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1213 - accuracy: 0.8288\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1131 - accuracy: 0.8393\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1134 - accuracy: 0.8363\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1090 - accuracy: 0.8432\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1108 - accuracy: 0.8394\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1106 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1074 - accuracy: 0.8452\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1088 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1474 - accuracy: 0.7973\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1169 - accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1113 - accuracy: 0.8377\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8419\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8370\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.8379\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1085 - accuracy: 0.8435\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8397\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1082 - accuracy: 0.8421\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1512 - accuracy: 0.7927\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1162 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1152 - accuracy: 0.8328\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1130 - accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1109 - accuracy: 0.8388\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1123 - accuracy: 0.8367\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1098 - accuracy: 0.8392\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1087 - accuracy: 0.8423\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1108 - accuracy: 0.8375\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1080 - accuracy: 0.8427\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1465 - accuracy: 0.7952\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1177 - accuracy: 0.8325\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1144 - accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1115 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1090 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1107 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.8402\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8423\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1083 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1110 - accuracy: 0.8384\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1472 - accuracy: 0.7970\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1250 - accuracy: 0.8261\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1155 - accuracy: 0.8339\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1145 - accuracy: 0.8343\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.8395\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8378\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1078 - accuracy: 0.8438\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1113 - accuracy: 0.8365\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8405\n",
            "5 HL: 0.838605 (0.005685)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1420 - accuracy: 0.8017\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1177 - accuracy: 0.8275\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1150 - accuracy: 0.8334\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.8418\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1108 - accuracy: 0.8372\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1102 - accuracy: 0.8386\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1058 - accuracy: 0.8459\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1066 - accuracy: 0.8434\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1480 - accuracy: 0.7926\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1156 - accuracy: 0.8347\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1118 - accuracy: 0.8381\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1112 - accuracy: 0.8379\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8398\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1083 - accuracy: 0.8426\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1068 - accuracy: 0.8428\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1072 - accuracy: 0.8459\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1073 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1093 - accuracy: 0.8402\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1539 - accuracy: 0.7815\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1174 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1136 - accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.8412\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1086 - accuracy: 0.8421\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1108 - accuracy: 0.8393\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1086 - accuracy: 0.8420\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1080 - accuracy: 0.8431\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1078 - accuracy: 0.8448\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1483 - accuracy: 0.7905\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1165 - accuracy: 0.8334\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1143 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1123 - accuracy: 0.8359\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1118 - accuracy: 0.8369\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1102 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1086 - accuracy: 0.8422\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1087 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8377\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1087 - accuracy: 0.8427\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1440 - accuracy: 0.8005\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1196 - accuracy: 0.8304\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1128 - accuracy: 0.8381\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1116 - accuracy: 0.8375\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1096 - accuracy: 0.8417\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1096 - accuracy: 0.8427\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8427\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1084 - accuracy: 0.8426\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1097 - accuracy: 0.8394\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1067 - accuracy: 0.8446\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1447 - accuracy: 0.8024\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1169 - accuracy: 0.8305\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1119 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8398\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1093 - accuracy: 0.8390\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1107 - accuracy: 0.8380\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.8379\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1085 - accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1087 - accuracy: 0.8422\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1084 - accuracy: 0.8441\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1459 - accuracy: 0.8061\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1168 - accuracy: 0.8306\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1126 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1123 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1117 - accuracy: 0.8385\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.8431\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8397\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1104 - accuracy: 0.8423\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1094 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8385\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1423 - accuracy: 0.8039\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1174 - accuracy: 0.8294\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1129 - accuracy: 0.8380\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1123 - accuracy: 0.8383\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.8430\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1080 - accuracy: 0.8432\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.8393\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1075 - accuracy: 0.8451\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1091 - accuracy: 0.8402\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1548 - accuracy: 0.7870\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1225 - accuracy: 0.8231\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1161 - accuracy: 0.8295\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1126 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1125 - accuracy: 0.8344\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1125 - accuracy: 0.8362\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8388\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.8417\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.8380\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1437 - accuracy: 0.7998\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1167 - accuracy: 0.8350\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1122 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1114 - accuracy: 0.8418\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1074 - accuracy: 0.8442\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1109 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.8437\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.8408\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1112 - accuracy: 0.8397\n",
            "6 HL: 0.839998 (0.006074)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d92edf2c2bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_validation_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_embedded_test_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-d92edf2c2bdc>\u001b[0m in \u001b[0;36mcalc_validation_error\u001b[0;34m(pca_embedded_test_features, Y_test, model)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_validation_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_embedded_test_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m'''returns out-of-sample error for already fit model.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_embedded_test_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tryA1lQUubI7",
        "outputId": "969670f0-dd8e-4712-da34-872c80ff952e"
      },
      "source": [
        "#to take out mean square error\n",
        "def creating_model1(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #one hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #2 hidden layer used on pca_Embedded dataset\n",
        "def creating_model2(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))              #two hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #3 hidden layer used on pca_Embedded dataset\n",
        "def creating_model3(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(30, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "\n",
        "  #4 hidden layer used on pca_Embedded dataset\n",
        "def creating_model4(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #four hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #5 hidden layer used on pca_Embedded dataset\n",
        "def creating_model5(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #five hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model6(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('1 HL', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('2 HL', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('3 HL', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('4 HL', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('5 HL', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('6 HL', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "\n",
        "results = []\n",
        "names=[]\n",
        "for name, model in models:\n",
        "   kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "   cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold)\n",
        "   results.append(cv_results)\n",
        "   model.fit(pca_embedded_train_features, Y_train)\n",
        "   y_pred = model.predict(pca_embedded_test_features)\n",
        "   #predictions = models.predict(pca_embedded_test_features)\n",
        "   mse = mean_squared_error(Y_test,  y_pred)\n",
        "   results.append(mse)\n",
        "   names.append(name)\n",
        "   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "   print(msg)\n",
        "  # msg = \"%s: \" % (name)\n",
        "  # print(msg)\n",
        "\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Squared Error')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of Hidden Layer(HL)')\n",
        "plt.ylabel('Generalization Error Rate')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 805us/step - loss: 0.1777 - accuracy: 0.7731\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 809us/step - loss: 0.1199 - accuracy: 0.8296\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1122 - accuracy: 0.8374\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 802us/step - loss: 0.1125 - accuracy: 0.8378\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1114 - accuracy: 0.8385\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 805us/step - loss: 0.1092 - accuracy: 0.8426\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1108 - accuracy: 0.8377\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 798us/step - loss: 0.1090 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 832us/step - loss: 0.1083 - accuracy: 0.8425\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 794us/step - loss: 0.1089 - accuracy: 0.8418\n",
            "151/151 [==============================] - 0s 805us/step - loss: 0.1120 - accuracy: 0.8349\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 777us/step - loss: 0.1686 - accuracy: 0.7678\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 806us/step - loss: 0.1185 - accuracy: 0.8337\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 807us/step - loss: 0.1128 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 841us/step - loss: 0.1119 - accuracy: 0.8381\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1100 - accuracy: 0.8399\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1100 - accuracy: 0.8371\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 841us/step - loss: 0.1089 - accuracy: 0.8409\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 869us/step - loss: 0.1096 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 841us/step - loss: 0.1076 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 838us/step - loss: 0.1087 - accuracy: 0.8400\n",
            "151/151 [==============================] - 0s 653us/step - loss: 0.1092 - accuracy: 0.8426\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 795us/step - loss: 0.1687 - accuracy: 0.7676\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 839us/step - loss: 0.1149 - accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 855us/step - loss: 0.1103 - accuracy: 0.8389\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 825us/step - loss: 0.1096 - accuracy: 0.8406\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1099 - accuracy: 0.8382\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 898us/step - loss: 0.1091 - accuracy: 0.8405\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 911us/step - loss: 0.1088 - accuracy: 0.8393\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 886us/step - loss: 0.1078 - accuracy: 0.8422\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 991us/step - loss: 0.1077 - accuracy: 0.8443\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1101 - accuracy: 0.8383\n",
            "151/151 [==============================] - 0s 700us/step - loss: 0.1086 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 808us/step - loss: 0.1678 - accuracy: 0.7691\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 863us/step - loss: 0.1172 - accuracy: 0.8287\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1144 - accuracy: 0.8321\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 826us/step - loss: 0.1122 - accuracy: 0.8361\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 870us/step - loss: 0.1108 - accuracy: 0.8372\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 853us/step - loss: 0.1098 - accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 871us/step - loss: 0.1104 - accuracy: 0.8355\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 832us/step - loss: 0.1093 - accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 898us/step - loss: 0.1074 - accuracy: 0.8447\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 943us/step - loss: 0.1099 - accuracy: 0.8410\n",
            "151/151 [==============================] - 0s 735us/step - loss: 0.1093 - accuracy: 0.8405\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 824us/step - loss: 0.1778 - accuracy: 0.7382\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1176 - accuracy: 0.8322\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1120 - accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 852us/step - loss: 0.1119 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 829us/step - loss: 0.1127 - accuracy: 0.8332\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 809us/step - loss: 0.1135 - accuracy: 0.8342\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 816us/step - loss: 0.1117 - accuracy: 0.8337\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 864us/step - loss: 0.1143 - accuracy: 0.8315\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1112 - accuracy: 0.8378\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 872us/step - loss: 0.1110 - accuracy: 0.8366\n",
            "151/151 [==============================] - 0s 608us/step - loss: 0.1133 - accuracy: 0.8355\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 792us/step - loss: 0.1632 - accuracy: 0.7867\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 845us/step - loss: 0.1158 - accuracy: 0.8338\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 824us/step - loss: 0.1083 - accuracy: 0.8417\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 845us/step - loss: 0.1123 - accuracy: 0.8361\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 875us/step - loss: 0.1111 - accuracy: 0.8385\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1096 - accuracy: 0.8418\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 844us/step - loss: 0.1049 - accuracy: 0.8480\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1063 - accuracy: 0.8456\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 875us/step - loss: 0.1093 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 835us/step - loss: 0.1083 - accuracy: 0.8397\n",
            "151/151 [==============================] - 0s 627us/step - loss: 0.1126 - accuracy: 0.8336\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 760us/step - loss: 0.1801 - accuracy: 0.7544\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 803us/step - loss: 0.1216 - accuracy: 0.8230\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 835us/step - loss: 0.1142 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 807us/step - loss: 0.1102 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 831us/step - loss: 0.1112 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 810us/step - loss: 0.1079 - accuracy: 0.8447\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 829us/step - loss: 0.1075 - accuracy: 0.8444\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 806us/step - loss: 0.1109 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 812us/step - loss: 0.1090 - accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 810us/step - loss: 0.1082 - accuracy: 0.8441\n",
            "151/151 [==============================] - 0s 654us/step - loss: 0.1085 - accuracy: 0.8432\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 799us/step - loss: 0.1888 - accuracy: 0.7257\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 852us/step - loss: 0.1248 - accuracy: 0.8218\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1123 - accuracy: 0.8363\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1111 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 855us/step - loss: 0.1113 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 876us/step - loss: 0.1106 - accuracy: 0.8383\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 844us/step - loss: 0.1104 - accuracy: 0.8374\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 796us/step - loss: 0.1089 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 843us/step - loss: 0.1083 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 801us/step - loss: 0.1096 - accuracy: 0.8394\n",
            "151/151 [==============================] - 0s 741us/step - loss: 0.1080 - accuracy: 0.8465\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 768us/step - loss: 0.1754 - accuracy: 0.7739\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 816us/step - loss: 0.1179 - accuracy: 0.8308\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 806us/step - loss: 0.1147 - accuracy: 0.8304\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1133 - accuracy: 0.8335\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 837us/step - loss: 0.1127 - accuracy: 0.8351\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 814us/step - loss: 0.1117 - accuracy: 0.8357\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 822us/step - loss: 0.1113 - accuracy: 0.8359\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 839us/step - loss: 0.1105 - accuracy: 0.8365\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 855us/step - loss: 0.1089 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 816us/step - loss: 0.1104 - accuracy: 0.8395\n",
            "151/151 [==============================] - 0s 732us/step - loss: 0.1038 - accuracy: 0.8501\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 1s 788us/step - loss: 0.1708 - accuracy: 0.7686\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 835us/step - loss: 0.1177 - accuracy: 0.8304\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 880us/step - loss: 0.1130 - accuracy: 0.8365\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1126 - accuracy: 0.8342\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 842us/step - loss: 0.1099 - accuracy: 0.8408\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 889us/step - loss: 0.1118 - accuracy: 0.8359\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 938us/step - loss: 0.1131 - accuracy: 0.8345\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1108 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 906us/step - loss: 0.1099 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1086 - accuracy: 0.8428\n",
            "151/151 [==============================] - 0s 727us/step - loss: 0.1039 - accuracy: 0.8458\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 817us/step - loss: 0.1633 - accuracy: 0.7834\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 998us/step - loss: 0.1173 - accuracy: 0.8297\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 1s 885us/step - loss: 0.1124 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 1s 893us/step - loss: 0.1097 - accuracy: 0.8389\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 1s 896us/step - loss: 0.1103 - accuracy: 0.8374\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 1s 857us/step - loss: 0.1093 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 1s 834us/step - loss: 0.1105 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 1s 888us/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 1s 880us/step - loss: 0.1089 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 1s 934us/step - loss: 0.1093 - accuracy: 0.8406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 HL: 0.841489 (0.005149)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 876us/step - loss: 0.1674 - accuracy: 0.7612\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1162 - accuracy: 0.8301\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 917us/step - loss: 0.1125 - accuracy: 0.8337\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1109 - accuracy: 0.8395\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 914us/step - loss: 0.1111 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1095 - accuracy: 0.8410\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1093 - accuracy: 0.8414\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 840us/step - loss: 0.1096 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1096 - accuracy: 0.8415\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 864us/step - loss: 0.1094 - accuracy: 0.8375\n",
            "151/151 [==============================] - 0s 612us/step - loss: 0.1152 - accuracy: 0.8320\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 865us/step - loss: 0.1630 - accuracy: 0.7739\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1152 - accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 863us/step - loss: 0.1108 - accuracy: 0.8349\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 850us/step - loss: 0.1105 - accuracy: 0.8372\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 823us/step - loss: 0.1112 - accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 815us/step - loss: 0.1082 - accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 813us/step - loss: 0.1102 - accuracy: 0.8376\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 881us/step - loss: 0.1092 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 847us/step - loss: 0.1085 - accuracy: 0.8415\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1093 - accuracy: 0.8395\n",
            "151/151 [==============================] - 0s 657us/step - loss: 0.1090 - accuracy: 0.8406\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 814us/step - loss: 0.1554 - accuracy: 0.7919\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 834us/step - loss: 0.1147 - accuracy: 0.8321\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 879us/step - loss: 0.1118 - accuracy: 0.8370\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 883us/step - loss: 0.1096 - accuracy: 0.8393\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 862us/step - loss: 0.1081 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1087 - accuracy: 0.8424\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 832us/step - loss: 0.1104 - accuracy: 0.8392\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 872us/step - loss: 0.1075 - accuracy: 0.8429\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 921us/step - loss: 0.1070 - accuracy: 0.8450\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 945us/step - loss: 0.1090 - accuracy: 0.8398\n",
            "151/151 [==============================] - 0s 822us/step - loss: 0.1089 - accuracy: 0.8435\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 826us/step - loss: 0.1555 - accuracy: 0.7874\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 844us/step - loss: 0.1124 - accuracy: 0.8357\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 873us/step - loss: 0.1101 - accuracy: 0.8384\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1093 - accuracy: 0.8433\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 906us/step - loss: 0.1101 - accuracy: 0.8412\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1095 - accuracy: 0.8408\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 902us/step - loss: 0.1089 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 930us/step - loss: 0.1101 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 879us/step - loss: 0.1100 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 867us/step - loss: 0.1094 - accuracy: 0.8409\n",
            "151/151 [==============================] - 0s 701us/step - loss: 0.1099 - accuracy: 0.8385\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 854us/step - loss: 0.1635 - accuracy: 0.7694\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 867us/step - loss: 0.1159 - accuracy: 0.8317\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 899us/step - loss: 0.1115 - accuracy: 0.8375\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 881us/step - loss: 0.1088 - accuracy: 0.8402\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1081 - accuracy: 0.8434\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 961us/step - loss: 0.1103 - accuracy: 0.8385\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 961us/step - loss: 0.1096 - accuracy: 0.8412\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1103 - accuracy: 0.8377\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 943us/step - loss: 0.1096 - accuracy: 0.8394\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1087 - accuracy: 0.8403\n",
            "151/151 [==============================] - 0s 768us/step - loss: 0.1122 - accuracy: 0.8355\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 868us/step - loss: 0.1572 - accuracy: 0.7840\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 887us/step - loss: 0.1132 - accuracy: 0.8380\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 897us/step - loss: 0.1126 - accuracy: 0.8372\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 859us/step - loss: 0.1104 - accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 918us/step - loss: 0.1134 - accuracy: 0.8329\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 808us/step - loss: 0.1102 - accuracy: 0.8409\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 844us/step - loss: 0.1111 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 865us/step - loss: 0.1101 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1101 - accuracy: 0.8380\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 916us/step - loss: 0.1102 - accuracy: 0.8372\n",
            "151/151 [==============================] - 0s 648us/step - loss: 0.1128 - accuracy: 0.8345\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 866us/step - loss: 0.1712 - accuracy: 0.7659\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 844us/step - loss: 0.1141 - accuracy: 0.8357\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 868us/step - loss: 0.1111 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 897us/step - loss: 0.1152 - accuracy: 0.8310\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 883us/step - loss: 0.1097 - accuracy: 0.8409\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1119 - accuracy: 0.8353\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 895us/step - loss: 0.1110 - accuracy: 0.8387\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 916us/step - loss: 0.1117 - accuracy: 0.8367\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1126 - accuracy: 0.8368\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 906us/step - loss: 0.1118 - accuracy: 0.8356\n",
            "151/151 [==============================] - 0s 770us/step - loss: 0.1124 - accuracy: 0.8312\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 880us/step - loss: 0.1549 - accuracy: 0.7828\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1143 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 920us/step - loss: 0.1151 - accuracy: 0.8310\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 929us/step - loss: 0.1132 - accuracy: 0.8329\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 901us/step - loss: 0.1126 - accuracy: 0.8367\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 948us/step - loss: 0.1138 - accuracy: 0.8323\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1130 - accuracy: 0.8327\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 983us/step - loss: 0.1114 - accuracy: 0.8343\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1117 - accuracy: 0.8351\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1100 - accuracy: 0.8377\n",
            "151/151 [==============================] - 0s 847us/step - loss: 0.1124 - accuracy: 0.8365\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 866us/step - loss: 0.1533 - accuracy: 0.7906\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 925us/step - loss: 0.1139 - accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 931us/step - loss: 0.1136 - accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 978us/step - loss: 0.1098 - accuracy: 0.8399\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1089 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 897us/step - loss: 0.1106 - accuracy: 0.8345\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8373\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 991us/step - loss: 0.1095 - accuracy: 0.8408\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8366\n",
            "151/151 [==============================] - 0s 834us/step - loss: 0.1065 - accuracy: 0.8458\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 952us/step - loss: 0.1653 - accuracy: 0.7654\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 994us/step - loss: 0.1149 - accuracy: 0.8328\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1129 - accuracy: 0.8370\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 930us/step - loss: 0.1133 - accuracy: 0.8357\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 908us/step - loss: 0.1109 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 883us/step - loss: 0.1132 - accuracy: 0.8320\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 890us/step - loss: 0.1127 - accuracy: 0.8339\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 928us/step - loss: 0.1113 - accuracy: 0.8377\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 939us/step - loss: 0.1121 - accuracy: 0.8359\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1102 - accuracy: 0.8387\n",
            "151/151 [==============================] - 0s 735us/step - loss: 0.1050 - accuracy: 0.8412\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 837us/step - loss: 0.1545 - accuracy: 0.7821\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 1s 884us/step - loss: 0.1143 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 1s 910us/step - loss: 0.1128 - accuracy: 0.8339\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 1s 879us/step - loss: 0.1110 - accuracy: 0.8383\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 1s 912us/step - loss: 0.1118 - accuracy: 0.8348\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 1s 899us/step - loss: 0.1100 - accuracy: 0.8406\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 1s 894us/step - loss: 0.1087 - accuracy: 0.8414\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 1s 945us/step - loss: 0.1073 - accuracy: 0.8441\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 1s 925us/step - loss: 0.1081 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 1s 975us/step - loss: 0.1072 - accuracy: 0.8444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2 HL: 0.837942 (0.004592)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 868us/step - loss: 0.1528 - accuracy: 0.7845\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 880us/step - loss: 0.1117 - accuracy: 0.8370\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 913us/step - loss: 0.1098 - accuracy: 0.8410\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 982us/step - loss: 0.1095 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 925us/step - loss: 0.1081 - accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 913us/step - loss: 0.1079 - accuracy: 0.8432\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1066 - accuracy: 0.8443\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 967us/step - loss: 0.1100 - accuracy: 0.8384\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 942us/step - loss: 0.1066 - accuracy: 0.8447\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1066 - accuracy: 0.8431\n",
            "151/151 [==============================] - 0s 823us/step - loss: 0.1135 - accuracy: 0.8336\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 877us/step - loss: 0.1495 - accuracy: 0.7891\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1150 - accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 892us/step - loss: 0.1140 - accuracy: 0.8307\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8388\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 962us/step - loss: 0.1098 - accuracy: 0.8401\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 916us/step - loss: 0.1122 - accuracy: 0.8365\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 960us/step - loss: 0.1103 - accuracy: 0.8392\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 923us/step - loss: 0.1125 - accuracy: 0.8352\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 963us/step - loss: 0.1116 - accuracy: 0.8367\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 972us/step - loss: 0.1101 - accuracy: 0.8413\n",
            "151/151 [==============================] - 0s 742us/step - loss: 0.1100 - accuracy: 0.8429\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 870us/step - loss: 0.1508 - accuracy: 0.7905\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 929us/step - loss: 0.1138 - accuracy: 0.8334\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 975us/step - loss: 0.1119 - accuracy: 0.8362\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 960us/step - loss: 0.1113 - accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 909us/step - loss: 0.1101 - accuracy: 0.8388\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 893us/step - loss: 0.1116 - accuracy: 0.8375\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 886us/step - loss: 0.1127 - accuracy: 0.8352\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1093 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 870us/step - loss: 0.1118 - accuracy: 0.8373\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 923us/step - loss: 0.1115 - accuracy: 0.8372\n",
            "151/151 [==============================] - 0s 720us/step - loss: 0.1133 - accuracy: 0.8345\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 871us/step - loss: 0.1464 - accuracy: 0.7916\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 878us/step - loss: 0.1138 - accuracy: 0.8341\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1130 - accuracy: 0.8348\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 912us/step - loss: 0.1125 - accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 931us/step - loss: 0.1102 - accuracy: 0.8416\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1108 - accuracy: 0.8365\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1104 - accuracy: 0.8389\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1091 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 913us/step - loss: 0.1087 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 892us/step - loss: 0.1077 - accuracy: 0.8440\n",
            "151/151 [==============================] - 0s 597us/step - loss: 0.1102 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 849us/step - loss: 0.1570 - accuracy: 0.7808\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 877us/step - loss: 0.1147 - accuracy: 0.8332\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 888us/step - loss: 0.1126 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 876us/step - loss: 0.1108 - accuracy: 0.8389\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 929us/step - loss: 0.1095 - accuracy: 0.8400\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 849us/step - loss: 0.1094 - accuracy: 0.8422\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 891us/step - loss: 0.1100 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1100 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 929us/step - loss: 0.1074 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 893us/step - loss: 0.1090 - accuracy: 0.8417\n",
            "151/151 [==============================] - 0s 832us/step - loss: 0.1111 - accuracy: 0.8359\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 862us/step - loss: 0.1511 - accuracy: 0.7897\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 876us/step - loss: 0.1146 - accuracy: 0.8338\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 884us/step - loss: 0.1131 - accuracy: 0.8362\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1125 - accuracy: 0.8348\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 879us/step - loss: 0.1101 - accuracy: 0.8402\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 903us/step - loss: 0.1074 - accuracy: 0.8434\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 869us/step - loss: 0.1082 - accuracy: 0.8450\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 885us/step - loss: 0.1105 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 907us/step - loss: 0.1094 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 886us/step - loss: 0.1073 - accuracy: 0.8421\n",
            "151/151 [==============================] - 0s 617us/step - loss: 0.1107 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 850us/step - loss: 0.1516 - accuracy: 0.7883\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 854us/step - loss: 0.1133 - accuracy: 0.8347\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 851us/step - loss: 0.1116 - accuracy: 0.8384\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 861us/step - loss: 0.1101 - accuracy: 0.8388\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 898us/step - loss: 0.1117 - accuracy: 0.8356\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 870us/step - loss: 0.1078 - accuracy: 0.8419\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 866us/step - loss: 0.1095 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 939us/step - loss: 0.1094 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 887us/step - loss: 0.1088 - accuracy: 0.8398\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 887us/step - loss: 0.1096 - accuracy: 0.8394\n",
            "151/151 [==============================] - 0s 756us/step - loss: 0.1082 - accuracy: 0.8375\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 845us/step - loss: 0.1513 - accuracy: 0.7894\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 886us/step - loss: 0.1133 - accuracy: 0.8344\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 917us/step - loss: 0.1120 - accuracy: 0.8350\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 948us/step - loss: 0.1093 - accuracy: 0.8419\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 902us/step - loss: 0.1108 - accuracy: 0.8382\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 867us/step - loss: 0.1097 - accuracy: 0.8374\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1099 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 906us/step - loss: 0.1086 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 966us/step - loss: 0.1082 - accuracy: 0.8450\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 917us/step - loss: 0.1084 - accuracy: 0.8408\n",
            "151/151 [==============================] - 0s 736us/step - loss: 0.1089 - accuracy: 0.8465\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 852us/step - loss: 0.1484 - accuracy: 0.7931\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 911us/step - loss: 0.1143 - accuracy: 0.8321\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 933us/step - loss: 0.1102 - accuracy: 0.8391\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 875us/step - loss: 0.1090 - accuracy: 0.8401\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 906us/step - loss: 0.1110 - accuracy: 0.8375\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1095 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 914us/step - loss: 0.1098 - accuracy: 0.8387\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 911us/step - loss: 0.1079 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 901us/step - loss: 0.1080 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1082 - accuracy: 0.8427\n",
            "151/151 [==============================] - 0s 783us/step - loss: 0.1020 - accuracy: 0.8511\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 862us/step - loss: 0.1553 - accuracy: 0.7829\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 890us/step - loss: 0.1165 - accuracy: 0.8297\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 904us/step - loss: 0.1150 - accuracy: 0.8328\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 938us/step - loss: 0.1125 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 910us/step - loss: 0.1105 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 906us/step - loss: 0.1120 - accuracy: 0.8349\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 925us/step - loss: 0.1102 - accuracy: 0.8389\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 952us/step - loss: 0.1087 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 991us/step - loss: 0.1094 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 889us/step - loss: 0.1108 - accuracy: 0.8388\n",
            "151/151 [==============================] - 1s 774us/step - loss: 0.1057 - accuracy: 0.8471\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 910us/step - loss: 0.1529 - accuracy: 0.7862\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 1s 878us/step - loss: 0.1129 - accuracy: 0.8358\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 1s 960us/step - loss: 0.1097 - accuracy: 0.8408\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 1s 983us/step - loss: 0.1099 - accuracy: 0.8398\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 1s 986us/step - loss: 0.1091 - accuracy: 0.8425\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 1s 943us/step - loss: 0.1091 - accuracy: 0.8408\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 1s 959us/step - loss: 0.1095 - accuracy: 0.8393\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 1s 926us/step - loss: 0.1087 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 1s 917us/step - loss: 0.1095 - accuracy: 0.8393\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 1s 914us/step - loss: 0.1077 - accuracy: 0.8449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3 HL: 0.841158 (0.005585)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 960us/step - loss: 0.1387 - accuracy: 0.8043\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 963us/step - loss: 0.1135 - accuracy: 0.8374\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 922us/step - loss: 0.1131 - accuracy: 0.8355\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 920us/step - loss: 0.1100 - accuracy: 0.8404\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1092 - accuracy: 0.8416\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1099 - accuracy: 0.8386\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 909us/step - loss: 0.1096 - accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 972us/step - loss: 0.1089 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 954us/step - loss: 0.1081 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1085 - accuracy: 0.8433\n",
            "151/151 [==============================] - 0s 732us/step - loss: 0.1131 - accuracy: 0.8373\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 943us/step - loss: 0.1430 - accuracy: 0.8030\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8380\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 989us/step - loss: 0.1126 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 997us/step - loss: 0.1083 - accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1090 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8407\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1076 - accuracy: 0.8451\n",
            "151/151 [==============================] - 0s 640us/step - loss: 0.1117 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 893us/step - loss: 0.1449 - accuracy: 0.7991\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1136 - accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 914us/step - loss: 0.1105 - accuracy: 0.8388\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 951us/step - loss: 0.1098 - accuracy: 0.8406\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 947us/step - loss: 0.1101 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 983us/step - loss: 0.1097 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 936us/step - loss: 0.1088 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 910us/step - loss: 0.1085 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1112 - accuracy: 0.8360\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 955us/step - loss: 0.1074 - accuracy: 0.8444\n",
            "151/151 [==============================] - 0s 657us/step - loss: 0.1101 - accuracy: 0.8349\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 923us/step - loss: 0.1462 - accuracy: 0.7965\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 938us/step - loss: 0.1171 - accuracy: 0.8269\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 976us/step - loss: 0.1130 - accuracy: 0.8335\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 905us/step - loss: 0.1149 - accuracy: 0.8295\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1137 - accuracy: 0.8315\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8354\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8334\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8347\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1114 - accuracy: 0.8373\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8372\n",
            "151/151 [==============================] - 0s 763us/step - loss: 0.1113 - accuracy: 0.8329\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 994us/step - loss: 0.1546 - accuracy: 0.7853\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1156 - accuracy: 0.8318\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8387\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1100 - accuracy: 0.8378\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1076 - accuracy: 0.8441\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8433\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 988us/step - loss: 0.1084 - accuracy: 0.8434\n",
            "151/151 [==============================] - 0s 721us/step - loss: 0.1110 - accuracy: 0.8352\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 953us/step - loss: 0.1407 - accuracy: 0.8057\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 976us/step - loss: 0.1140 - accuracy: 0.8315\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8316\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1116 - accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 926us/step - loss: 0.1111 - accuracy: 0.8397\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 987us/step - loss: 0.1115 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 984us/step - loss: 0.1106 - accuracy: 0.8395\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1117 - accuracy: 0.8388\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 981us/step - loss: 0.1099 - accuracy: 0.8387\n",
            "151/151 [==============================] - 0s 765us/step - loss: 0.1129 - accuracy: 0.8375\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 888us/step - loss: 0.1460 - accuracy: 0.7887\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 948us/step - loss: 0.1143 - accuracy: 0.8338\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 961us/step - loss: 0.1139 - accuracy: 0.8338\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 973us/step - loss: 0.1145 - accuracy: 0.8321\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 919us/step - loss: 0.1115 - accuracy: 0.8386\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 946us/step - loss: 0.1112 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 975us/step - loss: 0.1111 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 961us/step - loss: 0.1115 - accuracy: 0.8354\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 924us/step - loss: 0.1094 - accuracy: 0.8381\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 914us/step - loss: 0.1068 - accuracy: 0.8462\n",
            "151/151 [==============================] - 0s 658us/step - loss: 0.1082 - accuracy: 0.8342\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 874us/step - loss: 0.1413 - accuracy: 0.8042\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 923us/step - loss: 0.1155 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 955us/step - loss: 0.1142 - accuracy: 0.8324\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 928us/step - loss: 0.1136 - accuracy: 0.8327\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 964us/step - loss: 0.1109 - accuracy: 0.8380\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 931us/step - loss: 0.1110 - accuracy: 0.8345\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 965us/step - loss: 0.1112 - accuracy: 0.8359\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 938us/step - loss: 0.1096 - accuracy: 0.8390\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 963us/step - loss: 0.1094 - accuracy: 0.8402\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 952us/step - loss: 0.1097 - accuracy: 0.8356\n",
            "151/151 [==============================] - 0s 855us/step - loss: 0.1099 - accuracy: 0.8455\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 886us/step - loss: 0.1489 - accuracy: 0.7904\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 940us/step - loss: 0.1154 - accuracy: 0.8314\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 958us/step - loss: 0.1144 - accuracy: 0.8327\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1164 - accuracy: 0.8264\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 963us/step - loss: 0.1109 - accuracy: 0.8378\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8381\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 979us/step - loss: 0.1122 - accuracy: 0.8353\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8417\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 956us/step - loss: 0.1065 - accuracy: 0.8446\n",
            "151/151 [==============================] - 0s 719us/step - loss: 0.1055 - accuracy: 0.8452\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 913us/step - loss: 0.1472 - accuracy: 0.7933\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 926us/step - loss: 0.1162 - accuracy: 0.8317\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 950us/step - loss: 0.1120 - accuracy: 0.8389\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 951us/step - loss: 0.1131 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 991us/step - loss: 0.1137 - accuracy: 0.8324\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8375\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 938us/step - loss: 0.1113 - accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 941us/step - loss: 0.1098 - accuracy: 0.8385\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 961us/step - loss: 0.1086 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 939us/step - loss: 0.1078 - accuracy: 0.8404\n",
            "151/151 [==============================] - 0s 753us/step - loss: 0.1038 - accuracy: 0.8485\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 922us/step - loss: 0.1432 - accuracy: 0.7993\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 1s 959us/step - loss: 0.1133 - accuracy: 0.8376\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 1s 956us/step - loss: 0.1119 - accuracy: 0.8377\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 1s 968us/step - loss: 0.1127 - accuracy: 0.8352\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 1s 955us/step - loss: 0.1108 - accuracy: 0.8375\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 1s 981us/step - loss: 0.1074 - accuracy: 0.8439\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 1s 951us/step - loss: 0.1098 - accuracy: 0.8393\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 1s 966us/step - loss: 0.1101 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 993us/step - loss: 0.1080 - accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4 HL: 0.839069 (0.005165)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 913us/step - loss: 0.1413 - accuracy: 0.7987\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 927us/step - loss: 0.1119 - accuracy: 0.8374\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8392\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 962us/step - loss: 0.1107 - accuracy: 0.8384\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 984us/step - loss: 0.1082 - accuracy: 0.8439\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 973us/step - loss: 0.1089 - accuracy: 0.8411\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8384\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 976us/step - loss: 0.1083 - accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1074 - accuracy: 0.8451\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8401\n",
            "151/151 [==============================] - 0s 822us/step - loss: 0.1128 - accuracy: 0.8339\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 943us/step - loss: 0.1494 - accuracy: 0.7935\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 978us/step - loss: 0.1139 - accuracy: 0.8365\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 992us/step - loss: 0.1139 - accuracy: 0.8336\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8393\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8365\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8361\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8429\n",
            "151/151 [==============================] - 1s 753us/step - loss: 0.1110 - accuracy: 0.8426\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 947us/step - loss: 0.1368 - accuracy: 0.8085\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1146 - accuracy: 0.8350\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8339\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8338\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8348\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8366\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 952us/step - loss: 0.1130 - accuracy: 0.8329\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8381\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 980us/step - loss: 0.1115 - accuracy: 0.8355\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8374\n",
            "151/151 [==============================] - 0s 735us/step - loss: 0.1141 - accuracy: 0.8319\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 927us/step - loss: 0.1435 - accuracy: 0.8009\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1156 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8316\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8348\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8365\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8374\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8418\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8352\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8401\n",
            "151/151 [==============================] - 0s 777us/step - loss: 0.1114 - accuracy: 0.8392\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 974us/step - loss: 0.1491 - accuracy: 0.7879\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8368\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1132 - accuracy: 0.8340\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 984us/step - loss: 0.1120 - accuracy: 0.8363\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8352\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 979us/step - loss: 0.1122 - accuracy: 0.8374\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 997us/step - loss: 0.1126 - accuracy: 0.8376\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8385\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8424\n",
            "151/151 [==============================] - 0s 784us/step - loss: 0.1151 - accuracy: 0.8316\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 927us/step - loss: 0.1476 - accuracy: 0.7925\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1160 - accuracy: 0.8311\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8334\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 962us/step - loss: 0.1109 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8382\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 980us/step - loss: 0.1117 - accuracy: 0.8359\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8378\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 957us/step - loss: 0.1098 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8438\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 972us/step - loss: 0.1086 - accuracy: 0.8415\n",
            "151/151 [==============================] - 0s 730us/step - loss: 0.1136 - accuracy: 0.8306\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 961us/step - loss: 0.1429 - accuracy: 0.8053\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8382\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8392\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 958us/step - loss: 0.1116 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8411\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8433\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8399\n",
            "151/151 [==============================] - 0s 725us/step - loss: 0.1104 - accuracy: 0.8379\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 956us/step - loss: 0.1495 - accuracy: 0.7855\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1148 - accuracy: 0.8343\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 982us/step - loss: 0.1135 - accuracy: 0.8325\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 996us/step - loss: 0.1115 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8436\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8402\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 991us/step - loss: 0.1095 - accuracy: 0.8384\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8398\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8388\n",
            "151/151 [==============================] - 0s 739us/step - loss: 0.1113 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 936us/step - loss: 0.1423 - accuracy: 0.8011\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 985us/step - loss: 0.1144 - accuracy: 0.8341\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 986us/step - loss: 0.1148 - accuracy: 0.8315\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 990us/step - loss: 0.1123 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 995us/step - loss: 0.1118 - accuracy: 0.8393\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 959us/step - loss: 0.1097 - accuracy: 0.8407\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 990us/step - loss: 0.1131 - accuracy: 0.8342\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 999us/step - loss: 0.1124 - accuracy: 0.8387\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 988us/step - loss: 0.1110 - accuracy: 0.8395\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8412\n",
            "151/151 [==============================] - 0s 702us/step - loss: 0.1053 - accuracy: 0.8485\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 978us/step - loss: 0.1409 - accuracy: 0.7960\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1134 - accuracy: 0.8385\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 985us/step - loss: 0.1123 - accuracy: 0.8356\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8400\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 993us/step - loss: 0.1106 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8428\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8410\n",
            "151/151 [==============================] - 0s 725us/step - loss: 0.1044 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 981us/step - loss: 0.1438 - accuracy: 0.7994\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 1s 957us/step - loss: 0.1154 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1138 - accuracy: 0.8356\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1150 - accuracy: 0.8334\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8356\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 1s 989us/step - loss: 0.1105 - accuracy: 0.8396\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8425\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 993us/step - loss: 0.1082 - accuracy: 0.8432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5 HL: 0.838074 (0.005657)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 950us/step - loss: 0.1418 - accuracy: 0.7995\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 958us/step - loss: 0.1146 - accuracy: 0.8362\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8395\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8387\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8419\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8420\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8399\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8418\n",
            "151/151 [==============================] - 0s 761us/step - loss: 0.1137 - accuracy: 0.8300\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 994us/step - loss: 0.1456 - accuracy: 0.7948\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8362\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8409\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8386\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1061 - accuracy: 0.8459\n",
            "151/151 [==============================] - 0s 746us/step - loss: 0.1180 - accuracy: 0.8333\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1469 - accuracy: 0.7936\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8322\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8434\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8440\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8382\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8377\n",
            "151/151 [==============================] - 0s 647us/step - loss: 0.1096 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 962us/step - loss: 0.1398 - accuracy: 0.8076\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8352\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8380\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8370\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8404\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8410\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8461\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8358\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8424\n",
            "151/151 [==============================] - 1s 832us/step - loss: 0.1108 - accuracy: 0.8395\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1398 - accuracy: 0.7994\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 994us/step - loss: 0.1165 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 999us/step - loss: 0.1135 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8386\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8420\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8362\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8437\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8442\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8437\n",
            "151/151 [==============================] - 0s 764us/step - loss: 0.1133 - accuracy: 0.8342\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1447 - accuracy: 0.7945\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8383\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8367\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8404\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8394\n",
            "151/151 [==============================] - 0s 754us/step - loss: 0.1126 - accuracy: 0.8349\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1382 - accuracy: 0.8000\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8370\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8401\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8386\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8411\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8398\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8460\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8421\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8407\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8415\n",
            "151/151 [==============================] - 0s 778us/step - loss: 0.1093 - accuracy: 0.8412\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1458 - accuracy: 0.7946\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8357\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1147 - accuracy: 0.8317\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8313\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8363\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8423\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8405\n",
            "151/151 [==============================] - 0s 759us/step - loss: 0.1091 - accuracy: 0.8428\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1450 - accuracy: 0.7943\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8349\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8377\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8351\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8352\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8413\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8411\n",
            "151/151 [==============================] - 0s 739us/step - loss: 0.1056 - accuracy: 0.8452\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 985us/step - loss: 0.1371 - accuracy: 0.8051\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8367\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8368\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8396\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8400\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8369\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8429\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8426\n",
            "151/151 [==============================] - 0s 748us/step - loss: 0.1032 - accuracy: 0.8432\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 3s 1ms/step - loss: 0.1361 - accuracy: 0.8077\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8352\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8370\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8360\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8345\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8374\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8438\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1074 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6 HL: 0.838638 (0.004879)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JY1g2cgJH9I_",
        "outputId": "da0c50c6-0fe0-4f27-cb50-39d197aeb9cd"
      },
      "source": [
        "#2 Graph of No of Nodes vs Accuracy ans MSE of the Model\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "def creating_model1(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #one hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #2 hidden layer used on pca_Embedded dataset\n",
        "def creating_model2(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))              #two hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #3 hidden layer used on pca_Embedded dataset\n",
        "def creating_model3(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(20, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(30, activation='relu'))               #three hidden layers\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "\n",
        "  #4 hidden layer used on pca_Embedded dataset\n",
        "def creating_model4(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #four hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #5 hidden layer used on pca_Embedded dataset\n",
        "def creating_model5(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #five hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  #6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model6(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('10', KerasClassifier(build_fn=creating_model1, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('30', KerasClassifier(build_fn=creating_model2, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('60', KerasClassifier(build_fn=creating_model3, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('100', KerasClassifier(build_fn=creating_model4, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('145', KerasClassifier(build_fn=creating_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('195', KerasClassifier(build_fn=creating_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Accuracy Rate')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of Nodes')\n",
        "plt.ylabel('Generalization Accuracy')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "# again evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  mse = mean_squared_error(Y_test,  y_pred)\n",
        "  results.append(mse)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Squarred Error')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of Nodes')\n",
        "plt.ylabel('Generalization Error Rate')\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1701 - accuracy: 0.7741\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1191 - accuracy: 0.8290\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8363\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8298\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8359\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8389\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1651 - accuracy: 0.7797\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1235 - accuracy: 0.8255\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.8290\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8301\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8289\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8335\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8354\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8341\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8348\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1670 - accuracy: 0.7736\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.8246\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1180 - accuracy: 0.8339\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8352\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8380\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8421\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8415\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8415\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1730 - accuracy: 0.7671\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1254 - accuracy: 0.8213\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8290\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8311\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8388\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8367\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8389\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8418\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8395\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8410\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1737 - accuracy: 0.7611\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.8285\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8336\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8391\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8426\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8417\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8427\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8450\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8386\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8390\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1699 - accuracy: 0.7694\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1252 - accuracy: 0.8255\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.8282\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8417\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8389\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8388\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8389\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1718 - accuracy: 0.7603\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1218 - accuracy: 0.8265\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8387\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8412\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8380\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8377\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8351\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1746 - accuracy: 0.7439\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1242 - accuracy: 0.8282\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.8317\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1168 - accuracy: 0.8298\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8361\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8364\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8363\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8383\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8359\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1687 - accuracy: 0.7648\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1227 - accuracy: 0.8285\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8318\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8392\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8394\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8389\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8377\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1075 - accuracy: 0.8468\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1687 - accuracy: 0.7712\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1281 - accuracy: 0.8207\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1213 - accuracy: 0.8264\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8327\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8360\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8360\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8358\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8388\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8413\n",
            "10: 0.840097 (0.005760)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1660 - accuracy: 0.7716\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1219 - accuracy: 0.8259\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1155 - accuracy: 0.8336\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8420\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8390\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1065 - accuracy: 0.8433\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1071 - accuracy: 0.8422\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8402\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8393\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1665 - accuracy: 0.7703\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1198 - accuracy: 0.8284\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.8322\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8355\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8398\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8375\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8378\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8418\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1674 - accuracy: 0.7768\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1187 - accuracy: 0.8256\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8405\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8391\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8400\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8443\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8443\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1066 - accuracy: 0.8447\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8408\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1591 - accuracy: 0.7815\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1188 - accuracy: 0.8277\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8356\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8354\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8408\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8340\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8370\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1638 - accuracy: 0.7669\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1220 - accuracy: 0.8265\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1184 - accuracy: 0.8287\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8372\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8371\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8371\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8385\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8451\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8419\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1562 - accuracy: 0.7921\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8338\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8398\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8419\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8418\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8423\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8427\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8428\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8380\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1631 - accuracy: 0.7847\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1176 - accuracy: 0.8300\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8375\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8405\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8419\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8436\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8370\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8426\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1071 - accuracy: 0.8450\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1520 - accuracy: 0.7983\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1208 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1213 - accuracy: 0.8283\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1172 - accuracy: 0.8306\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8362\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8378\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8329\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8400\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1603 - accuracy: 0.7764\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1179 - accuracy: 0.8310\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1124 - accuracy: 0.8386\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8396\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8398\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8419\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8390\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8439\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8409\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1636 - accuracy: 0.7777\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1243 - accuracy: 0.8258\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1187 - accuracy: 0.8279\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8373\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1160 - accuracy: 0.8324\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1139 - accuracy: 0.8346\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8357\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8401\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8362\n",
            "30: 0.840163 (0.005188)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1534 - accuracy: 0.7858\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1182 - accuracy: 0.8310\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8387\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8377\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1072 - accuracy: 0.8436\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1072 - accuracy: 0.8431\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1530 - accuracy: 0.7901\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1215 - accuracy: 0.8267\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1163 - accuracy: 0.8283\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8431\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1068 - accuracy: 0.8434\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1053 - accuracy: 0.8481\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1493 - accuracy: 0.7955\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1197 - accuracy: 0.8312\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8416\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8423\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8370\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8432\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1069 - accuracy: 0.8433\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1066 - accuracy: 0.8434\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8387\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1484 - accuracy: 0.7997\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1215 - accuracy: 0.8314\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8374\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8362\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8344\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8410\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8362\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8373\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1548 - accuracy: 0.7905\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1160 - accuracy: 0.8332\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8359\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8416\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8394\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1070 - accuracy: 0.8467\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8429\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8454\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8430\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1563 - accuracy: 0.7902\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1161 - accuracy: 0.8283\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8384\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8394\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8410\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8431\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8436\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8395\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8392\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1533 - accuracy: 0.7851\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1175 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8387\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8405\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8355\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8368\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8383\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1068 - accuracy: 0.8452\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1071 - accuracy: 0.8441\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8415\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1592 - accuracy: 0.7790\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1214 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1170 - accuracy: 0.8298\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8344\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8355\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8423\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8387\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1640 - accuracy: 0.7739\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1209 - accuracy: 0.8289\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1177 - accuracy: 0.8283\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1169 - accuracy: 0.8282\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8321\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8316\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8358\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8358\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8381\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1463 - accuracy: 0.8012\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1189 - accuracy: 0.8332\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8377\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8393\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8431\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8418\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8434\n",
            "60: 0.841522 (0.003642)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1565 - accuracy: 0.7844\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1235 - accuracy: 0.8284\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1180 - accuracy: 0.8345\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8333\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8388\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8349\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8404\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1496 - accuracy: 0.7903\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1177 - accuracy: 0.8279\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8348\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8369\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8394\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8358\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8378\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8406\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1554 - accuracy: 0.7912\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1181 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8337\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8363\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8413\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8421\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8370\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1071 - accuracy: 0.8461\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8400\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1532 - accuracy: 0.7908\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1214 - accuracy: 0.8281\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1176 - accuracy: 0.8310\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8330\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8343\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8383\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8382\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1514 - accuracy: 0.7911\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1170 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8378\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8422\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8410\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8364\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8385\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8395\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8434\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8420\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1508 - accuracy: 0.7896\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1174 - accuracy: 0.8306\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8417\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8427\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8441\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8412\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8383\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1503 - accuracy: 0.7939\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1186 - accuracy: 0.8266\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1179 - accuracy: 0.8274\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8370\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8388\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1065 - accuracy: 0.8457\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8433\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1484 - accuracy: 0.7911\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8361\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8400\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8395\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1070 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1073 - accuracy: 0.8447\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1488 - accuracy: 0.7949\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1223 - accuracy: 0.8280\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1204 - accuracy: 0.8266\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1174 - accuracy: 0.8286\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8381\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8349\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8402\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8390\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1412 - accuracy: 0.8060\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1171 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8383\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.8309\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8412\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8365\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8393\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8354\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8402\n",
            "100: 0.837710 (0.007041)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1485 - accuracy: 0.7961\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1207 - accuracy: 0.8294\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8340\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8376\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8411\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8399\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1055 - accuracy: 0.8470\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8435\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1500 - accuracy: 0.7938\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1160 - accuracy: 0.8339\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8348\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8421\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8407\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8391\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8441\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1546 - accuracy: 0.7809\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1169 - accuracy: 0.8315\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8370\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8395\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8388\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8425\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8404\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8410\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1431 - accuracy: 0.8060\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1152 - accuracy: 0.8339\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8384\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8398\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8352\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8354\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8404\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8418\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8437\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1502 - accuracy: 0.7930\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1187 - accuracy: 0.8276\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8367\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8396\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1109 - accuracy: 0.8394\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8395\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1094 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1073 - accuracy: 0.8456\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1474 - accuracy: 0.7948\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1205 - accuracy: 0.8245\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1154 - accuracy: 0.8328\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1123 - accuracy: 0.8360\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8387\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8356\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8425\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8409\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1470 - accuracy: 0.8028\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1173 - accuracy: 0.8298\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8367\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8350\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8411\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8424\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8468\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8393\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1463 - accuracy: 0.7969\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1220 - accuracy: 0.8315\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1173 - accuracy: 0.8350\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1163 - accuracy: 0.8350\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.8325\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8385\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8361\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8420\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8379\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8408\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1460 - accuracy: 0.8011\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1165 - accuracy: 0.8293\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1135 - accuracy: 0.8349\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1117 - accuracy: 0.8375\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8408\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8405\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8418\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8429\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1438 - accuracy: 0.7996\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1193 - accuracy: 0.8326\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8359\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8335\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1104 - accuracy: 0.8418\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8398\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1093 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1102 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8414\n",
            "145: 0.838505 (0.005652)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1487 - accuracy: 0.7902\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1156 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1134 - accuracy: 0.8340\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8404\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1115 - accuracy: 0.8363\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1080 - accuracy: 0.8426\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1096 - accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1075 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.8430\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1097 - accuracy: 0.8389\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1451 - accuracy: 0.8038\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1142 - accuracy: 0.8358\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1120 - accuracy: 0.8380\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.8352\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1110 - accuracy: 0.8365\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1106 - accuracy: 0.8398\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1116 - accuracy: 0.8347\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1073 - accuracy: 0.8463\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1081 - accuracy: 0.8409\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1094 - accuracy: 0.8411\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1464 - accuracy: 0.8009\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1161 - accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1129 - accuracy: 0.8323\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.8415\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.8408\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1087 - accuracy: 0.8422\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1084 - accuracy: 0.8432\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1087 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1070 - accuracy: 0.8458\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1477 - accuracy: 0.7927\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1184 - accuracy: 0.8277\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1132 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1121 - accuracy: 0.8369\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1117 - accuracy: 0.8361\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1087 - accuracy: 0.8399\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1094 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1113 - accuracy: 0.8379\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1080 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1094 - accuracy: 0.8419\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1495 - accuracy: 0.7876\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1149 - accuracy: 0.8326\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1126 - accuracy: 0.8351\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8413\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1109 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8397\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.8399\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1074 - accuracy: 0.8441\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1091 - accuracy: 0.8432\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1521 - accuracy: 0.7889\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1169 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1123 - accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8431\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1118 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1090 - accuracy: 0.8420\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1083 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.8394\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1515 - accuracy: 0.7876\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1234 - accuracy: 0.8249\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1157 - accuracy: 0.8310\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1117 - accuracy: 0.8426\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.8416\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1113 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1087 - accuracy: 0.8431\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.8384\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1093 - accuracy: 0.8436\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1079 - accuracy: 0.8437\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1486 - accuracy: 0.7936\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1192 - accuracy: 0.8286\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1137 - accuracy: 0.8337\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1125 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1094 - accuracy: 0.8409\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1090 - accuracy: 0.8401\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8379\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1081 - accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1086 - accuracy: 0.8415\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1450 - accuracy: 0.7990\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1180 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1129 - accuracy: 0.8371\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1115 - accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1102 - accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1112 - accuracy: 0.8379\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1110 - accuracy: 0.8385\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1090 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1088 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1447 - accuracy: 0.7999\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1162 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1148 - accuracy: 0.8321\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1120 - accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.8427\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8413\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1084 - accuracy: 0.8435\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.8418\n",
            "195: 0.839566 (0.005883)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1656 - accuracy: 0.7609\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1243 - accuracy: 0.8259\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1168 - accuracy: 0.8313\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8350\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8345\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8367\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8390\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8393\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1696 - accuracy: 0.7711\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1257 - accuracy: 0.8280\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1202 - accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8324\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1124 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.8311\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8401\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8392\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8381\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8385\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1684 - accuracy: 0.7826\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1283 - accuracy: 0.8221\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1227 - accuracy: 0.8276\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1194 - accuracy: 0.8274\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8336\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.8322\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8345\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8379\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8378\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1679 - accuracy: 0.7749\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1277 - accuracy: 0.8244\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1198 - accuracy: 0.8324\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1164 - accuracy: 0.8287\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8311\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8414\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8438\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8374\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8427\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1736 - accuracy: 0.7613\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1238 - accuracy: 0.8261\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8340\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8362\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8423\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8373\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8385\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8388\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1067 - accuracy: 0.8459\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1603 - accuracy: 0.7839\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1207 - accuracy: 0.8260\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8381\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8402\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8418\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8428\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1066 - accuracy: 0.8461\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8438\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1072 - accuracy: 0.8436\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1769 - accuracy: 0.7589\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1246 - accuracy: 0.8282\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1180 - accuracy: 0.8326\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1163 - accuracy: 0.8337\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8364\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8421\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8405\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8360\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8365\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1687 - accuracy: 0.7712\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1217 - accuracy: 0.8274\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8309\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8341\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8349\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8417\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8427\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1723 - accuracy: 0.7714\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1185 - accuracy: 0.8331\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1164 - accuracy: 0.8325\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8302\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8372\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8378\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8385\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8361\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8393\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1614 - accuracy: 0.7711\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1231 - accuracy: 0.8265\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1178 - accuracy: 0.8322\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8361\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8364\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8368\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8385\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8439\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8366\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8397\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1714 - accuracy: 0.7621\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1221 - accuracy: 0.8244\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1157 - accuracy: 0.8318\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8392\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8373\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8376\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8379\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8402\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1074 - accuracy: 0.8441\n",
            "10: 0.839832 (0.004716)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1630 - accuracy: 0.7824\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1185 - accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1138 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8431\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8425\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8387\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1521 - accuracy: 0.7983\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1243 - accuracy: 0.8279\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1208 - accuracy: 0.8304\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1190 - accuracy: 0.8302\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.8335\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1154 - accuracy: 0.8325\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1147 - accuracy: 0.8343\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1157 - accuracy: 0.8316\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8344\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8361\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1610 - accuracy: 0.7797\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1228 - accuracy: 0.8252\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8397\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8364\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8374\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8403\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8386\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1603 - accuracy: 0.7887\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1280 - accuracy: 0.8191\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1201 - accuracy: 0.8286\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8355\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8379\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8373\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8385\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1643 - accuracy: 0.7802\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1295 - accuracy: 0.8195\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1250 - accuracy: 0.8213\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1184 - accuracy: 0.8308\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.8317\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8338\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8370\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8383\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8385\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1606 - accuracy: 0.7830\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1186 - accuracy: 0.8301\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8390\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1061 - accuracy: 0.8473\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8401\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8418\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1617 - accuracy: 0.7719\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1193 - accuracy: 0.8306\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8397\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8384\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8388\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8352\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8402\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8420\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8415\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8413\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1569 - accuracy: 0.7831\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1163 - accuracy: 0.8323\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8410\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8417\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8411\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8436\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8431\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1590 - accuracy: 0.7862\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1192 - accuracy: 0.8297\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8328\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8392\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8330\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8369\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8404\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8395\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1562 - accuracy: 0.7902\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1167 - accuracy: 0.8317\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8374\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8413\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8424\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8376\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8421\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8434\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8409\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8424\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1628 - accuracy: 0.7751\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1190 - accuracy: 0.8311\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8312\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8398\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8407\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8430\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8394\n",
            "30: 0.840760 (0.004753)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1551 - accuracy: 0.7929\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1155 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8358\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8381\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8394\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8380\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8369\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8394\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1538 - accuracy: 0.7819\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1162 - accuracy: 0.8319\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8378\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8416\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8347\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8363\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1581 - accuracy: 0.7877\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1237 - accuracy: 0.8302\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1209 - accuracy: 0.8292\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8324\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8358\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8350\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1124 - accuracy: 0.8343\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1071 - accuracy: 0.8444\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1577 - accuracy: 0.7821\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1156 - accuracy: 0.8318\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1124 - accuracy: 0.8384\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8395\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8424\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8421\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8414\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1069 - accuracy: 0.8443\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8432\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1465 - accuracy: 0.8072\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.8297\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1137 - accuracy: 0.8332\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8400\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8369\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8389\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8377\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1066 - accuracy: 0.8441\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8402\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1637 - accuracy: 0.7722\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1155 - accuracy: 0.8340\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8421\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8408\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8387\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8416\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1482 - accuracy: 0.7959\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1186 - accuracy: 0.8274\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8385\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8415\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8402\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8381\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1063 - accuracy: 0.8453\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8408\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1473 - accuracy: 0.8004\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1183 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1162 - accuracy: 0.8311\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1145 - accuracy: 0.8346\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8337\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8361\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8376\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8425\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8397\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1531 - accuracy: 0.7857\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1184 - accuracy: 0.8269\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8329\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8400\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8403\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8397\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8426\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1530 - accuracy: 0.7915\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1226 - accuracy: 0.8205\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.8335\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1163 - accuracy: 0.8319\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8358\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8417\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8344\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1521 - accuracy: 0.7905\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8368\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8373\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8383\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8363\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8417\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8407\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1066 - accuracy: 0.8432\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1075 - accuracy: 0.8422\n",
            "60: 0.836384 (0.006937)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1480 - accuracy: 0.7938\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1167 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8374\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8381\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1077 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1600 - accuracy: 0.7732\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1173 - accuracy: 0.8299\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8382\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8390\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8349\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1074 - accuracy: 0.8455\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1075 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8437\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1521 - accuracy: 0.7869\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1193 - accuracy: 0.8270\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1173 - accuracy: 0.8273\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8340\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8346\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8360\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8363\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8407\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1539 - accuracy: 0.7935\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1187 - accuracy: 0.8322\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8349\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8380\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8369\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8430\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8413\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1547 - accuracy: 0.7868\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1262 - accuracy: 0.8214\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8327\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8431\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8383\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8434\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1062 - accuracy: 0.8469\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1529 - accuracy: 0.7929\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1178 - accuracy: 0.8304\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1135 - accuracy: 0.8344\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8382\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8377\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8368\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1553 - accuracy: 0.7866\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1156 - accuracy: 0.8352\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8399\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8444\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8397\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8424\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1073 - accuracy: 0.8439\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8429\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1513 - accuracy: 0.7937\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8352\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8386\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8404\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8361\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1079 - accuracy: 0.8422\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1063 - accuracy: 0.8435\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8387\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8431\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1441 - accuracy: 0.7992\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8338\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1128 - accuracy: 0.8353\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8386\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8426\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8439\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8392\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8415\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1566 - accuracy: 0.7783\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1172 - accuracy: 0.8286\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1157 - accuracy: 0.8321\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8358\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8363\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1072 - accuracy: 0.8461\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 3s 1ms/step - loss: 0.1464 - accuracy: 0.7989\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1161 - accuracy: 0.8299\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8322\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8373\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1124 - accuracy: 0.8343\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8362\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8437\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8394\n",
            "100: 0.840661 (0.005215)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1433 - accuracy: 0.8006\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1163 - accuracy: 0.8291\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1131 - accuracy: 0.8381\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8421\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8415\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1076 - accuracy: 0.8466\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1071 - accuracy: 0.8437\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8431\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1429 - accuracy: 0.7979\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8387\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8360\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8411\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8373\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8371\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1070 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8430\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1066 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1435 - accuracy: 0.8034\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.8298\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8430\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.8438\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8433\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8386\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1055 - accuracy: 0.8479\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1486 - accuracy: 0.7917\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8354\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8422\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8395\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8397\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8440\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1084 - accuracy: 0.8439\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1114 - accuracy: 0.8365\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1467 - accuracy: 0.7957\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1253 - accuracy: 0.8254\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.8310\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1161 - accuracy: 0.8307\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8363\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8371\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8391\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8349\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8422\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1482 - accuracy: 0.7991\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1238 - accuracy: 0.8283\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1154 - accuracy: 0.8312\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8375\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8397\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1075 - accuracy: 0.8445\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1073 - accuracy: 0.8433\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8401\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1457 - accuracy: 0.7966\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8337\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8316\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8375\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8410\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8386\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1573 - accuracy: 0.7856\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1180 - accuracy: 0.8306\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8396\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8399\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8401\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1085 - accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1070 - accuracy: 0.8426\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1418 - accuracy: 0.8005\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1168 - accuracy: 0.8323\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8373\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8381\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8374\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8407\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8397\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8377\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8412\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1454 - accuracy: 0.7975\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1172 - accuracy: 0.8301\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8351\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.8384\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1100 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 3s 1ms/step - loss: 0.1467 - accuracy: 0.7932\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1171 - accuracy: 0.8284\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8351\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8351\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8432\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1092 - accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1074 - accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8403\n",
            "145: 0.839003 (0.006507)\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 2ms/step - loss: 0.1406 - accuracy: 0.7988\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1154 - accuracy: 0.8351\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1106 - accuracy: 0.8402\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1107 - accuracy: 0.8413\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8420\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1102 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.8418\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8397\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1064 - accuracy: 0.8453\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1442 - accuracy: 0.7986\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1188 - accuracy: 0.8297\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8379\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8404\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8380\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1102 - accuracy: 0.8438\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8422\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1090 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1111 - accuracy: 0.8380\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1486 - accuracy: 0.7995\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1208 - accuracy: 0.8233\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1095 - accuracy: 0.8419\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8394\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8403\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8371\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1093 - accuracy: 0.8404\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1471 - accuracy: 0.7949\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1210 - accuracy: 0.8295\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1104 - accuracy: 0.8404\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1112 - accuracy: 0.8365\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1096 - accuracy: 0.8405\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1066 - accuracy: 0.8453\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8416\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1462 - accuracy: 0.7986\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1150 - accuracy: 0.8354\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1133 - accuracy: 0.8333\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1126 - accuracy: 0.8344\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8428\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8409\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1081 - accuracy: 0.8440\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1078 - accuracy: 0.8453\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1533 - accuracy: 0.7874\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1179 - accuracy: 0.8278\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1147 - accuracy: 0.8326\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8338\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8373\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1088 - accuracy: 0.8437\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8380\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1072 - accuracy: 0.8448\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1467 - accuracy: 0.7985\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1150 - accuracy: 0.8341\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1145 - accuracy: 0.8357\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8382\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1115 - accuracy: 0.8397\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8407\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8378\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1087 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1098 - accuracy: 0.8419\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1443 - accuracy: 0.8000\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1204 - accuracy: 0.8264\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.8294\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1116 - accuracy: 0.8400\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1123 - accuracy: 0.8368\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.8404\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1117 - accuracy: 0.8363\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.8394\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1518 - accuracy: 0.7924\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1168 - accuracy: 0.8314\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1145 - accuracy: 0.8321\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1137 - accuracy: 0.8333\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1129 - accuracy: 0.8344\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8398\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1103 - accuracy: 0.8403\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8364\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8351\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.8378\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 3s 1ms/step - loss: 0.1502 - accuracy: 0.7981\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1175 - accuracy: 0.8337\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1122 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1125 - accuracy: 0.8340\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1094 - accuracy: 0.8434\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.8361\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1084 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1086 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1115 - accuracy: 0.8370\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 2ms/step - loss: 0.1078 - accuracy: 0.8419\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 3s 1ms/step - loss: 0.1443 - accuracy: 0.7953\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 2ms/step - loss: 0.1159 - accuracy: 0.8308\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 2ms/step - loss: 0.1133 - accuracy: 0.8353\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 2ms/step - loss: 0.1096 - accuracy: 0.8369\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 2ms/step - loss: 0.1112 - accuracy: 0.8383\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 2ms/step - loss: 0.1073 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1094 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 2ms/step - loss: 0.1079 - accuracy: 0.8396\n",
            "195: 0.841456 (0.005216)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rsAtOjyRLgpQ",
        "outputId": "d9678cb9-80af-49d7-ae91-131fc73f44cd"
      },
      "source": [
        "#3 Graph of No of Epochs for our model  to see Accuracy and MSE of the Model\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "\n",
        "\n",
        "  #6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model6(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# prepare models, as this model6 is chosen now we will analyse the effect of epochs increase to this model with same batch size\n",
        "models = []\n",
        "models.append(('6', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 6, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('8', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 8, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('10', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('12', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 12, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('14', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 14, init = 'glorot_uniform', optimizer='WAME')))\n",
        "models.append(('16', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 16, init = 'glorot_uniform', optimizer='WAME')))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Accuracy Rate')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of Epochs')\n",
        "plt.ylabel('Generalization Accuracy')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "# again evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  mse = mean_squared_error(Y_test,  y_pred)\n",
        "  results.append(mse)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Squarred Error')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('No. of Epochs')\n",
        "plt.ylabel('Generalization Error Rate')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1441 - accuracy: 0.7976\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8334\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8333\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8367\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8422\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1578 - accuracy: 0.7806\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 952us/step - loss: 0.1158 - accuracy: 0.8348\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 991us/step - loss: 0.1140 - accuracy: 0.8352\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8337\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8402\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8419\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1515 - accuracy: 0.7876\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.8320\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8386\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8377\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8375\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8400\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 1s 994us/step - loss: 0.1440 - accuracy: 0.8000\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8341\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 961us/step - loss: 0.1111 - accuracy: 0.8394\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8343\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 992us/step - loss: 0.1132 - accuracy: 0.8338\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 950us/step - loss: 0.1104 - accuracy: 0.8380\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1522 - accuracy: 0.7828\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1155 - accuracy: 0.8337\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8406\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8405\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8433\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8416\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1472 - accuracy: 0.7914\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8344\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 995us/step - loss: 0.1138 - accuracy: 0.8361\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8371\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8370\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8450\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1543 - accuracy: 0.7802\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 965us/step - loss: 0.1157 - accuracy: 0.8350\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 988us/step - loss: 0.1143 - accuracy: 0.8319\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8380\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 986us/step - loss: 0.1125 - accuracy: 0.8350\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8399\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1487 - accuracy: 0.7932\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8367\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8341\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8359\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8430\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8374\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1482 - accuracy: 0.7891\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.8288\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8368\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8397\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8352\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8422\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 1s 961us/step - loss: 0.1470 - accuracy: 0.7940\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 964us/step - loss: 0.1139 - accuracy: 0.8364\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 992us/step - loss: 0.1122 - accuracy: 0.8381\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8356\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8382\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8391\n",
            "6: 0.839666 (0.005798)\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1525 - accuracy: 0.7850\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8302\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8400\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8331\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 987us/step - loss: 0.1122 - accuracy: 0.8367\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8380\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8357\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 989us/step - loss: 0.1126 - accuracy: 0.8366\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1470 - accuracy: 0.7916\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8300\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8337\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8337\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8415\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8387\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8328\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8412\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1484 - accuracy: 0.7942\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8355\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8357\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8330\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8354\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8418\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8455\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8436\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1447 - accuracy: 0.7915\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8376\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8335\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8390\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8314\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8417\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8431\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8415\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1499 - accuracy: 0.7955\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8367\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8381\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8427\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8416\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8371\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8399\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1570 - accuracy: 0.7797\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.8277\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8333\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8360\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8390\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8425\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8399\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8389\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1530 - accuracy: 0.7881\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8296\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8379\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8373\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8361\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8328\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8407\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8345\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 2ms/step - loss: 0.1495 - accuracy: 0.7829\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8334\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8345\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8378\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8433\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8403\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8355\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8373\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 982us/step - loss: 0.1478 - accuracy: 0.7922\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.8326\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8405\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8377\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8353\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8374\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8375\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8424\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1548 - accuracy: 0.7786\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8293\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8296\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8343\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8379\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8335\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8398\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8377\n",
            "8: 0.837577 (0.004850)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1524 - accuracy: 0.7881\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8336\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8318\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8405\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8394\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8451\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8401\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1521 - accuracy: 0.7880\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.8316\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8376\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8336\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8351\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8428\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1509 - accuracy: 0.7977\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1196 - accuracy: 0.8285\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8313\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8370\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8382\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8374\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8384\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8376\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8438\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1458 - accuracy: 0.7927\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8310\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8377\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8305\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8370\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8376\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8404\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8386\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8366\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8456\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1499 - accuracy: 0.7906\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8320\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8340\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8409\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8365\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8431\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1545 - accuracy: 0.7834\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8376\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8365\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8375\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8419\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8417\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8436\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8448\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1514 - accuracy: 0.7883\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1193 - accuracy: 0.8248\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8342\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8316\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8405\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8372\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8383\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8408\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1478 - accuracy: 0.7843\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.8316\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8360\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8374\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8441\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8402\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1545 - accuracy: 0.7820\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8365\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8340\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8345\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8419\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8426\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8416\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8413\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.7850\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8305\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8334\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8363\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8393\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8394\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8377\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8450\n",
            "10: 0.839865 (0.005541)\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1480 - accuracy: 0.7906\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8362\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8335\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8417\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8365\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8394\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8416\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8442\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8428\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1058 - accuracy: 0.8458\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8407\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1058 - accuracy: 0.8440\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1550 - accuracy: 0.7881\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8366\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8378\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8396\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8410\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8405\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8383\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8445\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8434\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8393\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8401\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8379\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1542 - accuracy: 0.7919\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8340\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8340\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8382\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8403\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8413\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8410\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8422\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8451\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8441\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8419\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1487 - accuracy: 0.7897\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.8335\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8392\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8378\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8386\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8386\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8390\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8395\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8365\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8410\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1060 - accuracy: 0.8477\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8432\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1487 - accuracy: 0.7891\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.8302\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8345\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8445\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8367\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8384\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8411\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8408\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8409\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8406\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1052 - accuracy: 0.8477\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8433\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1492 - accuracy: 0.7900\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8335\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8384\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8388\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8458\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8445\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8475\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8411\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8414\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8459\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8434\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8407\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1528 - accuracy: 0.7837\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1197 - accuracy: 0.8262\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8361\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8327\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8356\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8387\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8431\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8429\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8397\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8416\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8399\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8438\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1480 - accuracy: 0.8009\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8335\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8348\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8350\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8361\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8386\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8335\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8429\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8417\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8405\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8410\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8445\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1462 - accuracy: 0.7949\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8344\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8311\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8310\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8339\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8337\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8401\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8419\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8416\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8420\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8436\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8415\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1520 - accuracy: 0.7866\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8327\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8344\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8392\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8380\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8396\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8445\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8404\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8421\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8467\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8399\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1066 - accuracy: 0.8451\n",
            "12: 0.840959 (0.004607)\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1437 - accuracy: 0.8021\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8318\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8389\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8385\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8418\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8389\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8394\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8430\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8443\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8419\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1048 - accuracy: 0.8494\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8432\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1055 - accuracy: 0.8479\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8458\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1513 - accuracy: 0.7896\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8331\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8386\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8382\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8358\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8364\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8354\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8440\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8420\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8408\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8382\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8418\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8416\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8433\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1528 - accuracy: 0.7924\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8318\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8426\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8358\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8400\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8420\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8424\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8440\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8427\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8410\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8435\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8422\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8453\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8434\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1487 - accuracy: 0.7923\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8283\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8314\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8373\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8406\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8373\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8391\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8370\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8432\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8438\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8446\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8417\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8404\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8465\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1561 - accuracy: 0.7832\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8294\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8329\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8357\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8360\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8385\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8378\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8394\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8410\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8424\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8425\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8425\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8438\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8432\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1614 - accuracy: 0.7756\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8367\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8362\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8328\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8355\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8337\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8357\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8403\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8377\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8421\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8423\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8382\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8418\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8447\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1418 - accuracy: 0.7993\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8343\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8348\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8355\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8399\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8424\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8399\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8442\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8425\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8435\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8395\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8410\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8461\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8414\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1635 - accuracy: 0.7746\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8330\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8384\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8394\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8388\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8453\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8426\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8426\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8397\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8422\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8410\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8401\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8407\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8390\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1480 - accuracy: 0.7887\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1182 - accuracy: 0.8279\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8343\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8314\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8318\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8370\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8349\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8369\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8411\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8372\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8394\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8420\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8374\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8390\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1541 - accuracy: 0.7875\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8313\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8378\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8384\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8366\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8363\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8393\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8453\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8438\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8446\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8401\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8410\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8385\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8420\n",
            "14: 0.842550 (0.003714)\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1455 - accuracy: 0.7980\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8330\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8420\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8372\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8375\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8333\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8387\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8423\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8396\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8432\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8413\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8425\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8405\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1062 - accuracy: 0.8462\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8417\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1057 - accuracy: 0.8458\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1496 - accuracy: 0.7876\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8314\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8334\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8354\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8335\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8366\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8357\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8363\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8400\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8391\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8379\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8396\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8403\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8415\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1066 - accuracy: 0.8443\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8391\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1436 - accuracy: 0.7954\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8313\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8368\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8405\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8416\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8388\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8410\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8398\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8400\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8425\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8400\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8416\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8436\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8445\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8404\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8424\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1521 - accuracy: 0.7775\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.8271\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8324\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8368\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8421\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8440\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8361\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8454\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8415\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8434\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8422\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8414\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8423\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 2ms/step - loss: 0.1103 - accuracy: 0.8384\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8445\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 2ms/step - loss: 0.1088 - accuracy: 0.8416\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1463 - accuracy: 0.7987\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8371\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8339\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8401\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8418\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8414\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8390\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8423\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8412\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8443\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8401\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8424\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8453\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8448\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1053 - accuracy: 0.8459\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8408\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1534 - accuracy: 0.7855\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8322\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8328\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8389\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8399\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8387\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8417\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8449\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8443\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8405\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1064 - accuracy: 0.8446\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1058 - accuracy: 0.8458\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8434\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8426\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8456\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1057 - accuracy: 0.8487\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1565 - accuracy: 0.7848\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8306\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8351\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8374\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8396\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8373\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8415\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8432\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8408\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8424\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8395\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8413\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1065 - accuracy: 0.8453\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8445\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8424\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8418\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1521 - accuracy: 0.7869\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8324\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8353\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8296\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8377\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8378\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8428\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8412\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8374\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8430\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8397\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8429\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8417\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8423\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8429\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8426\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1487 - accuracy: 0.7900\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.8318\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8338\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8376\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8349\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8421\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8419\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8417\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8369\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8426\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8356\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8413\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8415\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8423\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8446\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8389\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1464 - accuracy: 0.7937\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.8312\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8348\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8365\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8360\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8388\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8388\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8430\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8405\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8405\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8375\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8392\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8413\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8389\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8393\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8369\n",
            "16: 0.842783 (0.003944)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1416 - accuracy: 0.7971\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8381\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8367\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8392\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8368\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1509 - accuracy: 0.7859\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8314\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8369\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8417\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8404\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8420\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1432 - accuracy: 0.7941\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8340\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8355\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8386\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8431\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8420\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1435 - accuracy: 0.7966\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8368\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8408\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8410\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8379\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8415\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1439 - accuracy: 0.7988\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.8331\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8338\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8344\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8354\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8447\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1482 - accuracy: 0.7921\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8369\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8381\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8409\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8384\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8431\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1516 - accuracy: 0.7927\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8315\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8402\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8395\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8399\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8392\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1558 - accuracy: 0.7849\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8307\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8296\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8399\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8390\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8395\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1420 - accuracy: 0.8061\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8291\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8364\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8330\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8331\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8372\n",
            "Epoch 1/6\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1509 - accuracy: 0.7877\n",
            "Epoch 2/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8358\n",
            "Epoch 3/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8358\n",
            "Epoch 4/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8381\n",
            "Epoch 5/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8337\n",
            "Epoch 6/6\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8389\n",
            "Epoch 1/6\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1470 - accuracy: 0.7945\n",
            "Epoch 2/6\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8354\n",
            "Epoch 3/6\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8384\n",
            "Epoch 4/6\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8405\n",
            "Epoch 5/6\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8389\n",
            "Epoch 6/6\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8446\n",
            "6: 0.840992 (0.003947)\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1535 - accuracy: 0.7837\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8350\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8394\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8430\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8418\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8394\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8387\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1354 - accuracy: 0.8091\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8337\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8357\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8377\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8416\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8400\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8415\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8429\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1537 - accuracy: 0.7862\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1184 - accuracy: 0.8260\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8419\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8369\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8444\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8404\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8371\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8379\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1498 - accuracy: 0.7891\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8384\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8382\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 2ms/step - loss: 0.1094 - accuracy: 0.8411\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 2ms/step - loss: 0.1078 - accuracy: 0.8439\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8424\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8407\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8444\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1486 - accuracy: 0.7912\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8399\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8440\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8402\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8417\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8403\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8408\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8398\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1504 - accuracy: 0.7946\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8361\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8369\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8397\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8387\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8366\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8420\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8433\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1527 - accuracy: 0.7821\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8331\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8354\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8340\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8408\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8362\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8366\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8397\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1508 - accuracy: 0.7887\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8306\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8330\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8394\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8369\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8390\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8389\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8382\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1514 - accuracy: 0.7811\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1155 - accuracy: 0.8326\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8396\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8370\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8385\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8426\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8404\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8420\n",
            "Epoch 1/8\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1519 - accuracy: 0.7865\n",
            "Epoch 2/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8419\n",
            "Epoch 3/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8399\n",
            "Epoch 4/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8393\n",
            "Epoch 5/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8395\n",
            "Epoch 6/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8372\n",
            "Epoch 7/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8434\n",
            "Epoch 8/8\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8411\n",
            "Epoch 1/8\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1487 - accuracy: 0.7856\n",
            "Epoch 2/8\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8355\n",
            "Epoch 3/8\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8366\n",
            "Epoch 4/8\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8407\n",
            "Epoch 5/8\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8383\n",
            "Epoch 6/8\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8411\n",
            "Epoch 7/8\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8375\n",
            "Epoch 8/8\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8389\n",
            "8: 0.839169 (0.005438)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1521 - accuracy: 0.7872\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8371\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8404\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8371\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8419\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8405\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8436\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1541 - accuracy: 0.7828\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8336\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8370\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8308\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8371\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8369\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1451 - accuracy: 0.8023\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8326\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8391\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8382\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1064 - accuracy: 0.8454\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8392\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8432\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8373\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8346\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1476 - accuracy: 0.7916\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1154 - accuracy: 0.8298\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8343\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8374\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8388\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8401\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8429\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8424\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1479 - accuracy: 0.7862\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8353\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8337\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8369\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8389\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1502 - accuracy: 0.7868\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8386\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8394\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8388\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8453\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1065 - accuracy: 0.8447\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8373\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1056 - accuracy: 0.8488\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1553 - accuracy: 0.7852\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8397\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8418\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8381\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8423\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8429\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8445\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1428 - accuracy: 0.7987\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8289\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8290\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8383\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8421\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1441 - accuracy: 0.7978\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8356\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8385\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8400\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8398\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8438\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8422\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8393\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1621 - accuracy: 0.7772\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1213 - accuracy: 0.8268\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8372\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8385\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8420\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8451\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8431\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1440 - accuracy: 0.7959\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8317\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8398\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8390\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8405\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8412\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8398\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8400\n",
            "10: 0.841059 (0.005606)\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1589 - accuracy: 0.7773\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8329\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8353\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8382\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8412\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8366\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8403\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8392\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8430\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8425\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1064 - accuracy: 0.8434\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1527 - accuracy: 0.7907\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8321\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8366\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8354\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8355\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8445\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8382\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8418\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8404\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8423\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8462\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8407\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1381 - accuracy: 0.8094\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8361\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8332\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8401\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8444\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8420\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8428\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8388\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8377\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8348\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8426\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8425\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1445 - accuracy: 0.7973\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8308\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8329\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8349\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8342\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8329\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8386\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8363\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8335\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8402\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8386\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8414\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1493 - accuracy: 0.7854\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8309\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8350\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8438\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8409\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8383\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8418\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8394\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8407\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8410\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8396\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8443\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1561 - accuracy: 0.7823\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8335\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8368\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8413\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8425\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8401\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8346\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8389\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8428\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8409\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8440\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8408\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1558 - accuracy: 0.7768\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8312\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8351\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8376\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8396\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8403\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8404\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8425\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8404\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8403\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8443\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8428\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1542 - accuracy: 0.7831\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8323\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8326\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8387\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8363\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8416\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8388\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8392\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8406\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8371\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8385\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8430\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1573 - accuracy: 0.7814\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1239 - accuracy: 0.8244\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.8326\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8363\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8384\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8387\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8401\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8374\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8392\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8370\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8390\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8409\n",
            "Epoch 1/12\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1441 - accuracy: 0.7965\n",
            "Epoch 2/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8291\n",
            "Epoch 3/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8385\n",
            "Epoch 4/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8358\n",
            "Epoch 5/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8430\n",
            "Epoch 6/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8381\n",
            "Epoch 7/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8418\n",
            "Epoch 8/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8437\n",
            "Epoch 9/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8451\n",
            "Epoch 10/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8431\n",
            "Epoch 11/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1064 - accuracy: 0.8470\n",
            "Epoch 12/12\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8454\n",
            "Epoch 1/12\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1499 - accuracy: 0.7946\n",
            "Epoch 2/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1187 - accuracy: 0.8267\n",
            "Epoch 3/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8373\n",
            "Epoch 4/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8391\n",
            "Epoch 5/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8394\n",
            "Epoch 6/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8410\n",
            "Epoch 7/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8432\n",
            "Epoch 8/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8428\n",
            "Epoch 9/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8414\n",
            "Epoch 10/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8418\n",
            "Epoch 11/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1062 - accuracy: 0.8462\n",
            "Epoch 12/12\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1065 - accuracy: 0.8435\n",
            "12: 0.839699 (0.006576)\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1512 - accuracy: 0.7888\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8338\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8417\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8405\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8382\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8390\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8399\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8407\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1057 - accuracy: 0.8452\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8415\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1066 - accuracy: 0.8441\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8432\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8405\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1488 - accuracy: 0.7897\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8324\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8361\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8403\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8396\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8368\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8418\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8407\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8381\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8451\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8423\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8399\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8415\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8414\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1558 - accuracy: 0.7869\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.8298\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8329\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8347\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8354\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8384\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8378\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8410\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8405\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8442\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8427\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8445\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8413\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8407\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1520 - accuracy: 0.7844\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1147 - accuracy: 0.8320\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8346\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8344\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8420\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8401\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8433\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8440\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8408\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8432\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8405\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8424\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8394\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8438\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1516 - accuracy: 0.7861\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8385\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8334\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8381\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8376\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8442\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8403\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8377\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8414\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8420\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1059 - accuracy: 0.8471\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8433\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8432\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1053 - accuracy: 0.8497\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1580 - accuracy: 0.7817\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.8346\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8372\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8354\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8426\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8414\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8413\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8404\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8399\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8447\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8443\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8445\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8433\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8418\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1505 - accuracy: 0.7894\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8367\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8381\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8359\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8396\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8425\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8374\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8421\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8407\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8421\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8440\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8411\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8394\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8403\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1560 - accuracy: 0.7831\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8364\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8374\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8414\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8418\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8382\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8423\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8413\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8456\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1065 - accuracy: 0.8446\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8368\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1049 - accuracy: 0.8472\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8387\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8417\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1551 - accuracy: 0.7848\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.8308\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8387\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8377\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8376\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8374\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8422\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8406\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8411\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8427\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8408\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1060 - accuracy: 0.8485\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8405\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8425\n",
            "Epoch 1/14\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1408 - accuracy: 0.8041\n",
            "Epoch 2/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8325\n",
            "Epoch 3/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8351\n",
            "Epoch 4/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8398\n",
            "Epoch 5/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8377\n",
            "Epoch 6/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8430\n",
            "Epoch 7/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8390\n",
            "Epoch 8/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8386\n",
            "Epoch 9/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8405\n",
            "Epoch 10/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8420\n",
            "Epoch 11/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8428\n",
            "Epoch 12/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8410\n",
            "Epoch 13/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8407\n",
            "Epoch 14/14\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8393\n",
            "Epoch 1/14\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1453 - accuracy: 0.7957\n",
            "Epoch 2/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8334\n",
            "Epoch 3/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8307\n",
            "Epoch 4/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8347\n",
            "Epoch 5/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8392\n",
            "Epoch 6/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8388\n",
            "Epoch 7/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8419\n",
            "Epoch 8/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8383\n",
            "Epoch 9/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8392\n",
            "Epoch 10/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8411\n",
            "Epoch 11/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8422\n",
            "Epoch 12/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8440\n",
            "Epoch 13/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 14/14\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8402\n",
            "14: 0.841556 (0.004638)\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1497 - accuracy: 0.7935\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8333\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8397\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8336\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8400\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8402\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8333\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8403\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8413\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8384\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1050 - accuracy: 0.8481\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8426\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8468\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8436\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8398\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1061 - accuracy: 0.8435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1469 - accuracy: 0.7973\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.8330\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8385\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8345\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8306\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8377\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8366\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8370\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8393\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8395\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8438\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8420\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8435\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8439\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8446\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1451 - accuracy: 0.7933\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8345\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8327\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8373\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8381\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8366\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8407\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8435\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8418\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8437\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8448\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8394\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8424\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8459\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8434\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8454\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1488 - accuracy: 0.7858\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8376\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8395\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8392\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8363\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8443\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8399\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8394\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8396\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8420\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8445\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8402\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8415\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8433\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1060 - accuracy: 0.8435\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8407\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1510 - accuracy: 0.7921\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8365\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8369\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8401\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8395\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8360\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8456\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8413\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8426\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8424\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1065 - accuracy: 0.8446\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8446\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1062 - accuracy: 0.8461\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8453\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8447\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1066 - accuracy: 0.8443\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1510 - accuracy: 0.7871\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8322\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8308\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8349\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8317\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8421\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8423\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8407\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8426\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8421\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8465\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8429\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8408\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8429\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8446\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8448\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1437 - accuracy: 0.7970\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1147 - accuracy: 0.8331\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8329\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8368\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8355\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8359\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8354\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8368\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8387\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8329\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8354\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8372\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8389\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8385\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8375\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8364\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1449 - accuracy: 0.7949\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8357\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8374\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8369\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8389\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8345\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8383\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8414\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8430\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8462\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8428\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8396\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1062 - accuracy: 0.8466\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8420\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8447\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8392\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1447 - accuracy: 0.7994\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.8297\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8306\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8354\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8336\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8375\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8390\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8387\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8378\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8407\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8393\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 2ms/step - loss: 0.1090 - accuracy: 0.8407\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8413\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8415\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8414\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8420\n",
            "Epoch 1/16\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1437 - accuracy: 0.7985\n",
            "Epoch 2/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8350\n",
            "Epoch 3/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8398\n",
            "Epoch 4/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8351\n",
            "Epoch 5/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8407\n",
            "Epoch 6/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8412\n",
            "Epoch 7/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8406\n",
            "Epoch 8/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8422\n",
            "Epoch 9/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8461\n",
            "Epoch 10/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8408\n",
            "Epoch 11/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8398\n",
            "Epoch 12/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8453\n",
            "Epoch 13/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8443\n",
            "Epoch 14/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8423\n",
            "Epoch 15/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8397\n",
            "Epoch 16/16\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8413\n",
            "Epoch 1/16\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1433 - accuracy: 0.7957\n",
            "Epoch 2/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1176 - accuracy: 0.8301\n",
            "Epoch 3/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8326\n",
            "Epoch 4/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8419\n",
            "Epoch 5/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8404\n",
            "Epoch 6/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8409\n",
            "Epoch 7/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8422\n",
            "Epoch 8/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8438\n",
            "Epoch 9/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8415\n",
            "Epoch 10/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8397\n",
            "Epoch 11/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8392\n",
            "Epoch 12/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8424\n",
            "Epoch 13/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1046 - accuracy: 0.8488\n",
            "Epoch 14/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8430\n",
            "Epoch 15/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8444\n",
            "Epoch 16/16\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8432\n",
            "16: 0.840694 (0.006836)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_IYVlhVpO1yS",
        "outputId": "7db2915d-b216-4c99-e62c-0ede1e98cbc1"
      },
      "source": [
        "#4 Graph for learning rates for our  model to see Accuracy and MSE of the Model\n",
        "# optimizers = ['WAME']\n",
        "# inits = ['uniform', 'glorot_uniform'] \n",
        "#optimizers = ['WAME', 'WAME(learning_rate=0.01)', 'WAME(learning_rate=0.0001)', 'WAME(learning_rate=0.00001)']\n",
        "\n",
        "  #6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model6(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "# prepare models, as this model6 is chosen now we will analyse the effect of epochs increase to this model with same batch size\n",
        "models = []\n",
        "models.append(('0.00001', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME(learning_rate=0.00001)')))\n",
        "models.append(('0.0001', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "models.append(('0.001', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME(learning_rate=0.001)')))\n",
        "models.append(('0.01', KerasClassifier(build_fn=creating_model6, batch_size= 40, epochs= 10, init = 'glorot_uniform', optimizer='WAME(learning_rate=0.01)')))\n",
        "\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Accuracy Rate')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Generalization Accuracy')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "# again evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  mse = mean_squared_error(Y_test,  y_pred)\n",
        "  results.append(mse)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Squarred Error')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Generalization Error Rate')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1549 - accuracy: 0.7825\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.8311\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8323\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8399\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8470\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8447\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8389\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1066 - accuracy: 0.8447\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 992us/step - loss: 0.1094 - accuracy: 0.8403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 989us/step - loss: 0.1532 - accuracy: 0.7837\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 970us/step - loss: 0.1156 - accuracy: 0.8349\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8362\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8372\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8439\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8421\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8409\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8405\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8383\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8467\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1415 - accuracy: 0.8029\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8285\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8315\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8335\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8365\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8365\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8388\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8389\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.7870\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8316\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8343\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8298\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8333\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8358\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 998us/step - loss: 0.1430 - accuracy: 0.7978\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8378\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8391\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8397\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8374\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8443\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1541 - accuracy: 0.7834\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.8311\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1154 - accuracy: 0.8323\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8341\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8331\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 999us/step - loss: 0.1119 - accuracy: 0.8345\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8330\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8347\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8389\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8344\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1466 - accuracy: 0.7952\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8299\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8380\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8386\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8438\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8442\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8418\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 954us/step - loss: 0.1087 - accuracy: 0.8437\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 998us/step - loss: 0.1060 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1495 - accuracy: 0.7811\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 990us/step - loss: 0.1151 - accuracy: 0.8308\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8339\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 955us/step - loss: 0.1111 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8360\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8390\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8412\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 965us/step - loss: 0.1484 - accuracy: 0.7876\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1154 - accuracy: 0.8321\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8335\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8348\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8357\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8446\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1521 - accuracy: 0.7777\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8393\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8401\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8412\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8446\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8443\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8428\n",
            "0.00001: 0.840495 (0.005646)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1519 - accuracy: 0.7921\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8331\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8315\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8334\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8350\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8340\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8349\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8386\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8401\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1498 - accuracy: 0.7970\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8334\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8387\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8424\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8412\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8430\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8418\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 964us/step - loss: 0.1541 - accuracy: 0.7868\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.8304\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8312\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8384\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8364\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8366\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8452\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8384\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1532 - accuracy: 0.7847\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8364\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8388\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8428\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8405\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8377\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8431\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1065 - accuracy: 0.8452\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8419\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8400\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1562 - accuracy: 0.7695\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1186 - accuracy: 0.8266\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8370\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8405\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8383\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8421\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1454 - accuracy: 0.7901\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8374\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8401\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8395\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8423\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8395\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8422\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8418\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1508 - accuracy: 0.7902\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8326\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8385\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8422\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8400\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8378\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8393\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8446\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1471 - accuracy: 0.7882\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.8331\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8369\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8362\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8409\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8383\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8429\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8434\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1467 - accuracy: 0.7943\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8368\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8376\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8374\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8372\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8395\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8398\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8378\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1477 - accuracy: 0.7943\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8364\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8329\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8375\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8426\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8402\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8427\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8450\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8364\n",
            "0.0001: 0.838473 (0.006844)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1450 - accuracy: 0.7934\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8367\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8381\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8419\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8385\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8395\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1523 - accuracy: 0.7923\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1155 - accuracy: 0.8301\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8365\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8410\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1556 - accuracy: 0.7849\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1176 - accuracy: 0.8281\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8372\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8385\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8436\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8366\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8395\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8442\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1557 - accuracy: 0.7891\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.8342\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8405\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8390\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8406\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1479 - accuracy: 0.7895\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8398\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8410\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8376\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8418\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8422\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8427\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1065 - accuracy: 0.8454\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1497 - accuracy: 0.7864\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8326\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8404\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8387\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8447\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8444\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8472\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1561 - accuracy: 0.7744\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.8298\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8342\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8378\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8350\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8385\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8442\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8393\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8416\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1457 - accuracy: 0.7942\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8344\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8370\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8386\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8409\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8408\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8444\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1504 - accuracy: 0.7962\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1154 - accuracy: 0.8343\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8313\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8333\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8344\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8358\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8371\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8367\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8376\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1517 - accuracy: 0.7921\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8294\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8362\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8364\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8393\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8371\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8372\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8429\n",
            "0.001: 0.840760 (0.003824)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1445 - accuracy: 0.7996\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8355\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8359\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8413\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8411\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8478\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8439\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1053 - accuracy: 0.8475\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.7944\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8342\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8336\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8348\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8395\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8371\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8376\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1547 - accuracy: 0.7833\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8312\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8400\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8439\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8380\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8399\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8375\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1507 - accuracy: 0.7915\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8328\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8296\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8390\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8371\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8418\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8370\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8405\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1519 - accuracy: 0.7843\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8334\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8361\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8392\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8393\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8373\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8405\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8393\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1553 - accuracy: 0.7827\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.8291\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8360\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8331\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8382\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8380\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8417\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1552 - accuracy: 0.7875\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8340\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8401\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8409\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8409\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8424\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8397\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8444\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8421\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1432 - accuracy: 0.7939\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8335\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8367\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8340\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8309\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8314\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8435\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1553 - accuracy: 0.7847\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.8291\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8379\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8410\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8415\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8407\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8402\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1512 - accuracy: 0.7884\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.8277\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8335\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8418\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8405\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8423\n",
            "0.01: 0.839367 (0.004729)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1470 - accuracy: 0.7929\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8401\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8348\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8415\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8401\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8434\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1513 - accuracy: 0.7905\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8386\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8367\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8371\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8415\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8417\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8361\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8390\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1558 - accuracy: 0.7858\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8345\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8374\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8349\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8357\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8396\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8427\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8442\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1455 - accuracy: 0.7969\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8398\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8360\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8425\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8384\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8380\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.8432\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8349\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1585 - accuracy: 0.7796\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.8278\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8381\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8383\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8407\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8454\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8424\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8420\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8415\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1426 - accuracy: 0.7963\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8341\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8330\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8417\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8372\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8405\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8435\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8355\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8437\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1529 - accuracy: 0.7881\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8319\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.8304\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8365\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8355\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8414\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8386\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8416\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8439\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1448 - accuracy: 0.7944\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8331\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8378\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8419\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8408\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8412\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8411\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1078 - accuracy: 0.8425\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1535 - accuracy: 0.7816\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8328\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8353\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8418\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8391\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8397\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8380\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8413\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1489 - accuracy: 0.7876\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8343\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8302\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8364\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8358\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8368\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8432\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8411\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1495 - accuracy: 0.7880\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1155 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8431\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8415\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8422\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8399\n",
            "0.00001: 0.840694 (0.005482)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1511 - accuracy: 0.7885\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8371\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8351\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8303\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8329\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8361\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8378\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8373\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1572 - accuracy: 0.7834\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8299\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8365\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8408\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8406\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8382\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8366\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8443\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.8436\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1537 - accuracy: 0.7780\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8332\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8292\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8359\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8353\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8338\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8363\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8385\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1461 - accuracy: 0.7959\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.8320\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8363\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8382\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8422\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8407\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8385\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1518 - accuracy: 0.7865\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8358\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8406\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8363\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8375\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8436\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1422 - accuracy: 0.7962\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8307\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8363\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8391\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8447\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8424\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1492 - accuracy: 0.7871\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8362\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8445\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8381\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8359\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.7836\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8399\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8357\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8367\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8438\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8378\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8410\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1513 - accuracy: 0.7847\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.8283\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8386\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8346\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8360\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8376\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8446\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8403\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1501 - accuracy: 0.7908\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8352\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.8378\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8339\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8386\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8397\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8420\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8386\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8424\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8428\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1503 - accuracy: 0.7889\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.8304\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8322\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8365\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8363\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8413\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8385\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8413\n",
            "0.0001: 0.838970 (0.005152)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1425 - accuracy: 0.7982\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1155 - accuracy: 0.8328\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8341\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8404\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8435\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8417\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8446\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8359\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1067 - accuracy: 0.8446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1510 - accuracy: 0.7856\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8362\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8362\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8360\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8375\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8362\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8421\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8404\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1514 - accuracy: 0.7907\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8342\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1144 - accuracy: 0.8349\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8377\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8438\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8437\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1582 - accuracy: 0.7753\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.8339\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8370\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8389\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8434\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8432\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8403\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8409\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1490 - accuracy: 0.7880\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8364\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8407\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8360\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8427\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8384\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8370\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8440\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1412 - accuracy: 0.8046\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8375\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8399\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8405\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8459\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8397\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1056 - accuracy: 0.8466\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1485 - accuracy: 0.7933\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8392\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8407\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8389\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8423\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8421\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1542 - accuracy: 0.7846\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.8304\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8345\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8386\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8407\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8397\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1582 - accuracy: 0.7808\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.8312\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8342\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8353\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8340\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8409\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8426\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1506 - accuracy: 0.7895\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8356\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8350\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8370\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8361\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8364\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8381\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8424\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8428\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1496 - accuracy: 0.7904\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8323\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8350\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8372\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8380\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8348\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8408\n",
            "0.001: 0.839798 (0.003891)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1421 - accuracy: 0.7973\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8387\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8352\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8434\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8357\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8431\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8398\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8430\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1593 - accuracy: 0.7828\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8356\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8335\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8394\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8415\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8357\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8439\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8413\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1452 - accuracy: 0.7910\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8351\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8379\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8392\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8389\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8424\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8403\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8388\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1432 - accuracy: 0.7965\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8318\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8390\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8386\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8399\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8424\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8369\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8430\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8388\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1493 - accuracy: 0.7870\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8376\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8352\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8411\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8408\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8427\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8477\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1498 - accuracy: 0.7873\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1147 - accuracy: 0.8347\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8380\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8392\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1062 - accuracy: 0.8463\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8392\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1452 - accuracy: 0.8021\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8321\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8334\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8324\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8330\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8348\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8366\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1465 - accuracy: 0.7883\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8338\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8340\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8386\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8346\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8377\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8365\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8384\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8404\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1468 - accuracy: 0.7969\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8357\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8356\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8349\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8377\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8376\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8381\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1501 - accuracy: 0.7861\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8343\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8386\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8340\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8385\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8349\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8396\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8351\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8396\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1481 - accuracy: 0.7951\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8335\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8384\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8420\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.8380\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8439\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8425\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8432\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8407\n",
            "0.01: 0.838837 (0.006738)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QRL-gdSrTBLs",
        "outputId": "ac3290be-cab2-45d7-eb40-39fd761da6f3"
      },
      "source": [
        "#5 Graph for Activation for our  model to see Accuracy and MSE of the Model\n",
        "\n",
        "#6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model1(optimizer=optimizers, init=inits):# input and hidden uses relu and output layer uses sigmoid\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  def creating_model2(optimizer=optimizers, init=inits):\n",
        "    classifier = Sequential() #Sequential module Initialises the ANN\n",
        "    classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "    classifier.add(Dense(10, activation='relu')) \n",
        "    classifier.add(Dense(20, activation='relu'))\n",
        "    classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "    classifier.add(Dense(40, activation='relu'))\n",
        "    classifier.add(Dense(45, activation='relu'))\n",
        "    classifier.add(Dense(50, activation='relu'))\n",
        "    classifier.add(Dense(1, activation='relu'))             #one output layer\n",
        "  # Compile model\n",
        "    classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "    return classifier\n",
        "\n",
        "def creating_model3(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='sigmoid')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='sigmoid')) \n",
        "  classifier.add(Dense(20, activation='sigmoid'))\n",
        "  classifier.add(Dense(30, activation='sigmoid'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='sigmoid'))\n",
        "  classifier.add(Dense(45, activation='sigmoid'))\n",
        "  classifier.add(Dense(50, activation='sigmoid'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('relu/sigmoid', KerasClassifier(build_fn=creating_model1, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "models.append(('relu', KerasClassifier(build_fn=creating_model2, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "models.append(('sigmoid', KerasClassifier(build_fn=creating_model3, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Accuracy Rate')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('Activation Function')\n",
        "plt.ylabel('Generalization Accuracy')\n",
        "pyplot.show()\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "# again evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  mse = mean_squared_error(Y_test,  y_pred)\n",
        "  results.append(mse)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Mean Squarred Error')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.xlabel('Activation Function')\n",
        "plt.ylabel('Generalization Error Rate')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 988us/step - loss: 0.1459 - accuracy: 0.7929\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8297\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8317\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8382\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8405\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1094 - accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8367\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1500 - accuracy: 0.7853\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8367\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8397\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8350\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8375\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8367\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8407\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1069 - accuracy: 0.8431\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1490 - accuracy: 0.7918\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8358\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8367\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8352\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8428\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1059 - accuracy: 0.8461\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8408\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1519 - accuracy: 0.7844\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8342\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8348\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8350\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8392\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8363\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1064 - accuracy: 0.8463\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1480 - accuracy: 0.7923\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1154 - accuracy: 0.8338\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8385\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8360\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8425\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8439\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8411\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8445\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1057 - accuracy: 0.8473\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1460 - accuracy: 0.7951\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8379\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8402\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8398\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8427\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8419\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8415\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8428\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8441\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1532 - accuracy: 0.7835\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8360\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8351\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8379\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8381\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8378\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8401\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8422\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1441 - accuracy: 0.7963\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8342\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8389\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8400\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8404\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1051 - accuracy: 0.8474\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8373\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8398\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.8446\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1416 - accuracy: 0.8025\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8376\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8323\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8381\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8385\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8338\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8404\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1613 - accuracy: 0.7756\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8372\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8421\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8399\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8431\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8421\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8390\n",
            "relu/sigmoid: 0.840064 (0.005299)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 850us/step - loss: 0.1831 - accuracy: 0.7328\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 878us/step - loss: 0.1174 - accuracy: 0.8328\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 845us/step - loss: 0.1153 - accuracy: 0.8321\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 876us/step - loss: 0.1125 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 861us/step - loss: 0.1120 - accuracy: 0.8367\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1101 - accuracy: 0.8406\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 811us/step - loss: 0.1102 - accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 835us/step - loss: 0.1115 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 903us/step - loss: 0.1107 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1112 - accuracy: 0.8403\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 844us/step - loss: 0.1766 - accuracy: 0.7478\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 792us/step - loss: 0.1225 - accuracy: 0.8247\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 853us/step - loss: 0.1143 - accuracy: 0.8324\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 883us/step - loss: 0.1127 - accuracy: 0.8346\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 878us/step - loss: 0.1123 - accuracy: 0.8353\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 880us/step - loss: 0.1121 - accuracy: 0.8355\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 823us/step - loss: 0.1120 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 871us/step - loss: 0.1089 - accuracy: 0.8406\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 889us/step - loss: 0.1083 - accuracy: 0.8431\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 864us/step - loss: 0.1093 - accuracy: 0.8413\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 885us/step - loss: 0.1671 - accuracy: 0.7697\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 909us/step - loss: 0.1199 - accuracy: 0.8264\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 888us/step - loss: 0.1140 - accuracy: 0.8349\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 916us/step - loss: 0.1131 - accuracy: 0.8344\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 894us/step - loss: 0.1122 - accuracy: 0.8375\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 860us/step - loss: 0.1094 - accuracy: 0.8427\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 887us/step - loss: 0.1077 - accuracy: 0.8453\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 870us/step - loss: 0.1079 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 831us/step - loss: 0.1069 - accuracy: 0.8438\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 876us/step - loss: 0.1086 - accuracy: 0.8427\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 851us/step - loss: 0.1700 - accuracy: 0.7734\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 850us/step - loss: 0.1247 - accuracy: 0.8221\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 840us/step - loss: 0.1142 - accuracy: 0.8312\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 872us/step - loss: 0.1083 - accuracy: 0.8411\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 845us/step - loss: 0.1104 - accuracy: 0.8392\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 883us/step - loss: 0.1079 - accuracy: 0.8425\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 916us/step - loss: 0.1080 - accuracy: 0.8419\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 906us/step - loss: 0.1086 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 881us/step - loss: 0.1104 - accuracy: 0.8367\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 922us/step - loss: 0.1114 - accuracy: 0.8381\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 846us/step - loss: 0.1615 - accuracy: 0.7810\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 866us/step - loss: 0.1153 - accuracy: 0.8318\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 889us/step - loss: 0.1135 - accuracy: 0.8326\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 887us/step - loss: 0.1132 - accuracy: 0.8361\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 853us/step - loss: 0.1120 - accuracy: 0.8347\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 922us/step - loss: 0.1108 - accuracy: 0.8383\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 862us/step - loss: 0.1117 - accuracy: 0.8367\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 889us/step - loss: 0.1097 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 932us/step - loss: 0.1095 - accuracy: 0.8385\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 895us/step - loss: 0.1095 - accuracy: 0.8369\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 883us/step - loss: 0.1626 - accuracy: 0.7799\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 858us/step - loss: 0.1168 - accuracy: 0.8325\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 903us/step - loss: 0.1121 - accuracy: 0.8373\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 839us/step - loss: 0.1116 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 920us/step - loss: 0.1109 - accuracy: 0.8397\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 853us/step - loss: 0.1095 - accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 883us/step - loss: 0.1120 - accuracy: 0.8369\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 851us/step - loss: 0.1093 - accuracy: 0.8430\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 868us/step - loss: 0.1100 - accuracy: 0.8399\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 928us/step - loss: 0.1104 - accuracy: 0.8374\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 843us/step - loss: 0.1772 - accuracy: 0.7623\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 859us/step - loss: 0.1145 - accuracy: 0.8316\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 876us/step - loss: 0.1108 - accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 926us/step - loss: 0.1117 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 843us/step - loss: 0.1104 - accuracy: 0.8397\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 950us/step - loss: 0.1100 - accuracy: 0.8377\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 923us/step - loss: 0.1106 - accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 901us/step - loss: 0.1079 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 926us/step - loss: 0.1081 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 928us/step - loss: 0.1076 - accuracy: 0.8421\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 856us/step - loss: 0.1643 - accuracy: 0.7783\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1171 - accuracy: 0.8285\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 861us/step - loss: 0.1139 - accuracy: 0.8327\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 908us/step - loss: 0.1132 - accuracy: 0.8325\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 915us/step - loss: 0.1106 - accuracy: 0.8383\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 911us/step - loss: 0.1121 - accuracy: 0.8343\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 877us/step - loss: 0.1111 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 871us/step - loss: 0.1128 - accuracy: 0.8362\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 862us/step - loss: 0.1131 - accuracy: 0.8355\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 872us/step - loss: 0.1098 - accuracy: 0.8392\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 901us/step - loss: 0.1726 - accuracy: 0.7647\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 915us/step - loss: 0.1158 - accuracy: 0.8350\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 838us/step - loss: 0.1143 - accuracy: 0.8309\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 833us/step - loss: 0.1137 - accuracy: 0.8330\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 929us/step - loss: 0.1121 - accuracy: 0.8372\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 950us/step - loss: 0.1120 - accuracy: 0.8386\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 988us/step - loss: 0.1118 - accuracy: 0.8353\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 915us/step - loss: 0.1132 - accuracy: 0.8337\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 913us/step - loss: 0.1102 - accuracy: 0.8408\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 918us/step - loss: 0.1121 - accuracy: 0.8362\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 913us/step - loss: 0.1750 - accuracy: 0.7631\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 935us/step - loss: 0.1169 - accuracy: 0.8312\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 892us/step - loss: 0.1162 - accuracy: 0.8296\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 890us/step - loss: 0.1149 - accuracy: 0.8310\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 926us/step - loss: 0.1112 - accuracy: 0.8370\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 893us/step - loss: 0.1134 - accuracy: 0.8336\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 887us/step - loss: 0.1134 - accuracy: 0.8297\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 940us/step - loss: 0.1110 - accuracy: 0.8369\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 890us/step - loss: 0.1110 - accuracy: 0.8391\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8402\n",
            "relu: 0.837146 (0.002918)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.2022 - accuracy: 0.7131\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1877 - accuracy: 0.7503\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1689 - accuracy: 0.7626\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1411 - accuracy: 0.7951\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1390 - accuracy: 0.8011\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1382 - accuracy: 0.8037\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1401 - accuracy: 0.8013\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1392 - accuracy: 0.8018\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1370 - accuracy: 0.8054\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1396 - accuracy: 0.8023\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1884 - accuracy: 0.7496\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1843 - accuracy: 0.7570\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1719 - accuracy: 0.7623\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1374 - accuracy: 0.8050\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1389 - accuracy: 0.8009\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1398 - accuracy: 0.8017\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1410 - accuracy: 0.8003\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1387 - accuracy: 0.8047\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1401 - accuracy: 0.8018\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1376 - accuracy: 0.8049\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1897 - accuracy: 0.7458\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1864 - accuracy: 0.7527\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1521 - accuracy: 0.7798\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8031\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1395 - accuracy: 0.8021\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1396 - accuracy: 0.8041\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1373 - accuracy: 0.8069\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1374 - accuracy: 0.8079\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1380 - accuracy: 0.8054\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1369 - accuracy: 0.8070\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1891 - accuracy: 0.7466\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1885 - accuracy: 0.7492\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.7552\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1412 - accuracy: 0.7972\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8039\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1379 - accuracy: 0.8079\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1392 - accuracy: 0.8037\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1387 - accuracy: 0.8043\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1391 - accuracy: 0.8014\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1374 - accuracy: 0.8073\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1891 - accuracy: 0.7489\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1878 - accuracy: 0.7504\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1591 - accuracy: 0.7733\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1413 - accuracy: 0.7986\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1402 - accuracy: 0.8013\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8090\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1381 - accuracy: 0.8073\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8052\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1401 - accuracy: 0.8027\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1381 - accuracy: 0.8058\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1989 - accuracy: 0.7221\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1883 - accuracy: 0.7494\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1583 - accuracy: 0.7744\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1429 - accuracy: 0.7931\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1371 - accuracy: 0.8071\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1381 - accuracy: 0.8026\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1389 - accuracy: 0.8028\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1400 - accuracy: 0.8045\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1400 - accuracy: 0.8000\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1385 - accuracy: 0.8042\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1873 - accuracy: 0.7486\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1875 - accuracy: 0.7509\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1652 - accuracy: 0.7680\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1400 - accuracy: 0.7962\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1381 - accuracy: 0.8028\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8029\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1366 - accuracy: 0.8062\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1387 - accuracy: 0.8026\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8076\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1372 - accuracy: 0.8069\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1876 - accuracy: 0.7514\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1877 - accuracy: 0.7508\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1675 - accuracy: 0.7645\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1401 - accuracy: 0.7983\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1413 - accuracy: 0.7960\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8038\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1394 - accuracy: 0.8018\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1399 - accuracy: 0.8007\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1352 - accuracy: 0.8099\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1374 - accuracy: 0.8064\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1904 - accuracy: 0.7476\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1868 - accuracy: 0.7522\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1628 - accuracy: 0.7691\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1404 - accuracy: 0.7986\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1399 - accuracy: 0.8019\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1391 - accuracy: 0.8048\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1368 - accuracy: 0.8072\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1367 - accuracy: 0.8088\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8032\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1374 - accuracy: 0.8067\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1853 - accuracy: 0.7574\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1885 - accuracy: 0.7491\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1678 - accuracy: 0.7634\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8007\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1389 - accuracy: 0.8006\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8064\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1418 - accuracy: 0.8012\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1406 - accuracy: 0.8027\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1374 - accuracy: 0.8069\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1395 - accuracy: 0.8034\n",
            "sigmoid: 0.805385 (0.007324)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEjCAYAAAAomJYLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgdVbnv8e+PMIQpEwkqCRmMDAmgiA3IEdQIaIyPgMoRglMwV1SGc0Q4ggYPgStOqHiVSSCYQ8Rg9Ao3CsggAQ2CpJMwJRgNIZAASjQRZA7kvX/UalJsqrurh+q9u/P7PE89vatqVdW7ew/vrrWq1lJEYGZmVmuzegdgZmaNyQnCzMwKOUGYmVkhJwgzMyvkBGFmZoWcIMzMrJAThJmZFXKCsEpJWinpRUlDa5YvlhSSRtcprjGSNki6qB7H7wmSZqb//dOS1kq6SdLuJbcdnV6fzauO0xqXE4T1hIeAyS0zkvYCtqlfOAB8ElgHHCVpq548sKR+PXi4b0fEdsBw4FFgRg8e23o5JwjrCbPIvpBbfAq4Il9A0laSviPpEUl/k3SxpK3TusGSfi1pjaR16fGI3La3Svrfkm6X9C9JN9aesdQcSymeM4D1wAdr1h8u6W5JT0l6UNLEtHyIpB9LeizFcU1aPkXS/Jp9hKQ3pcczJV0k6TpJzwATJH0gnUU9JWmVpOk12x8o6Q+S/pnWT5G0b/rf9MuV+7Cke9r5/xMRzwFzgL1z27YVw+/S33+mM5AD0jaflvRAev43SBrV3rGt93KCsJ5wJzBA0rj05XY08JOaMt8EdiX7AnsT2S/e/07rNgN+DIwCRgLPAefXbH8McCywI7AlcGob8RwIjACuIvvS/FTLCkn7kSWv/wIGAe8EVqbVs8jOfPZIxzmvvSdeE985wPbAfOAZsiQ1CPgA8HlJR6QYRgHXAz8EhpH9T+6OiAXAP4D35vb7CWqSbRFJ25KdxS3PLW41BrLnDTAoIraLiDskHQ58Bfhwiuv3wOwO/A+st4kIT54qm8i+XA8h+7X+DWAicBOwORDAaEBkX1Zjc9sdADzUyj73Btbl5m8FzsjNHw/8po2YLgOuyR1nPbBjmv8RcF7BNm8ANgCDC9ZNAebXLAvgTenxTOCKdv5P3285LvBl4OpWyp0GXJkeDwGeBd7QStmZwPPAP1PsDwFvLhnD6PQcNs+tvx6YmpvfLB1/VL3fZ56qmXwGYT1lFtmv6Cm89hfvMLJf5gtTlco/gd+k5UjaRtKPJD0s6Smy6o9BNXX5f809fhbYriiIVG3178CVABFxB/BIig1gZ+DBgk13BtZGxLpyT/c1VtXEsb+keana7Engc0BLtVhrMUB25vXBdEbwUeD3EfF4G8f9TkQMIvvCfw7YrWQMRUYB/yf3Gq0lS+7D29jGejEnCOsREfEw2S/YScAva1b/nezLa4+IGJSmgZE1rgKcQvbFtn9EDGBj9Yc6EcqHgAHAhZL+KumvZF9wLdVMq4CxBdutAoZIGlSw7hlyje6SXl9Qprbb5J8Cc4GdI2IgcDEbn09rMRARjwJ3kFXzfIIs8bYrIh4B/pPsC37rEjEUdfO8Cvhs7jUaFBFbR8QfysRgvY8ThPWkqcB7IuKZ/MKI2ABcCpwnaUcAScMlvS8V2Z4sgfxT0hDgzC7E8CngcmAvsqqqvYF3AG9JV1fNAI6VdLCkzVIcu6df6deTJZbBkraQ1JKo7gH2kLS3pP7A9BJxbE92RvJ8avc4JrfuSuAQSR+VtLmkHSTtnVt/BfCl9Bxqk22rIuIm4DHguBIxrCGrlnpjbtnFwJcl7QEgaaCkfy97fOt9nCCsx0TEgxHR3Mrq08gaUO9M1Ug3s7E65PvA1mRnGneSVT91mKThwMHA9yPir7lpYdrnpyLiLrLG7vOAJ4HbyKpWIPvFvh74E/AE8IX0vP4MnJ1i/gtZI3R7jgfOlvQvssb4OS0r0q/9SWRnTmuBu4G35La9OsV0dUQ828F/w7nAl9KlvW3F8CxZo/rtqUrp7RFxNfAt4Kr0Gt0PvL+Dx7deRBEeMMist5H0IFl1z831jsX6Lp9BmPUykj5C1kZwS71jsb7Nt9Gb9SKSbgXGA59IbTdmlXEVk5mZFXIVk5mZFXKCMDOzQk4QZmZWyAnCzMwKOUGYmVkhJwgzMyvkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QZmZWyAnCzMwKOUGYmVkhJwgzMyvUZ8aDGDp0aIwePbreYZiZ9SoLFy78e0QMK1rXZxLE6NGjaW5ubbhjMzMrIunh1ta5isnMzAo5QZiZWaFKE4SkiZKWSVou6fSC9SMlzZO0WNK9kial5aMlPSfp7jRdXGWcZmb2WpW1QUjqB1wAHAqsBhZImhsRS3PFzgDmRMRFksYD1wGj07oHI2LvquIzM7O2VXkGsR+wPCJWRMSLwFXA4TVlAhiQHg8EHqswHjMz64AqE8RwYFVufnValjcd+Lik1WRnDyfl1o1JVU+3STqo6ACSjpPULKl5zZo13Ri6mZnVu5F6MjAzIkYAk4BZkjYDHgdGRsRbgS8CP5U0oHbjiLgkIpoiomnYsMLLeM3MrJOqTBCPAjvn5kekZXlTgTkAEXEH0B8YGhEvRMQ/0vKFwIPArhXGamZmNapMEAuAXSSNkbQlcDQwt6bMI8DBAJLGkSWINZKGpUZuJL0R2AVYUWGsdSepy5OZWXeq7CqmiHhJ0onADUA/4PKIWCLpbKA5IuYCpwCXSjqZrMF6SkSEpHcCZ0taD2wAPhcRa6uKtRFERJvrJbVbxsysO6mvfOk0NTVFX+5qwwnCzKogaWFENBWtq3cjtZmZNSgnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICaIHDBkypFtGi+vqPoYMGVLn/4SZ9SaVjShnG61bt64hBvvxsKRm1hE+gzAzs0JOEGZmVsgJwszMCrkNwqyLuqttpxHaqczynCDMuqjMF7skJwDrdVzFZGZmhZwgzMyskBOEmZkVcoIwM7NClSYISRMlLZO0XNLpBetHSponabGkeyVNKlj/tKRTq4zTzMxeq7IEIakfcAHwfmA8MFnS+JpiZwBzIuKtwNHAhTXrvwdcX1WMZmbWuirPIPYDlkfEioh4EbgKOLymTAAD0uOBwGMtKyQdATwELKkwRjMza0WV90EMB1bl5lcD+9eUmQ7cKOkkYFvgEABJ2wGnAYcCvb56Kc4cANMH1juMLA4zs5LqfaPcZGBmRHxX0gHALEl7kiWO8yLi6bbuUpV0HHAcwMiRI3sg3M7RWU81xE1Skojp9Y7CzHqLKhPEo8DOufkRaVneVGAiQETcIak/MJTsTONISd8GBgEbJD0fEefnN46IS4BLAJqamur/DWxm1odUmSAWALtIGkOWGI4Gjqkp8whwMDBT0jigP7AmIg5qKSBpOvB0bXIwM7NqVdZIHREvAScCNwAPkF2ttETS2ZIOS8VOAT4j6R5gNjAlGqEuxszMUF/5Pm5qaorm5uZ6h1GoUTpqa5Q4epshQ4awbt26usYwePBg1q5dW9cYrG+StDAimorW1buR2qzhNcKQsR4u1urBXW2YmVmhdhOEpL16IhAzM2ssZc4gLpR0l6TjJdX/bi8zM+sR7SaIdMnpx8juaVgo6aeSDq08MjMzq6tSjdQR8RdJZwDNwA+AtyprNftKRPyyygD7ikZoZBw8eHC9QzCzXqTdBCHpzcCxwAeAm4APRsQiSTsBdwBOEO3ojitgfImqmfW0MmcQPwQuIztbeK5lYUQ8ls4qzPq0Ruhs0R0tWj2USRAfAJ6LiJcBJG0G9I+IZyNiVqXRmTWARuhs0R0tWj2UuYrpZmDr3Pw2aZmZmfVhZRJE/4h4umUmPd6mupDMzKwRlEkQz0jap2VG0tuA59oob2ZmfUCZNogvAD+X9Bgg4PXAUZVGZdZg6n2Zsi9RtnpoN0FExAJJuwO7pUXLImJ9tWGZNQ5fpmybqrK9ue4GjCcb0Gef9Ga/orqwzMys3srcKHcm8G6yBHEd8H5gPuAEYWbWh5VppD6SbFjQv0bEscBbAHfaZ2bWx5VJEM9FxAbgJUkDgCfIOu4zM7M+rEwbRLOkQcClwELgabI+mMzMrA9rM0GkHlu/ERH/BC6W9BtgQETc2yPRmfUCZS+Bba+cr3KyRtNmgoiIkHQdsFeaX9kTQZn1Jv5it76qTBvEIkn7Vh6JmZk1lDJtEPsDH5P0MPAM2d3UERFvrjQyMzOrqzIJ4n2VR2FmZg2nTIJwBauZ2SaoTIK4lixJiKyrjTHAMmCPCuMyM7M6K9NZ3175+dT19/GVRbSJKnOppC+TNLOeVOYqpleJiEVkDdftkjRR0jJJyyWdXrB+pKR5khZLulfSpLR8P0l3p+keSR/qaJy9TUR0eTIz605lOuv7Ym52M2Af4LES2/UDLgAOBVYDCyTNjYiluWJnAHMi4iJJLZ0BjgbuB5oi4iVJbwDukfSriHip5PMyM7MuKnMGsX1u2oqsTeLwEtvtByyPiBUR8SJwVcF2AQxIjweSEk9EPJtLBv1xQ7mZWY8r0wZxVif3PRxYlZtfzWurpqYDN0o6CdgWOKRlhaT9gcuBUcAnis4eJB0HHAcwcuTIToZpZmZF2j2DkHRT6qyvZX6wpBu66fiTgZkRMQKYBMyStBlARPwxIvYA9gW+LKl/7cYRcUlENEVE07Bhw7opJDMzg3JVTMNSZ30ARMQ6YMcS2z3Kq7sFH5GW5U0F5qT93kFWnTQ0XyAiHiDrQXbPEsc0M7NuUiZBvCzplfobSaMo1yawANhF0hhJWwJHA3NryjxCNhgRksaRJYg1aZvNc8fbHVhZ4phmZtZNytwoNw2YL+k2spvlDiLV+7clXYF0InAD0A+4PCKWSDobaI6IucApwKWSTiZLOlNSD7IHAqdLWg9sAI6PiL935gmamVnnqMz185KGAm9Ps3c24pd1U1NTNDc31zsMM7NeRdLCiGgqWlemkfpDwPqI+HVE/Jps6NEjujtIMzNrLGXaIM6MiCdbZlKD9ZnVhWRmZo2gTIIoKlOm7cLMzHqxMgmiWdL3JI1N03nAwqoDMzOz+iqTIE4CXgR+lqbncG+uZmZ9XpmuNp4BXumJNd0TcQJwboVxmZlZnZXq7lvSMEnHS/o9MA94XbVhmZlZvbV6BiFpe+DDwDHArsAvgTGp3yQzM+vj2qpiegK4i2zMhvnpDuc+P3CPmZll2qpi+jLZ+A8XkvWmOrZnQjIzs0bQaoKIiO9HxNvZOMjPNcBOkk6TtGuPRGdmZnXTbiN1GhHu6xGxF9BENgLcdZVHZmZmdVXqKqYWEXF/REyLiDdVFZCZmTWGDiUIMzPbdDhBmJlZIScIMzMr1G5XG5LeAUwHRqXyAiIi3lhtaGZmVk9luu2eAZxM1oPry9WGY2ZmjaJMgngyIq6vPBIzM2soZRLEPEnnkvXF9ELLwohYVFlUZmZWd2USxP7pb35Q6wDe0/3hmJlZoygzHsSEngjEzMwaS7uXuUoamIYcbU7TdyUN7IngzMysfsrcB3E58C/go2l6CvhxlUGZmVn9lUkQYyPizNRp34qIOAvwPRBm1qfNnj2bPffck379+rHnnnsye/bseofU48o0Uj8n6cCImA+v3Dj3XLVhmZnVz+zZs5k2bRozZszgwAMPZP78+UydOhWAyZMn1zm6nlPmDOLzwAWSVkp6GDgf+FyZnUuaKGmZpOWSTi9YP1LSPEmLJd0raVJafqikhZLuS399xZSZ9ZhzzjmHGTNmMGHCBLbYYgsmTJjAjBkzOOecc+odWo9SRJQrKA0AiIinSpbvB/wZOBRYDSwAJkfE0lyZS4DFEXGRpPHAdRExWtJbgb9FxGOS9gRuiIjhbR2vqakpmpubSz0XM7O29OvXj+eff54tttjilWXr16+nf//+vPxy3+pQQtLCiGgqWtdqFZOkj0fETyR9sWY5ABHxvXaOux+wPCJWpO2uIhudbmmuTJANQAQwEHgs7XtxrswSYGtJW0XEC5iZVWzcuHHMnz+fCRM2XuU/f/58xo0bV8eoel5bVUzbpr/bF0zbldj3cGBVbn51WpY3Hfi4pNVko9SdVLCfjwCLnBzMrKdMmzaNqVOnMm/ePNavX8+8efOYOnUq06ZNq3doParVM4iI+FF6eHNE3J5flxqqu8NkYGZEfFfSAcAsSXtGxIZ0nD2AbwHvLdpY0nHAcQAjR47sppDMbFPX0hB90kkn8cADDzBu3DjOOeecTaqBGkq0QUhaFBH7tLesYLsDgOkR8b40/2WAiPhGrswSYGJErErzK4C3R8QTkkYAtwDH1iaoIm6DMDPruM62QRwA/BswrKYdYgDQr8RxFwC7SBoDPAocDRxTU+YR4GBgpqRxQH9gjaRBwLXA6WWSg5mZdb+22iC2JGtr2JxXtz88BRzZ3o4j4iXgROAG4AFgTkQskXS2pMNSsVOAz0i6B5gNTInslOZE4E3Af0u6O007duoZmplZp5SpYhoVEQ/3UDyd5iomM7OO61QVU86zaTyIPciqgACICN+8ZmbWh5W5k/pK4E/AGOAsYCVZ+4KZmfVhZRLEDhExA1gfEbdFxKfxYEFmZn1emSqm9env45I+QHa385DqQjIzs0ZQJkF8LQ0QdArwQ7LLXE+uNCozM6u7Mgninoh4EngSmAAg6fWVRmVmZnVXpg3iIUmzJW2TW3ZdVQGZmVljKJMg7gN+D8yXNDYtU3UhmZlZIyhTxRQRcWG62/lXkk4j66bbzMz6sDIJQgARcbukg4E5wO6VRmVmZnVXJkFMankQEY9LmkDWiZ+ZmfVh7Y4oB0xuGUWuxu8qi8rMzOqurTOI/IhyZma2iWl3RLmIOKvnwjEzs0bRVhXTD9raMCL+o/vDMTOzRtFWFdPCHovCzMwaTltVTP/Tk4GYmVljafcyV0nDgNOA8XjAIDOzTUbZAYMewAMGmZltUjxgkJmZFfKAQWZmVsgDBpnZJq2VniI6LKLv9WHaZoKQ1A/YJSJ+TW7AIDOzvqK9L3ZJffLLv4w22yAi4mVgcg/FYmZmDaRMFdPtks4HfgY807IwIhZVFpWZmdVdmQSxd/p7dm5Z4CuZzMz6tHYTRES43cHMbBPU7n0Qkl4naYak69P8eElTy+xc0kRJyyQtl3R6wfqRkuZJWizpXkmT0vId0vKnU/WWmZn1sDI3ys0EbgB2SvN/Br7Q3kbpCqgLgPeTddMxWdL4mmJnAHMi4q3A0cCFafnzwFeBU0vEZ2ZmFSiTIIZGxBxgA0BEvAS8XGK7/YDlEbEiIl4ErgIOrykTZPdVAAwkuwmPiHgmIuaTJQozM6uDMgniGUk7kH2ZI+ntZPdEtGc4sCo3vzoty5sOfFzSauA64KQS+32FpOMkNUtqXrNmTUc2NTOzdpRJEF8E5gJjJd0OXEEHv8jbMBmYGREjgEnALEllYgIgIi6JiKaIaBo2bFg3hWRmZlDuKqZFkt4F7AYIWBYR69vZDOBRYOfc/Ii0LG8qMDEd5w5J/YGhwBMl9m9mZhUq+2t9P+AtwD5kjc2fLLHNAmAXSWMkbUnWCD23pswjwMEAksaRjTfhuiIzswZQZsCgWcBY4G42Nk4HWVVTqyLiJUknkl0B1Q+4PCKWSDobaI6IuWQdAF4q6eS0zymROj2RtJKsAXtLSUcA742IpZ14jmZm1gll7qRuAsZHJ3qriojryBqf88v+O/d4KfCOVrYd3dHjmZlZ9ylTxXQ/8PqqAzEzs8ZS5gxiKLBU0l3ACy0LI+KwyqIyM7O6K5MgplcdhJmZNZ4yl7neJmkU2cBBN0vahqzR2cys4Q0ZMoR169Z1aR9dHXVu8ODBrF27tkv7qIcyVzF9BjiObBzqsWR3Q19MujzVzKyRrVu3ru4jwnXXsKY9rUwj9QlkVxo9BRARfwF2rDIoMzOrvzIJ4oXU2R4AkjYn9ctkZmZ9V5kEcZukrwBbSzoU+Dnwq2rDMjOzeiuTIE4n6/7iPuCzZDe+nVFlUGZmVn9lrmLaAFyaJjOzXiXOHADTB9Y/hl6o1QQh6XBgRERckOb/CLT0qX1aRPy8B+IzM+sSnfVUQ1zFFNPrGkKntFXF9CVe3fvqVsC+wLuBz1UYk5mZNYC2qpi2jIj8iHDzI+IfwD8kbVtxXGZmVmdtnUEMzs9ExIm5WQ/fZmbWx7WVIP6Y7qJ+FUmfBe6qLiQzM2sEbVUxnQxcI+kYYFFa9jaytogjqg7MzMzqq9UEERFPAP8m6T3AHmnxtRFxS49EZmZmdVXmPohbACcFM7NNTJk7qc3MbBPkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QZmZWyAnCzMwKOUGYmVmhShOEpImSlklaLun0gvUjJc2TtFjSvZIm5dZ9OW23TNL7qozTzMxeq907qTtLUj/gAuBQYDWwQNLciFiaK3YGMCciLpI0nmw409Hp8dFkXXzsBNwsadeIeLmqeM3M7NWqPIPYD1geESsi4kXgKuDwmjIBtIzFNxB4LD0+HLgqIl6IiIeA5Wl/ZmYdJqmu0+DBg9sPsgFVdgYBDAfyAw6tBvavKTMduFHSScC2wCG5be+s2XZ4NWGaWV/W1eFGJdV9yNJ6qXcj9WRgZkSMACYBsySVjknScZKaJTWvWbOmsiDNzDZFVSaIR4Gdc/Mj0rK8qcAcgIi4A+gPDC25LRFxSUQ0RUTTsGEe5M7MrDtVmSAWALtIGiNpS7JG57k1ZR4BDgaQNI4sQaxJ5Y6WtJWkMcAueBQ7M7MeVVkbRES8JOlE4AagH3B5RCyRdDbQHBFzgVOASyWdTNZgPSWyyr4lkuYAS4GXgBN8BZOZWc9SX2l8aWpqiubm5nqHYWZ9TF9vpJa0MCKaitbVu5HazMwalBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVqmxMajOz3kBSt5Tpi8OSOkGY2SatL36xdxdXMZmZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAqpr9wkImkN8HC946jQUODv9Q7COs2vX+/V11+7URExrGhFn0kQfZ2k5ohoqncc1jl+/XqvTfm1cxWTmZkVcoIwM7NCThC9xyX1DsC6xK9f77XJvnZugzAzs0I+gzAzs0JOEBWR9HTJckdLmtbKup0k/aJ7I3vNMZok/aCVdSslDa3y+JuSsu8Jq4akyySNr/gY10kaVLB8uqRTqzx2FTxgUBcoG2ZKEbGhC7t5P1D4BR0RjwFHdmHf7YqIZqC5ymNsSrrpPWEViIj/1QPHmFT1MXqSzyA6SNJoScskXQHcD3xV0gJJ90o6q6D8uyX9Ojd/vqQp6bGAvYFFkt4l6e40LZa0fTrW/ansNpLmSFoq6WpJf5TUlNY9LelcSUsk3SxpP0m3Sloh6bBUpr+kH0u6L+1/Qm18knaQdGPaz2VA++MsWre+J6x7SNpW0rWS7pF0v6Sj0mei5TMzVdKfJd0l6VJJ56flMyVdJOnO9Pl5t6TLJT0gaWZu/5PTZ+l+Sd/KLX/lrFvStHSM+cBuPfsf6B5OEJ2zC3AhcDIwHNiP7Iv+bZLe2YH9vBW4J7IrBU4FToiIvYGDgOdqyh4PrIuI8cBXgbfl1m0L3BIRewD/Ar4GHAp8CDg7lTkBiIjYC5gM/I+k/jXHOBOYn/ZzNTCyA89lU9dd7wnrHhOBxyLiLRGxJ/CblhWSdiL7DL0deAewe822g4EDyF7LucB5wB7AXpL2Ttt/C3gP2Wu8r6Qj8juQ9Dbg6LR+ErBvtz/DHuAE0TkPR8SdwHvTtBhYRPZG26UD+5kIXJ8e3w58T9J/AIMi4qWasgcCVwFExP3Avbl1L7LxA3AfcFtErE+PR+e2/0na/k9k3ZLsWnOMd+bKXAus68Bz2dR113vCusd9wKGSviXpoIh4MrduP7LPyNr0Ofl5zba/Sj/a7gP+FhH3pSrDJWSfp32BWyNiTfqcXkn22ck7CLg6Ip6NiKfIEk2v4zaIznkm/RXwjYj4URtlX+LViTj/q/29wEcAIuKbkq4l+7Vxu6T3Ac+XjGd9bLxeeQPwQtrnBkl+jXtGd70nrBtExJ8l7UP2efqapN92YPMX0t8Nucct85sD67snysbnM4iuuQH4tKTtACQNl7RjTZmHgfGStkpXNxycyg4ENo+If6T5semXyreABbz2tPd24KOp7Hhgrw7G+nvgY2n7Xcmqj5bVlPkdcEwq836yU23rmE6/J6z7pGqgZyPiJ8C5wD651QuAd0kanH5AfaSDu78rbT9UUj+yKtvbasr8DjhC0taStgc+2KknUmf+ddkFEXGjpHHAHVl7M08DHweeyJVZJWkOWePlQ2RVD5C1Edyc290XUsNxy6ns9cAbcusvJGs3WAr8KZXJnza350LgIkn3kf2CnRIRL6S4W5wFzJa0BPgD8EgH9m90+T1h3Wcv4FxJG8h+8X8e+A5ARDwq6etkX/RryT5PpT9LEfG4pNOBeWRnjNdGxP+rKbNI0s+Ae8he+wVdf0o9z3dS10m6SuiyVG9dpnw/YIuIeF7SWLLksltEvFhlnGZ9kaTtIuLpdAZxNXB5RFxd77gajc8g6qQT12RvA8yTtAXZr5bjnRzMOm26pEPI2n9uBK6pczwNyWcQZmZWyI3UZmZWyAnCzMwKOUGYmVkhJwjrVSQdISkk1d4nUlT2C5K2yc0X9rRZYj+DJB2fm++2XnZT/0DLtLEfrm7rnDF1CzEpN39YujzTrBQ3Uluvkq4t34ms76kz2ym7EmiKiL938ZijgV+nPn26laRbgVNTr7rdve8pZM//xO7et20afAZhvUa6O/lAYCpZR2gty/tJ+k7qWfNeSSelPq12Irs0eF4qtzLd/fpNSSfktp8u6VRJ20n6raRFqafOw1ORbwJj0y/8c/XqXnZb6yV3iqRfSvqNpL9I+gDf9fEAAALSSURBVHYHnufM/JmE0jgSynoWvVXSLyT9SdKVSnfjSdpX0h+U9V56V7pT/2zgqBT3USmmll5LR0u6Jf2/fitpZO7YP0j7WtGdZzTWC0WEJ0+9YiLrKmRGevwH4G3p8eeBX5B1XQIwJP1dCQzNbb8SGErWi+5tueVLgZ3J7gsakJYNBZaT3XMyGrg/V/6VeeAUspusIOse5RGya+unACuAgWn+YWDngud0K1mXJ3enaQdgJnBkrszT6e+7ye74HUH24+4OsoS5ZTrWvqncgPRcpgDn5/bzyjzwK+BT6fGngWvS45lknddtBowHltf7dfdUv8lnENabTCb1aJv+Tk6PDwF+FKkH3IhY29ZOImIxsGNqS3gLWTfqq8iSwdcl3Ut2p/pw4HXtxNRWL7m/jYgnI+J5siQ0qpV9fCwi9k7TP9o53l0RsTqy3kXvJktWuwGPR8SCFMdT8dregGsdAPw0PZ6VnkeLayJiQ0Qspf3nb32Y76S2XkHSELL+9/eSFEA/ICT9Vyd3+XOy0fpeD/wsLfsYMIzszGR9asPoSk+r+Z5AX6b85+2V3l4lbUZ2htDVfXZE/hgeNGoT5jMI6y2OBGZFxKiIGB0RO5N1dHcQcBPw2dSvTksygWzwpO1b2d/PyNoxjmTjeAADgSdScpjAxl/8be2nTC+5HbWSjQNCHQZs0U75ZcAbJO2b4tg+/S/aivsPbGzH+RjZ8zB7FScI6y0mk3Wqlvd/0/LLyOr+75V0D6nLcuAS4DctjdR5EbGE7Mvz0Yh4PC2+EmhKPd5+kqyXT1K1z+2pEfzcml1dCGyWtvkZqZfcrj1VLiXrTvoesqqgZ9oqHFmfXEcBP0zb3ER25jOPrFvxuyUdVbPZScCxqTrtE8B/djFm64N8mauZmRXyGYSZmRVygjAzs0JOEGZmVsgJwszMCjlBmJlZIScIMzMr5ARhZmaFnCDMzKzQ/weV0WfmNsCMQAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 996us/step - loss: 0.1517 - accuracy: 0.7833\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.8308\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8336\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8372\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8391\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8383\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1441 - accuracy: 0.7934\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8385\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8373\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8389\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8348\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8404\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8415\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1504 - accuracy: 0.7917\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8352\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8408\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8378\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8353\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8431\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.8449\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8401\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 988us/step - loss: 0.1483 - accuracy: 0.7967\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8366\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8372\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8330\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8376\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8369\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8384\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8436\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1467 - accuracy: 0.7877\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8350\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8397\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1110 - accuracy: 0.8402\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8396\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8406\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8419\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 994us/step - loss: 0.1086 - accuracy: 0.8425\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 990us/step - loss: 0.1066 - accuracy: 0.8430\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8408\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1432 - accuracy: 0.8011\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 995us/step - loss: 0.1147 - accuracy: 0.8324\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8351\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 965us/step - loss: 0.1086 - accuracy: 0.8419\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8438\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8429\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8432\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8399\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 989us/step - loss: 0.1487 - accuracy: 0.7899\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8342\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8334\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8417\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8419\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8441\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8416\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1497 - accuracy: 0.7934\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1188 - accuracy: 0.8292\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1148 - accuracy: 0.8327\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8363\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8367\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8368\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8378\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8422\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8434\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1452 - accuracy: 0.7964\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8387\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8351\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8380\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8410\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8374\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8381\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1075 - accuracy: 0.8436\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1478 - accuracy: 0.7929\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8313\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.8341\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8323\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.8376\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8398\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8360\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8422\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8434\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1498 - accuracy: 0.7911\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8350\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8370\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1080 - accuracy: 0.8444\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8417\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1090 - accuracy: 0.8415\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1093 - accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8403\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1066 - accuracy: 0.8444\n",
            "relu/sigmoid: 0.838937 (0.006094)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 875us/step - loss: 0.1777 - accuracy: 0.7648\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 921us/step - loss: 0.1185 - accuracy: 0.8246\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 869us/step - loss: 0.1131 - accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 890us/step - loss: 0.1127 - accuracy: 0.8334\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 888us/step - loss: 0.1096 - accuracy: 0.8384\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 932us/step - loss: 0.1084 - accuracy: 0.8430\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 883us/step - loss: 0.1089 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 949us/step - loss: 0.1071 - accuracy: 0.8444\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 892us/step - loss: 0.1092 - accuracy: 0.8409\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 896us/step - loss: 0.1082 - accuracy: 0.8416\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "679/679 [==============================] - 1s 829us/step - loss: 0.1650 - accuracy: 0.7744\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 848us/step - loss: 0.1217 - accuracy: 0.8242\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 794us/step - loss: 0.1123 - accuracy: 0.8383\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 950us/step - loss: 0.1126 - accuracy: 0.8360\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 868us/step - loss: 0.1103 - accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 932us/step - loss: 0.1106 - accuracy: 0.8367\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 894us/step - loss: 0.1084 - accuracy: 0.8436\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 878us/step - loss: 0.1100 - accuracy: 0.8378\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 914us/step - loss: 0.1067 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 865us/step - loss: 0.1080 - accuracy: 0.8401\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 845us/step - loss: 0.1613 - accuracy: 0.7856\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 825us/step - loss: 0.1137 - accuracy: 0.8367\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 836us/step - loss: 0.1134 - accuracy: 0.8341\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 862us/step - loss: 0.1103 - accuracy: 0.8390\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 940us/step - loss: 0.1102 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 856us/step - loss: 0.1109 - accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 877us/step - loss: 0.1088 - accuracy: 0.8406\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1083 - accuracy: 0.8412\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1096 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1087 - accuracy: 0.8413\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 847us/step - loss: 0.1790 - accuracy: 0.7641\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 872us/step - loss: 0.1155 - accuracy: 0.8339\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 888us/step - loss: 0.1120 - accuracy: 0.8355\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 846us/step - loss: 0.1078 - accuracy: 0.8440\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 814us/step - loss: 0.1099 - accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 866us/step - loss: 0.1090 - accuracy: 0.8410\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 902us/step - loss: 0.1079 - accuracy: 0.8440\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 890us/step - loss: 0.1077 - accuracy: 0.8426\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 870us/step - loss: 0.1085 - accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 871us/step - loss: 0.1076 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 850us/step - loss: 0.1812 - accuracy: 0.7636\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 809us/step - loss: 0.1215 - accuracy: 0.8290\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 834us/step - loss: 0.1123 - accuracy: 0.8374\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 824us/step - loss: 0.1132 - accuracy: 0.8345\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 870us/step - loss: 0.1112 - accuracy: 0.8402\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 840us/step - loss: 0.1116 - accuracy: 0.8366\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 875us/step - loss: 0.1113 - accuracy: 0.8396\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 922us/step - loss: 0.1132 - accuracy: 0.8349\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1120 - accuracy: 0.8374\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 847us/step - loss: 0.1134 - accuracy: 0.8313\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 811us/step - loss: 0.1708 - accuracy: 0.7693\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 807us/step - loss: 0.1147 - accuracy: 0.8317\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 856us/step - loss: 0.1136 - accuracy: 0.8338\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 860us/step - loss: 0.1124 - accuracy: 0.8342\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 831us/step - loss: 0.1115 - accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 876us/step - loss: 0.1101 - accuracy: 0.8389\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 776us/step - loss: 0.1097 - accuracy: 0.8404\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 847us/step - loss: 0.1096 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 845us/step - loss: 0.1089 - accuracy: 0.8412\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 860us/step - loss: 0.1092 - accuracy: 0.8407\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 815us/step - loss: 0.1651 - accuracy: 0.7707\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 883us/step - loss: 0.1138 - accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 862us/step - loss: 0.1137 - accuracy: 0.8328\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 808us/step - loss: 0.1119 - accuracy: 0.8353\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 915us/step - loss: 0.1128 - accuracy: 0.8346\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 841us/step - loss: 0.1133 - accuracy: 0.8349\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 873us/step - loss: 0.1126 - accuracy: 0.8364\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 813us/step - loss: 0.1130 - accuracy: 0.8336\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 915us/step - loss: 0.1104 - accuracy: 0.8369\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 853us/step - loss: 0.1113 - accuracy: 0.8348\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 846us/step - loss: 0.1659 - accuracy: 0.7697\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 871us/step - loss: 0.1181 - accuracy: 0.8245\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 877us/step - loss: 0.1125 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1135 - accuracy: 0.8334\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 887us/step - loss: 0.1109 - accuracy: 0.8378\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 943us/step - loss: 0.1118 - accuracy: 0.8342\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 867us/step - loss: 0.1110 - accuracy: 0.8386\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 941us/step - loss: 0.1129 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 915us/step - loss: 0.1098 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 904us/step - loss: 0.1109 - accuracy: 0.8390\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 854us/step - loss: 0.1701 - accuracy: 0.7633\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 855us/step - loss: 0.1141 - accuracy: 0.8365\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 870us/step - loss: 0.1141 - accuracy: 0.8318\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 892us/step - loss: 0.1121 - accuracy: 0.8351\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 836us/step - loss: 0.1125 - accuracy: 0.8371\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 881us/step - loss: 0.1099 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 862us/step - loss: 0.1092 - accuracy: 0.8413\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 890us/step - loss: 0.1085 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 924us/step - loss: 0.1087 - accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 922us/step - loss: 0.1082 - accuracy: 0.8414\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 862us/step - loss: 0.1739 - accuracy: 0.7684\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 866us/step - loss: 0.1168 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 886us/step - loss: 0.1138 - accuracy: 0.8329\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 859us/step - loss: 0.1103 - accuracy: 0.8383\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 887us/step - loss: 0.1087 - accuracy: 0.8398\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 884us/step - loss: 0.1102 - accuracy: 0.8396\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 862us/step - loss: 0.1104 - accuracy: 0.8352\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 880us/step - loss: 0.1090 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 881us/step - loss: 0.1097 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 856us/step - loss: 0.1080 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 1s 860us/step - loss: 0.1709 - accuracy: 0.7649\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 854us/step - loss: 0.1185 - accuracy: 0.8311\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 851us/step - loss: 0.1136 - accuracy: 0.8389\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 853us/step - loss: 0.1118 - accuracy: 0.8399\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 857us/step - loss: 0.1104 - accuracy: 0.8390\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 880us/step - loss: 0.1081 - accuracy: 0.8433\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 905us/step - loss: 0.1079 - accuracy: 0.8437\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 930us/step - loss: 0.1091 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 871us/step - loss: 0.1081 - accuracy: 0.8414\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 909us/step - loss: 0.1099 - accuracy: 0.8359\n",
            "relu: 0.839500 (0.005745)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1921 - accuracy: 0.7392\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1887 - accuracy: 0.7479\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1489 - accuracy: 0.7812\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1369 - accuracy: 0.8049\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1377 - accuracy: 0.8033\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1378 - accuracy: 0.8028\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1380 - accuracy: 0.8067\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1365 - accuracy: 0.8078\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1385 - accuracy: 0.8048\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1377 - accuracy: 0.8031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1887 - accuracy: 0.7432\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1859 - accuracy: 0.7538\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1579 - accuracy: 0.7717\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1393 - accuracy: 0.7980\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1396 - accuracy: 0.7993\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1415 - accuracy: 0.7995\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1391 - accuracy: 0.8022\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1408 - accuracy: 0.7980\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1380 - accuracy: 0.8041\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1378 - accuracy: 0.8053\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1872 - accuracy: 0.7519\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1864 - accuracy: 0.7531\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1789 - accuracy: 0.7571\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1399 - accuracy: 0.8016\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1384 - accuracy: 0.8034\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1410 - accuracy: 0.7995\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1379 - accuracy: 0.8065\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1371 - accuracy: 0.8074\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1369 - accuracy: 0.8068\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1382 - accuracy: 0.8067\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1948 - accuracy: 0.7288\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1882 - accuracy: 0.7494\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1553 - accuracy: 0.7722\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1389 - accuracy: 0.8044\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1407 - accuracy: 0.8015\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1382 - accuracy: 0.8056\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1370 - accuracy: 0.8067\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1368 - accuracy: 0.8067\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1372 - accuracy: 0.8062\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1368 - accuracy: 0.8052\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1940 - accuracy: 0.7331\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1885 - accuracy: 0.7492\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.7595\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1412 - accuracy: 0.8003\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8022\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1394 - accuracy: 0.8013\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1396 - accuracy: 0.8038\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1376 - accuracy: 0.8048\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1403 - accuracy: 0.8035\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8044\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 977us/step - loss: 0.1895 - accuracy: 0.7475\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1851 - accuracy: 0.7555\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7563\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1423 - accuracy: 0.7941\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1361 - accuracy: 0.8080\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1394 - accuracy: 0.8043\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1369 - accuracy: 0.8073\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1353 - accuracy: 0.8085\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1372 - accuracy: 0.8063\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1368 - accuracy: 0.8075\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1883 - accuracy: 0.7496\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1849 - accuracy: 0.7557\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1683 - accuracy: 0.7623\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1408 - accuracy: 0.7969\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1374 - accuracy: 0.8090\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1389 - accuracy: 0.8036\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1390 - accuracy: 0.8026\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8047\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1390 - accuracy: 0.8017\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1395 - accuracy: 0.8024\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.2016 - accuracy: 0.7142\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1878 - accuracy: 0.7503\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1644 - accuracy: 0.7678\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1404 - accuracy: 0.7980\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1408 - accuracy: 0.7996\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1371 - accuracy: 0.8071\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1389 - accuracy: 0.8043\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1391 - accuracy: 0.8031\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1404 - accuracy: 0.8022\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1382 - accuracy: 0.8048\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1873 - accuracy: 0.7521\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1865 - accuracy: 0.7530\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1823 - accuracy: 0.7503\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1394 - accuracy: 0.8017\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1392 - accuracy: 0.8026\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1387 - accuracy: 0.8035\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1358 - accuracy: 0.8105\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1383 - accuracy: 0.8032\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1380 - accuracy: 0.8041\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8051\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1885 - accuracy: 0.7493\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1878 - accuracy: 0.7505\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1883 - accuracy: 0.7480\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1438 - accuracy: 0.7902\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1398 - accuracy: 0.8011\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1385 - accuracy: 0.8031\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1390 - accuracy: 0.8061\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1404 - accuracy: 0.8020\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1398 - accuracy: 0.8015\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8051\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 1ms/step - loss: 0.1892 - accuracy: 0.7510\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.7553\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1459 - accuracy: 0.7836\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1400 - accuracy: 0.7983\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1402 - accuracy: 0.7995\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1385 - accuracy: 0.8030\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1389 - accuracy: 0.8047\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1371 - accuracy: 0.8050\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1385 - accuracy: 0.8052\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1402 - accuracy: 0.8009\n",
            "sigmoid: 0.803462 (0.007278)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEjCAYAAAA/ugbCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wcVZ338c+XkBiBAAkJKpcQxIgJQUEHkBVXwmUFXQP7wLMmwQuSFZfLqCgKa1gIKKsIC48m4IqAQS4BROFBidwDbFAuCYRbIhADgSCuAQJINCQkv/2jzoSimempmenqnpn+vl+vfqVP1alTv+p5pX9d51SdUkRgZmbNa4NGB2BmZo3lRGBm1uScCMzMmpwTgZlZk3MiMDNrck4EZmZNzonArJeT9JSk/Rodh/VfTgTWY+mLarWk4RXLH5AUkkY1IKZvSXpS0quSlkm6st4x1IOkmemzfzX3erDRcVnf4kRgtfIkMKmtIGlnYKNGBCLp88Bngf0iYhOgBbi1AXFsWGRZDXw/IjbJvT5QVjwlxW8N5kRgtXIJ8Llc+fPAz/IVJL1N0lmSnpb0P5L+S9Lb07qhkn4tabmkFen9Nrltb5f0bUl3SfqLpJsqz0BydgNujIg/AETEnyLi/Fxb20u6I7Vzs6QZki5N6/aWtKwi7vVdM5J2l/Q7SS9Jei5tOyhXNyQdI+kJ4Im29iSdIOlPwE8lbSDpREl/kPSCpKskDcu18VlJS9O6qV34G7yJpFEpnimSngZuk3R4+gzPkfQCME3SZpJ+lj77pZJOkrRBauMt9bsbj/VeTgRWK3cDm0oaI2kAMBG4tKLO94D3ArsA7wG2Bk5O6zYAfgpsB4wE/gbMqNh+MvAFYEtgEHB8lVg+J+kbklpSPHmXA/OB4cC3yZJWUWuB49K2ewL7AkdX1DkY2AMYm8rvBIalYzsSaE11PgZsBawAzgWQNBb4EdkZzVbAFsA29MzHgDHAx1N5D2AJ8A7gdGA6sBnw7lT3c2SfMx3Ut/4mIvzyq0cv4ClgP+Ak4LvAAcDNwIZAAKMAASuBHXLb7Qk82UGbuwArcuXbgZNy5aOBG6rEdBhwS9rnC8AJaflI4HVg41zdy4FL0/u9gWXtHV8H+/kqcE2uHMA+ufLewGpgcG7ZImDfXPldwJr0eZ0MXJFbt3HavqP9zwRWAS/lXhendaNSPO/O1T8ceDpXHpDaH5tb9iXg9vbq+9U/X+7vs1q6BLgT2J6KbiFgBNmYwXxJbctE9kWEpI2Ac8iSyNC0foikARGxNpX/lGvvr8AmHQUSEZcBl0kaSPbr+zJJC4CXyRLMylz1pcC2RQ5Q0nuBs8nGHTYi+/KeX1HtmYry8ohYlStvB1wjaV1u2VqyX9xb5bePiJWpS6aasyLipCrrK+PJl4cDA8k+gzZLyc7WOtre+hl3DVnNRMRSskHjTwC/rFj9PFl3z04RsXl6bRbZYC7A14EdgT0iYlPg79Ny0QMRsSYifg48BIwDngOGSto4V21k7v1KcoPcqVtpRG79j4DfA6NTnN9qJ8bKKX0ry88AB+Y+h80jYnBEPJviW5+UUoLcotjRdqhaPM+TnY1sl1s2Eni2yvbWzzgRWK1NIesayf/iJiLWAT8BzpG0JYCkrSW19VsPIUsUL6WB01O6G0Aa4PykpCFpYPZAYCfgnpSs5gGnShokaS/gU7nNHwcGp+0HknV3vS23fgjwCvCqpPcBR3UjxP8CTpe0XYp3hKSD0rqrgX+UtFcahD6NEv+fprOtq1I8Q1JMX+Ot4zvWjzkRWE1FxB8iYl4Hq08AFgN3S3qFrA9/x7Tu/wFvJ/uFejdwQw/CeIXsl/rTZH3m3weOioi5af1ksgHQF8kSzvpurIh4mWz84QKyX8UrgfxVRMen7f9Clti6c3/CD4DrgJsk/YXsePdI+38UOIZs3OI5soHkZR200+abFfcRPN/FeFrJjnMJMDft+6IutmF9mCJ81mfNTdI04D0R8ZlGx2LWCD4jMDNrck4EZmZNzl1DZmZNzmcEZmZNzonAzKzJORGYmTU5JwIzsybnRGBm1uScCMzMmpwTgZlZk3MiMDNrck4EZmZNzonAzKzJORGYmTU5JwIzsybnRGBm1uScCMzMmtyGjQ6gq4YPHx6jRo1qdBhmZn3K/Pnzn4+IEe2t63OJYNSoUcyb19Ejcc3MrD2Slna0zl1DZmZNzonAzKzJORGYmTU5JwIzsybnRGBm1uScCDowa9Ysxo0bx4ABAxg3bhyzZs1qdEhmZqXoc5eP1sOsWbOYOnUqF154IXvttRdz585lypQpAEyaNKnB0ZmZ1ZYiotExdElLS0uUfR/BuHHjmD59OuPHj1+/bM6cObS2tvLII4+Uum8zszJImh8RLe2ucyJ4qwEDBrBq1SoGDhy4ftmaNWsYPHgwa9euLXXfZmZlqJYIPEbQjjFjxjB37tw3LZs7dy5jxoxpUERWjaRuv8zMiaBdU6dOZcqUKcyZM4c1a9YwZ84cpkyZwtSpUxsdmrUjIjp8FVlv1uw8WNyOtgHh1tZWFi1axJgxYzj99NM9UGxm/ZLHCKBHXQR97fPrq4YNG8aKFSvqtr+hQ4fy4osv1m1/ZmWrNkbQPGcE0zbrcFWcsmkp7TLt5e63a2/y4pfXAj34O3WZLwqw5tE0ZwSS6vrrvd776+/89zPrGZ8RJPW8SmTo0KF121ez8N/PrBxNkwi6++vOvwx7h2p/A4/xmPVM0ySCajr7Iqm23l8kjee/gVnPOBHgLxIza26+oczMehXP/Ft/PiMws17DM/82RtNcPmpmvZ9n/i2PZx81sz7BM/+Wx7OPmlmf4Jl/G6PUMQJJBwA/AAYAF0TE9yrWjwQuBjZPdU6MiNllxmRmvUd7l2bvs88+her2td6M3qy0MwJJA4BzgQOBscAkSWMrqp0EXBURuwITgfPKisfMGmPYsGGlPA+iozaHDRtWo8ibR5lnBLsDiyNiCYCkK4CDgIW5OsEbM4ltBvyxxHjMrAFWrFhR93mirGvKTARbA8/kysuAPSrqTANuktQKbAzsV2I8ZtYAccqm1WfpLWN/1iWNvo9gEjAzIv5T0p7AJZLGRcS6fCVJRwJHAowcObIBYZpZt3VhOvaO7iPwg6HKVeZVQ88C2+bK26RleVOAqwAi4nfAYGB4ZUMRcX5EtEREy4gRI0oK18wa7fTTT2fy5Mm0trYyePBgWltbmTx5MqeffnqjQ+vXyjwjuA8YLWl7sgQwEZhcUedpYF9gpqQxZIlgeYkxmVkvtnDhQlauXMlFF120/ozgiCOOYOnSpY0OrV8r7YwgIl4HjgVuBBaRXR30qKTTJE1I1b4OfFHSg8As4PDwNWFmTWvQoEG0trYyfvx4Bg4cyPjx42ltbWXQoEGNDq1fK3WMIN0TMLti2cm59wuBj5QZg5n1HatXr2bGjBnsuuuu688IZsyYwerVqxsdWr/W6MFiM7P1xo4dy8EHH0xrayuLFi1izJgxTJ48mWuvvbbRofVrnmLCzHqNqVOncvnllzN9+nRWrVrF9OnTufzyy5k6dWqjQ+vXfEZgZr1G2yWi+TMCXzpaPs8+ambWBDz7qJmZdciJwMysyTkRmJk1OScCM7Mm50RgZtbknAjMzJqcE4GZWZNzIjAza3JOBGZmTa5QIpC0l6QvpPcj0jMGzMysH+g0EUg6BTgB+Le0aCBwaZlBmZlZ/RQ5I/gnYAKwEiAi/ggMKTMoMzOrnyKJYHV6algASNq43JDMzKyeiiSCqyT9GNhc0heBW4ALyg3LzMzqpdPnEUTEWZL2B14BdgROjoibS4/MzMzqotNEIOmMiDgBuLmdZWZm1scV6Rrav51lB9Y6EDMza4wOzwgkHQUcDbxb0kO5VUOAu8oOzMzM6qNa19DlwG+A7wIn5pb/JSJeLDUqMzOrmw4TQUS8DLwMTAKQtCUwGNhE0iYR8XR9QjQzszIVubP4U5KeAJ4E7gCeIjtTMDOzfqDIYPF3gA8Dj0fE9sC+wN2lRmVmZnVTJBGsiYgXgA0kbRARc4CWkuMyM7M66fQ+AuAlSZsAdwKXSfozad4hMzPr+4qcERwE/A04DrgB+APwj2UGZWZm9dNpIoiIlRGxNiJej4iLgRuBM4o0LukASY9JWizpxHbWnyNpQXo9Lumlrh+CmZn1RIeJQNL7Jd0k6RFJ35H0Lkm/AG4FFnbWsKQBwLlkdyGPBSZJGpuvExHHRcQuEbELMB34ZU8OxszMuq7aGcFPyG4qOwRYDiwg6xZ6T0ScU6Dt3YHFEbEkIlYDV5B1M3VkEjCrUNRmZlYz1RLB2yJiZkQ8FhE/AFZGxDcjYlXBtrcGnsmVl6VlbyFpO2B74LYO1h8paZ6kecuXLy+4ezMzK6LaVUODJe0KKJVfy5cj4v4axjERuDoi1ra3MiLOB84HaGlpiRru18ys6VVLBM8BZ+fKf8qVA9ink7afBbbNlbdJy9ozETimk/bMzKwE1eYaGt/Dtu8DRkvaniwBTAQmV1aS9D5gKPC7Hu7PzMy6och9BN0SEa8Dx5JdbroIuCoiHpV0mqQJuaoTgSvSc5HNzKzOitxZ3G0RMRuYXbHs5IrytDJjMDOz6qqeESizbbU6ZmbWt1VNBKm7Zna1OmZm1rcVGSO4X9JupUdiZmYNUWSMYA/gMElLyWYdFdnJwvtLjczMzOqiSCL4eOlRmJlZwxSZfXQpsDnwqfTaPC0zM7N+oMgzi78CXAZsmV6XSmotOzAzM6uPIl1DU4A9ImIlgKQzyO4Cnl5mYGZmVh9FrhoSkJ8Mbi1vTERnZmZ9XJEzgp8C90i6JpUPBi4sLyQzM6unqolA0gbA3cDtwF5p8Rci4oGS4zIzszqpmggiYp2kcyNiV6CWzx8wM7NeosgYwa2SDpHkcQEzs36oSCL4EvBzsieUvSLpL5JeKTkuMzOrkyJjBAdExF11isfMzOqss9lH1wEz6hSLmZk1gMcIzMyanMcIzMyaXKc3lEXEkHoEYmZmjdHhGYGkz+Tef6Ri3bFlBmVmZvVTrWvoa7n3lRPMHVFCLGZm1gDVEoE6eN9e2czM+qhqiSA6eN9e2czM+qhqg8Xvk/QQ2a//HdJ7UvndpUdmZmZ1US0RjKlbFGZm1jAdJgI/l9jMrDkUuaHMzMz6MScCM7MmV2oikHSApMckLZZ0Ygd1/lnSQkmPSrq8zHjMzOytOp1iIt1VPA3YLtUXEBFR9cohSQOAc4H9gWXAfZKui4iFuTqjgX8DPhIRKyRt2d0DMTOz7iny8PoLgeOA+cDaLrS9O7A4IpYASLoCOAhYmKvzReDciFgBEBF/7kL7ZmZWA0USwcsR8ZtutL018EyuvAzYo6LOewEk3QUMAKZFxA2VDUk6EjgSYOTIkd0IxczMOlIkEcyRdCbwS+C1toURUYuH2W8IjAb2BrYB7pS0c0S8lK8UEecD5wO0tLT4rmYzsxoqkgjafsW35JYFsE8n2z0LbJsrb5OW5S0D7omINcCTkh4nSwz3FYjLzMxqoMjzCMZ3s+37gNGStidLABOByRV1rgUmAT+VNJysq2hJN/dnZmbd0Onlo5I2k3S2pHnp9Z+SNutsu4h4HTgWuBFYBFwVEY9KOk3ShFTtRuAFSQuBOcA3IuKF7h+OmZl1lSKqd7lL+gXwCHBxWvRZ4AMR8X9Kjq1dLS0tMW/evEbs2sysz5I0PyJa2ltXZIxgh4g4JFc+VdKC2oRmZmaNVuTO4r9J2qutkG4w+1t5IZmZWT0VOSM4Crg4jQsIeBE4vMygzMysfopcNbQA+ICkTVP5ldKjMjOzuukwEUj6TERcKulrFcsBiIizS47NzMzqoNoZwcbp3yHtrPPdvWZm/US1J5T9OL29JSLuyq9LA8ZmZtYPFLlqaHrBZWZm1gdVGyPYE/g7YETFOMGmZDOFmplZP1BtjGAQsEmqkx8neAU4tMygzMysfqqNEdwB3CFpZkQsrWNMZmZWR0VuKPtreh7BTsDgtoUR0dk01GZm1gcUGSy+DPg9sD1wKvAUfl6AmVm/USQRbBERFwJrIuKOiDiCzh9KY2ZmfUSRrqE16d/nJH0S+CMwrLyQzMysnookgu+kCee+Tnb/wKbAcaVGZWZmdVMkETwYES8DLwPjASS9s9SozMysboqMETwpaZakjXLLZpcVkJmZ1VeRRPAw8N/AXEk7pGUqLyQzM6unIl1DERHnSXoQ+JWkE/Dso2Zm/UaRRCCAiLhL0r7AVcD7So3KzMzqpkgi+ETbm4h4TtJ4ssnozMysH+j0CWXApLanklW4s7SozMysbrr7hDIzM+snOn1CWUScWr9wzMys3qp1Df2w2oYR8eXah2NmZvVWrWtoft2iMDOzhqnWNXRxPQMxM7PG6PTOYkkjJJ0labak29peRRqXdICkxyQtlnRiO+sPl7Rc0oL0+pfuHISZmXVf0QfTLKKLD6aRNAA4FzgQGEt2GerYdqpeGRG7pNcFRQM3M7PaKPPBNLsDiyNiSUSsBq4ADupBrGZmVoIiieBND6aRtCvFHkyzNfBMrrwsLat0iKSHJF0tadv2GpJ0pKR5kuYtX768wK7NzKyoIokg/2Ca44ELqN2DaX4FjIqI9wM3A+0OUEfE+RHREhEtI0aMqNGuzcwMOplrKPXzj46IX5N7ME1BzwL5X/jbpGXrRcQLueIFwPe70L6ZmdVA1TOCiFgLTOpm2/cBoyVtL2kQMBG4Ll9B0rtyxQlkg9JmZlZHRWYfvUvSDOBKYGXbwoi4v9pGEfG6pGOBG4EBwEUR8aik04B5EXEd8GVJE4DXgReBw7t3GGZm1l2KqP6MGUlz2lkcEVHkyqGaa2lpiXnz5jVi12ZmfZak+RHR0t66Ts8IIqIr4wJmZtbHFLmz+B2SLpT0m1QeK2lK+aGZmVk9FLl8dCZZP/9Wqfw48NWyAjIzs/oqkgiGR8RVwDrIBoGBtaVGZWZmdVMkEayUtAUQAJI+THZPgZmZ9QNFLh/9Gtn1/ztIugsYARxaalRmZlY3Ra4aul/Sx4AdAQGPRcSaTjYzM7M+osgZAWQziY5K9T8oiYj4WWlRmZlZ3XSaCCRdAuwALOCNQeIAnAjMzPqBImcELcDY6OwWZDMz65OKXDX0CPDOsgMxM7PGKHJGMBxYKOle4LW2hRExobSozMysbookgmllB2FmZo1T5PLROyRtR/aAmlskbUQ2rbSZmfUDRSad+yJwNfDjtGhr4NoygzIzs/opMlh8DPAR4BWAiHgC2LLMoMzMrH6KJILXImJ1W0HShqR5h8zMrO8rkgjukPQt4O2S9gd+Dvyq3LDMzKxeiiSCE4HlwMPAl4DZwEllBmVmZvVT5KqhdcBP0svMzPqZDs8IJB0k6Zhc+R5JS9Lr/9YnPDMzK1u1rqFvkj2HoM3bgN2AvYF/LTEmMzOro2pdQ4Mi4plceW5EvAC8IGnjkuMyM7M6qXZGMDRfiIhjc8UR5YRjZmb1Vi0R3JPuKn4TSV8C7i0vJDMzq6dqXUPHAddKmgzcn5Z9iGys4OCyAzMzs/roMBFExJ+Bv5O0D7BTWnx9RNxWl8jMzKwuitxHcBvgL38zs36qyJ3F3SbpAEmPSVos6cQq9Q6RFJJayozHzMzeqrREIGkAcC5wIDAWmCRpbDv1hgBfAe4pKxYzM+tYmWcEuwOLI2JJmr30CuCgdup9GzgDWFViLGZm1oEyE8HWQP6GtGVp2XqSPghsGxHXV2tI0pGS5kmat3z58tpHambWxEodI6hG0gbA2cDXO6sbEedHREtEtIwY4XvZzMxqqcxE8Cywba68TVrWZggwDrhd0lPAh4HrPGBsZlZfZSaC+4DRkraXNAiYSG4Su4h4OSKGR8SoiBgF3A1MiIh5JcZkZmYVSksEEfE6cCxwI7AIuCoiHpV0mqQJZe3XzMy6ptMbynoiImaTPdEsv+zkDuruXWYsZmbWvoYNFpuZWe/gRGBm1uScCMzMmpwTgZlZk3MiMDNrck4EZmZNzonAzKzJORGYmTW5Um8oMzPr96Zt1oB9vlzT5pwIzMx6osZfyo3griEzsybnRGBm1uTcNWTWaP2gj9n6NicCs0bzl7I1mLuGzMyanBOBmVmTcyIwM2tyTgRmZk3OicDMrMk5EZiZNTknAjOzJudEYGbW5JwIzMyanBOBmVmT8xQT/YXnqzGzbnIi6C/8pWxm3eSuITOzJudEYGbW5EpNBJIOkPSYpMWSTmxn/b9KeljSAklzJY0tMx4zM3ur0hKBpAHAucCBwFhgUjtf9JdHxM4RsQvwfeDssuIxM7P2lXlGsDuwOCKWRMRq4ArgoHyFiHglV9wYiBLjMTOzdpR51dDWwDO58jJgj8pKko4BvgYMAvZpryFJRwJHAowcObLmgZqZNbOGDxZHxLkRsQNwAnBSB3XOj4iWiGgZMWJEfQM0M+vnykwEzwLb5srbpGUduQI4uMR4zMysHWV2Dd0HjJa0PVkCmAhMzleQNDoinkjFTwJP0In58+c/L2lprYOtYjjwfB33V28+vr6rPx8b+PhqbbuOVpSWCCLidUnHAjcCA4CLIuJRSacB8yLiOuBYSfsBa4AVwOcLtFvXviFJ8yKipZ77rCcfX9/Vn48NfHz1VOoUExExG5hdsezk3PuvlLl/MzPrXMMHi83MrLGcCDp3fqMDKJmPr+/qz8cGPr66UYTv4TIza2Y+IzAza3L9LhFIerVgvYmSpnawbitJV9c2srfso0XSDztY95Sk4WXuv2J/hT6zRpF0QdkTEkqaLWnzdpZPk3R8mfs2a7Q++WAaSSLr1lrXg2YOBNr9Io6IPwKH9qDtTkXEPGBemfvIq9Fn1hAR8S912Mcnyt6HWW/VZ84IJI1KU1r/DHgE+HdJ90l6SNKp7dTfW9Kvc+UZkg5P7wXsAtwv6WNpGuwFkh6QNCTt65FUdyNJV0laKOkaSfdIaknrXpV0pqRHJd0iaXdJt0taImlCqjNY0k/TdNsPSBpfGZ+kLSTdlNq5AFBv+8zqRdLGkq6X9KCkRyR9On2mbZ/5FEmPS7pX0k8kzUjLZ0r6kaS70+e/t6SLJC2SNDPX/qT0t3hE0hm55evPwiRNTfuYC+xYz+M3a4Q+kwiS0cB5wHFkk9rtTvaF/iFJf9+FdnYFHoxspPx44Jg0FfZHgb9V1D0aWBERY4F/Bz6UW7cxcFtE7AT8BfgOsD/wT8Bpqc4xQETEzsAk4GJJgyv2cQowN7VzDVDLmfVq9ZnVywHAHyPiAxExDrihbYWkrcj+Bh8GPgK8r2LbocCeZMd6HXAOsBOws6Rd0vZnkE1uuAuwm6Q3TWsi6UNkd8HvAnwC2K3mR2jWy/S1RLA0Iu4G/iG9HgDuJ/tCGN2Fdg4AfpPe3wWcLenLwOYR8XpF3b3I5kEiIh4BHsqtW80bX1QPA3dExJr0flRu+0vT9r8HlgLvrdjH3+fqXE92l3Wt1Oozq5eHgf0lnSHpoxGRfxjz7mSf8Yvpc/55xba/Ssn9YeB/IuLh1BX2KNnfYzfg9ohYnv7Ol5F99nkfBa6JiL+madKvq/kRmvUyfW2MYGX6V8B3I+LHVeq+zpsTXf5X+D8AhwBExPckXU/26+8uSR8HVhWMZ028cf3tOuC11OY6Sb3ls63VZ1YXEfG4pA+S/T2+I+nWLmz+Wvp3Xe59W3lDsqlMzKxCXzsjaHMjcISkTQAkbS1py4o6S4Gxkt6WrgbZN9XdDNgwIl5I5R3SL8czyCbKq+xuuAv451R3LLBzF2P9b+CwtP17ybp9HquocydpQj5JB5J1cdRatz+zekrdN3+NiEuBM4EP5lbfB3xM0tCUaA/pYvP3pu2HK3uC3iTgjoo6dwIHS3q7pCHAp7p1IGZ9SG/51dolEXGTpDHA77JxX14FPgP8OVfnGUlXkQ2SPknWJQJZH/4tuea+mgZw27oQfgO8K7f+PLJ+/YXA71OdfHdFZ84DfiTpYbJf3IdHxGsp7janArMkPQr8Fni6C+0X0sPPrJ52Bs6UtI7sF/xRwFkpvmcl/QfZF/qLZH+Pwn+LiHhO2bOz55CdIV0fEf+/os79kq4EHiT7bO7r+SGZ9W5Nd2dxuirngtRvXqT+AGBgRKyStANZEtkxPX7T6kzSJhHxajojuIZsVttrGh2XWV/WJ88IeqIb16RvBMyRNJDsV+TRTgINNU3Z1OWDgZuAaxscj1mf13RnBGZm9mZ9dbDYzMxqxInAzKzJORGYmTU5JwLrlSQdLCkkVd7X0V7dr0raKFdudybRAu1sLunoXLlms9Cm+ZIe0xvzWtVsUsM0fcYncuUJ6TJZs0I8WGy9UrqWfyuyuZxO6aTuU0BLRDzfw32OAn6d5jiqKUm3A8enWWdr3fbhZMd/bK3btubgMwLrddLdz3sBU8gmgGtbPkDSWWnm0IcktaY5orYiu8R3Tqr3VLp7+HuSjsltP03S8ZI2kXSrpPvTTKQHpSrfA3ZIv9jP1Jtnoe1oFtnDJf1S0g2SnpD0/S4c58z8mYHScyGUzZx6u6SrJf1e0mVKdwFK2k3Sb5XNznpvulP+NODTKe5Pp5jaZmUdJem29HndKmlkbt8/TG0tqeUZivVBEeGXX73qRTYlx4Xp/W+BD6X3RwFXk00RAjAs/fsUMDy3/VPAcLJZZu/ILV8IbEt2/8ymadlwYDHZPSKjgEdy9deXga+T3bwG2TQkT5Pdy3A4sATYLJWXAtu2c0y3k00tsiC9tgBmAofm6rya/t2b7I7pbch+rP2OLDEOSvvaLdXbNB3L4cCMXDvry8CvgM+n90cA16b3M8km7dsAGAssbvTf3a/GvXxGYL3RJNKMr+nfSen9fsCPI80QGxEvVmskIh4Atkx9/R8gm078GbIv/f+Q9BDZneJbA+/oJKZqs8jeGhEvR8QqsmSzXQdtHBYRu6TXC53s796IWBbZ7KkLyJLSjsBzEXFfiuOVeOtsuZX2BC5P7y9Jx9Hm2ohYFxEL6fz4rR9rujuLrXeTNIzseQE7SwpgABCSvtHNJn9O9rS5dwJXpmWHASPIzjTWpDGGnsy0mp/pdC3F/1+tn+1V0gZkv/h72mZX5PdRk4chWd/kMwLrbQ4FLomI7SJiVERsS5YCuqMAAAELSURBVDYB3keBm4EvpXmG2pIGZA8FGtJBe1eSjTMcyhvPL9gM+HNKAuN54xd8tXaKzCLbVU/xxoOOJgADO6n/GPAuSbulOIakz6Ja3L/ljXGWw8iOw+xNnAist5lENplc3i/S8gvI+uYfkvQgaepu4HzghrbB4ryIeJTsS/LZiHguLb4MaEkzwn6ObBZTUnfNXWkw+syKps4DNkjbXEmaRbZnh8pPyKbFfpCsC2dltcqRzXH1aWB62uZmsjOZOWTThy+Q9OmKzVqBL6RusM8CX+lhzNYP+fJRM7Mm5zMCM7Mm50RgZtbknAjMzJqcE4GZWZNzIjAza3JOBGZmTc6JwMysyTkRmJk1uf8FGa2VLHTvG04AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xA2s093vDMr"
      },
      "source": [
        "Analysing the f1 score for 6 hidden layers is better as compared to the rest.\n",
        "That is why I chose 6 hidden layers for the model **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1x_XwggjciC",
        "outputId": "bbd41d92-0d7c-48fa-9681-c094a359d6b1"
      },
      "source": [
        "#Analysing the f1 score as per the number of hidden layers\n",
        "#f1 score for one hidden layer\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "#1 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  #classifier.add(Dense(20, activation='relu'))\n",
        "  #classifier.add(Dense(30, activation='relu'))               #one hidden layers\n",
        "  #classifier.add(Dense(40, activation='relu'))\n",
        "  #classifier.add(Dense(45, activation='relu'))\n",
        "  #classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print('\\n')\n",
        "  print(\"Model : \", name)\n",
        "  model_build_time = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  print('\\n')\n",
        "  print(\"Time to build ANN Model in seconds : %.4f \" % round(time.time()-model_build_time,4))\n",
        "  print('\\n')\n",
        "  start_time = time.time()\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test the ANN Model in seconds : %.4f \" % round(time.time()-start_time,4))\n",
        "  print('\\n')\n",
        "  matrix = confusion_matrix(Y_test, y_pred)\n",
        "  print(\"Time elapsed in seconds: %.4f \" % round(time.time()-model_build_time, 4))\n",
        "  print('\\n')\n",
        "  print(matrix)\n",
        "  print('\\n')\n",
        "  report = classification_report(Y_test, y_pred)\n",
        "  print(report)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model :  ANN WITH 1 HIDDEN LAYER\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 1s 824us/step - loss: 0.1885 - accuracy: 0.7489\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 841us/step - loss: 0.1261 - accuracy: 0.8300\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 859us/step - loss: 0.1192 - accuracy: 0.8311\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 816us/step - loss: 0.1131 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 822us/step - loss: 0.1114 - accuracy: 0.8379\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 875us/step - loss: 0.1117 - accuracy: 0.8383\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 835us/step - loss: 0.1131 - accuracy: 0.8341\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 766us/step - loss: 0.1088 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 787us/step - loss: 0.1101 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 800us/step - loss: 0.1064 - accuracy: 0.8428\n",
            "\n",
            "\n",
            "Time to build ANN Model in seconds : 6.7536 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test the ANN Model in seconds : 0.2148 \n",
            "\n",
            "\n",
            "Time elapsed in seconds: 6.9831 \n",
            "\n",
            "\n",
            "[[9228 2132]\n",
            " [2520 1180]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.81      0.80     11360\n",
            "           1       0.36      0.32      0.34      3700\n",
            "\n",
            "    accuracy                           0.69     15060\n",
            "   macro avg       0.57      0.57      0.57     15060\n",
            "weighted avg       0.68      0.69      0.69     15060\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbFRU3IZnvMo",
        "outputId": "9efffbfd-d287-4bb9-e762-f82e0d6b7845"
      },
      "source": [
        "#Analysing the f1 score as per the number of hidden layers\n",
        "#f1 score for two hidden layer\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "#2 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  #classifier.add(Dense(30, activation='relu'))               #two hidden layers\n",
        "  #classifier.add(Dense(40, activation='relu'))\n",
        "  #classifier.add(Dense(45, activation='relu'))\n",
        "  #classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print('\\n')\n",
        "  print(\"Model : \", name)\n",
        "  model_build_time = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  print('\\n')\n",
        "  print(\"Time to build ANN Model in seconds : %.4f \" % round(time.time()-model_build_time,4))\n",
        "  print('\\n')\n",
        "  start_time = time.time()\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test the ANN Model in seconds : %.4f \" % round(time.time()-start_time,4))\n",
        "  print('\\n')\n",
        "  matrix = confusion_matrix(Y_test, y_pred)\n",
        "  print(\"Time elapsed in seconds: %.4f \" % round(time.time()-model_build_time, 4))\n",
        "  print('\\n')\n",
        "  print(matrix)\n",
        "  print('\\n')\n",
        "  report = classification_report(Y_test, y_pred)\n",
        "  print(report)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model :  ANN WITH 1 HIDDEN LAYER\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 1s 827us/step - loss: 0.1631 - accuracy: 0.7797\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 829us/step - loss: 0.1278 - accuracy: 0.8207\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 843us/step - loss: 0.1223 - accuracy: 0.8255\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 833us/step - loss: 0.1201 - accuracy: 0.8281\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 883us/step - loss: 0.1179 - accuracy: 0.8289\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 887us/step - loss: 0.1142 - accuracy: 0.8360\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 845us/step - loss: 0.1135 - accuracy: 0.8348\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 824us/step - loss: 0.1129 - accuracy: 0.8351\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 829us/step - loss: 0.1093 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 814us/step - loss: 0.1104 - accuracy: 0.8403\n",
            "\n",
            "\n",
            "Time to build ANN Model in seconds : 6.9599 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test the ANN Model in seconds : 0.2358 \n",
            "\n",
            "\n",
            "Time elapsed in seconds: 7.2101 \n",
            "\n",
            "\n",
            "[[8725 2635]\n",
            " [2289 1411]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78     11360\n",
            "           1       0.35      0.38      0.36      3700\n",
            "\n",
            "    accuracy                           0.67     15060\n",
            "   macro avg       0.57      0.57      0.57     15060\n",
            "weighted avg       0.68      0.67      0.68     15060\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GzEFpVToOCA",
        "outputId": "de823702-8751-4edf-888c-d059c6c23c4d"
      },
      "source": [
        "#Analysing the f1 score as per the number of hidden layers\n",
        "#f1 score for three hidden layer\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "#3 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #three hidden layers\n",
        "  #classifier.add(Dense(40, activation='relu'))\n",
        "  #classifier.add(Dense(45, activation='relu'))\n",
        "  #classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print('\\n')\n",
        "  print(\"Model : \", name)\n",
        "  model_build_time = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  print('\\n')\n",
        "  print(\"Time to build ANN Model in seconds : %.4f \" % round(time.time()-model_build_time,4))\n",
        "  print('\\n')\n",
        "  start_time = time.time()\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test the ANN Model in seconds : %.4f \" % round(time.time()-start_time,4))\n",
        "  print('\\n')\n",
        "  matrix = confusion_matrix(Y_test, y_pred)\n",
        "  print(\"Time elapsed in seconds: %.4f \" % round(time.time()-model_build_time, 4))\n",
        "  print('\\n')\n",
        "  print(matrix)\n",
        "  print('\\n')\n",
        "  report = classification_report(Y_test, y_pred)\n",
        "  print(report)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model :  ANN WITH 1 HIDDEN LAYER\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 1s 849us/step - loss: 0.1656 - accuracy: 0.7745\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 848us/step - loss: 0.1155 - accuracy: 0.8302\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 864us/step - loss: 0.1128 - accuracy: 0.8353\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 838us/step - loss: 0.1103 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 929us/step - loss: 0.1090 - accuracy: 0.8434\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 889us/step - loss: 0.1113 - accuracy: 0.8374\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 854us/step - loss: 0.1079 - accuracy: 0.8433\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 910us/step - loss: 0.1086 - accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 908us/step - loss: 0.1091 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 864us/step - loss: 0.1062 - accuracy: 0.8424\n",
            "\n",
            "\n",
            "Time to build ANN Model in seconds : 7.3060 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test the ANN Model in seconds : 0.2433 \n",
            "\n",
            "\n",
            "Time elapsed in seconds: 7.5628 \n",
            "\n",
            "\n",
            "[[9189 2171]\n",
            " [2430 1270]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.81      0.80     11360\n",
            "           1       0.37      0.34      0.36      3700\n",
            "\n",
            "    accuracy                           0.69     15060\n",
            "   macro avg       0.58      0.58      0.58     15060\n",
            "weighted avg       0.69      0.69      0.69     15060\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODEg4xLko5_W",
        "outputId": "7f530641-ca80-44d1-d103-637ac57ea672"
      },
      "source": [
        "#Analysing the f1 score as per the number of hidden layers\n",
        "#f1 score for 4 hidden layer\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "#4 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #four hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  #classifier.add(Dense(45, activation='relu'))\n",
        "  #classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print('\\n')\n",
        "  print(\"Model : \", name)\n",
        "  model_build_time = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  print('\\n')\n",
        "  print(\"Time to build ANN Model in seconds : %.4f \" % round(time.time()-model_build_time,4))\n",
        "  print('\\n')\n",
        "  start_time = time.time()\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test the ANN Model in seconds : %.4f \" % round(time.time()-start_time,4))\n",
        "  print('\\n')\n",
        "  matrix = confusion_matrix(Y_test, y_pred)\n",
        "  print(\"Time elapsed in seconds: %.4f \" % round(time.time()-model_build_time, 4))\n",
        "  print('\\n')\n",
        "  print(matrix)\n",
        "  print('\\n')\n",
        "  report = classification_report(Y_test, y_pred)\n",
        "  print(report)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model :  ANN WITH 1 HIDDEN LAYER\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 1s 881us/step - loss: 0.1580 - accuracy: 0.7828\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 842us/step - loss: 0.1173 - accuracy: 0.8264\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 895us/step - loss: 0.1118 - accuracy: 0.8360\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 837us/step - loss: 0.1102 - accuracy: 0.8388\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 917us/step - loss: 0.1092 - accuracy: 0.8412\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 901us/step - loss: 0.1114 - accuracy: 0.8373\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 912us/step - loss: 0.1077 - accuracy: 0.8443\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 925us/step - loss: 0.1083 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 905us/step - loss: 0.1078 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 905us/step - loss: 0.1076 - accuracy: 0.8376\n",
            "\n",
            "\n",
            "Time to build ANN Model in seconds : 7.5060 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test the ANN Model in seconds : 0.2362 \n",
            "\n",
            "\n",
            "Time elapsed in seconds: 7.7570 \n",
            "\n",
            "\n",
            "[[9940 1420]\n",
            " [2767  933]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.88      0.83     11360\n",
            "           1       0.40      0.25      0.31      3700\n",
            "\n",
            "    accuracy                           0.72     15060\n",
            "   macro avg       0.59      0.56      0.57     15060\n",
            "weighted avg       0.69      0.72      0.70     15060\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bluPNKxSpCpe",
        "outputId": "4146d74c-50c5-405a-893f-2cba2a220610"
      },
      "source": [
        "#Analysing the f1 score as per the number of hidden layers\n",
        "#f1 score for five hidden layer\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "#5 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #five hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  #classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print('\\n')\n",
        "  print(\"Model : \", name)\n",
        "  model_build_time = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  print('\\n')\n",
        "  print(\"Time to build ANN Model in seconds : %.4f \" % round(time.time()-model_build_time,4))\n",
        "  print('\\n')\n",
        "  start_time = time.time()\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test the ANN Model in seconds : %.4f \" % round(time.time()-start_time,4))\n",
        "  print('\\n')\n",
        "  matrix = confusion_matrix(Y_test, y_pred)\n",
        "  print(\"Time elapsed in seconds: %.4f \" % round(time.time()-model_build_time, 4))\n",
        "  print('\\n')\n",
        "  print(matrix)\n",
        "  print('\\n')\n",
        "  report = classification_report(Y_test, y_pred)\n",
        "  print(report)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model :  ANN WITH 1 HIDDEN LAYER\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 959us/step - loss: 0.1452 - accuracy: 0.8004\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1176 - accuracy: 0.8292\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 929us/step - loss: 0.1145 - accuracy: 0.8345\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 998us/step - loss: 0.1122 - accuracy: 0.8369\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 975us/step - loss: 0.1092 - accuracy: 0.8397\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 967us/step - loss: 0.1085 - accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 946us/step - loss: 0.1076 - accuracy: 0.8426\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 962us/step - loss: 0.1080 - accuracy: 0.8414\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 933us/step - loss: 0.1063 - accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 965us/step - loss: 0.1067 - accuracy: 0.8441\n",
            "\n",
            "\n",
            "Time to build ANN Model in seconds : 8.4425 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test the ANN Model in seconds : 0.2389 \n",
            "\n",
            "\n",
            "Time elapsed in seconds: 8.6962 \n",
            "\n",
            "\n",
            "[[9786 1574]\n",
            " [2760  940]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.86      0.82     11360\n",
            "           1       0.37      0.25      0.30      3700\n",
            "\n",
            "    accuracy                           0.71     15060\n",
            "   macro avg       0.58      0.56      0.56     15060\n",
            "weighted avg       0.68      0.71      0.69     15060\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-_u7ReA3Y5A",
        "outputId": "4f640e41-e1a9-4512-f97e-5a25d11ce1c7"
      },
      "source": [
        "# #Analysing the f1 score for 6 hidden layers have already done in the evaluation section also and in this section as well which is better as compared to the rest.\n",
        "#That is why I chose 6 hidden layers for the model\n",
        "#f1 score for six hidden layer\n",
        "optimizers = ['WAME']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "#6 hidden layer used on pca_Embedded dataset\n",
        "def creating_model(optimizer=optimizers, init=inits):\n",
        "  # this functions creates the model \n",
        "  classifier = Sequential() #Sequential module Initialises the ANN\n",
        "  classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "  classifier.add(Dense(10, activation='relu')) \n",
        "  classifier.add(Dense(20, activation='relu'))\n",
        "  classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "  classifier.add(Dense(40, activation='relu'))\n",
        "  classifier.add(Dense(45, activation='relu'))\n",
        "  classifier.add(Dense(50, activation='relu'))\n",
        "  classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "  # Compile model\n",
        "  classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return classifier\n",
        "\n",
        "  # Compile model\n",
        "#using the best hyperparameter for the model\n",
        "models = []\n",
        "models.append(('ANN WITH 1 HIDDEN LAYER', KerasClassifier(build_fn=creating_model, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print('\\n')\n",
        "  print(\"Model : \", name)\n",
        "  model_build_time = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y_train)\n",
        "  print('\\n')\n",
        "  print(\"Time to build ANN Model in seconds : %.4f \" % round(time.time()-model_build_time,4))\n",
        "  print('\\n')\n",
        "  start_time = time.time()\n",
        "  y_pred = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test the ANN Model in seconds : %.4f \" % round(time.time()-start_time,4))\n",
        "  print('\\n')\n",
        "  matrix = confusion_matrix(Y_test, y_pred)\n",
        "  print(\"Time elapsed in seconds: %.4f \" % round(time.time()-model_build_time, 4))\n",
        "  print('\\n')\n",
        "  print(matrix)\n",
        "  print('\\n')\n",
        "  report = classification_report(Y_test, y_pred)\n",
        "  print(report)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model :  ANN WITH 1 HIDDEN LAYER\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 2s 983us/step - loss: 0.1529 - accuracy: 0.7805\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.8316\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 988us/step - loss: 0.1118 - accuracy: 0.8399\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 972us/step - loss: 0.1075 - accuracy: 0.8447\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8393\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 968us/step - loss: 0.1085 - accuracy: 0.8430\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8375\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 976us/step - loss: 0.1074 - accuracy: 0.8431\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8352\n",
            "\n",
            "\n",
            "Time to build ANN Model in seconds : 8.5202 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test the ANN Model in seconds : 0.2748 \n",
            "\n",
            "\n",
            "Time elapsed in seconds: 8.8078 \n",
            "\n",
            "\n",
            "[[10165  1195]\n",
            " [ 2798   902]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.89      0.84     11360\n",
            "           1       0.43      0.24      0.31      3700\n",
            "\n",
            "    accuracy                           0.73     15060\n",
            "   macro avg       0.61      0.57      0.57     15060\n",
            "weighted avg       0.70      0.73      0.71     15060\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ywB41GxlQ3qx",
        "outputId": "806a2c50-3b9f-421a-e2f8-58dbcc15e9e4"
      },
      "source": [
        "# #5 Practice to improve the graph of MSE Graph x axis to align for Activation for our  model to see Accuracy and MSE of the Model\n",
        "\n",
        "# #6 hidden layer used on pca_Embedded dataset\n",
        "# def creating_model1(optimizer=optimizers, init=inits):# input and hidden uses relu and output layer uses sigmoid\n",
        "#   # this functions creates the model \n",
        "#   classifier = Sequential() #Sequential module Initialises the ANN\n",
        "#   classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "#   classifier.add(Dense(10, activation='relu')) \n",
        "#   classifier.add(Dense(20, activation='relu'))\n",
        "#   classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "#   classifier.add(Dense(40, activation='relu'))\n",
        "#   classifier.add(Dense(45, activation='relu'))\n",
        "#   classifier.add(Dense(50, activation='relu'))\n",
        "#   classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "#   # Compile model\n",
        "#   classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "#   return classifier\n",
        "\n",
        "#   def creating_model2(optimizer=optimizers, init=inits):\n",
        "#     classifier = Sequential() #Sequential module Initialises the ANN\n",
        "#     classifier.add(Dense(12, input_dim=6, activation='relu')) # one input layer. Dense module builds the layer for ANN\n",
        "#     classifier.add(Dense(10, activation='relu')) \n",
        "#     classifier.add(Dense(20, activation='relu'))\n",
        "#     classifier.add(Dense(30, activation='relu'))               #six hidden layers\n",
        "#     classifier.add(Dense(40, activation='relu'))\n",
        "#     classifier.add(Dense(45, activation='relu'))\n",
        "#     classifier.add(Dense(50, activation='relu'))\n",
        "#     classifier.add(Dense(1, activation='relu'))             #one output layer\n",
        "#   # Compile model\n",
        "#     classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "#     return classifier\n",
        "\n",
        "# def creating_model3(optimizer=optimizers, init=inits):\n",
        "#   # this functions creates the model \n",
        "#   classifier = Sequential() #Sequential module Initialises the ANN\n",
        "#   classifier.add(Dense(12, input_dim=6, activation='sigmoid')) # one input layer. Dense module builds the layer for ANN\n",
        "#   classifier.add(Dense(10, activation='sigmoid')) \n",
        "#   classifier.add(Dense(20, activation='sigmoid'))\n",
        "#   classifier.add(Dense(30, activation='sigmoid'))               #six hidden layers\n",
        "#   classifier.add(Dense(40, activation='sigmoid'))\n",
        "#   classifier.add(Dense(45, activation='sigmoid'))\n",
        "#   classifier.add(Dense(50, activation='sigmoid'))\n",
        "#   classifier.add(Dense(1, activation='sigmoid'))             #one output layer\n",
        "#   # Compile model\n",
        "#   classifier.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "#   return classifier\n",
        "\n",
        "\n",
        "#   # Compile model\n",
        "# #using the best hyperparameter for the model\n",
        "# models = []\n",
        "# models.append(('relu/sigmoid', KerasClassifier(build_fn=creating_model1, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "# models.append(('relu', KerasClassifier(build_fn=creating_model2, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "# models.append(('sigmoid', KerasClassifier(build_fn=creating_model3, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME(learning_rate=0.0001)')))\n",
        "\n",
        "\n",
        "# # evaluate each model in turn\n",
        "# results = []\n",
        "# names = []\n",
        "# scoring = 'accuracy'\n",
        "# for name, model in models:\n",
        "#   kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "#   cv_results = cross_val_score(model, pca_embedded_train_features, Y_train, cv=kfold, scoring=scoring)\n",
        "#   results.append(cv_results)\n",
        "#   names.append(name)\n",
        "#   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "#   print(msg)\n",
        "# # boxplot algorithm comparison\n",
        "# fig = pyplot.figure()\n",
        "# fig.suptitle('Mean Accuracy Rate')\n",
        "# ax = fig.add_subplot(111)\n",
        "# pyplot.boxplot(results)\n",
        "# ax.set_xticklabels(names)\n",
        "# plt.xlabel('Activation Function')\n",
        "# plt.ylabel('Generalization Accuracy')\n",
        "# pyplot.show()\n",
        "# print('\\n')\n",
        "# print('\\n')\n",
        "# # again evaluate each model in turn\n",
        "# results = []\n",
        "# names = []\n",
        "# scoring = 'accuracy'\n",
        "# for name, model in models:\n",
        "#   kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "#   cv_results = cross_val_score(model, pca_embedded_train_features, cv=kfold, scoring=scoring)\n",
        "#   #results.append(cv_results)\n",
        "#   model= tf.stack(model.fit(cv_results, Y_train))\n",
        "#   #model.fit(pca_embedded_train_features, Y_train)\n",
        "#   y_pred = model.predict(pca_embedded_test_features)\n",
        "#   mse = mean_squared_error(Y_test,  y_pred)\n",
        "#   results.append(mse)\n",
        "#   names.append(name)\n",
        "#   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "#   print(msg)\n",
        "# # boxplot algorithm comparison\n",
        "# fig = pyplot.figure()\n",
        "# fig.suptitle('Mean Squarred Error')\n",
        "# ax = fig.add_subplot(111)\n",
        "# pyplot.boxplot(results)\n",
        "# ax.set_xticklabels(names)\n",
        "# plt.xlabel('Activation Function')\n",
        "# plt.ylabel('Generalization Error Rate')\n",
        "# pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1462 - accuracy: 0.7975\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.8348\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8379\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8366\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8426\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 986us/step - loss: 0.1083 - accuracy: 0.8439\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8409\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8410\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.8384\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1514 - accuracy: 0.7852\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.8321\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8330\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8377\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8357\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8386\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8409\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8388\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.8435\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8423\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1493 - accuracy: 0.7967\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8402\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8377\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1103 - accuracy: 0.8406\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8377\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 997us/step - loss: 0.1095 - accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.8377\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 971us/step - loss: 0.1093 - accuracy: 0.8390\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1076 - accuracy: 0.8452\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.8449\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1489 - accuracy: 0.7886\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 976us/step - loss: 0.1162 - accuracy: 0.8315\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8327\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8405\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8382\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8417\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1074 - accuracy: 0.8429\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8429\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1518 - accuracy: 0.7898\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.8363\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8327\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8397\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.8412\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8417\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8417\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1504 - accuracy: 0.7903\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8340\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8312\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.8388\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8413\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8402\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1079 - accuracy: 0.8434\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1086 - accuracy: 0.8416\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.8429\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1461 - accuracy: 0.7958\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.8285\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8388\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8350\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8357\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1096 - accuracy: 0.8412\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1085 - accuracy: 0.8424\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.8407\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8381\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1088 - accuracy: 0.8415\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1694 - accuracy: 0.7688\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1179 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8342\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.8314\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8360\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.8346\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.8362\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.8382\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8353\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8409\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1462 - accuracy: 0.7881\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8303\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8358\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8401\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8372\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8398\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.8433\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1081 - accuracy: 0.8423\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8416\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1486 - accuracy: 0.7938\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8341\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8349\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8367\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8360\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8353\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8367\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8391\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8368\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.8411\n",
            "relu/sigmoid: 0.840793 (0.005396)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 878us/step - loss: 0.1658 - accuracy: 0.7796\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 924us/step - loss: 0.1197 - accuracy: 0.8286\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 887us/step - loss: 0.1118 - accuracy: 0.8355\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 877us/step - loss: 0.1129 - accuracy: 0.8357\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 918us/step - loss: 0.1124 - accuracy: 0.8345\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 868us/step - loss: 0.1112 - accuracy: 0.8365\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 879us/step - loss: 0.1115 - accuracy: 0.8365\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 920us/step - loss: 0.1097 - accuracy: 0.8380\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 918us/step - loss: 0.1097 - accuracy: 0.8396\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1077 - accuracy: 0.8416\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 907us/step - loss: 0.1764 - accuracy: 0.7620\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 861us/step - loss: 0.1152 - accuracy: 0.8319\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 874us/step - loss: 0.1130 - accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 963us/step - loss: 0.1122 - accuracy: 0.8375\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 925us/step - loss: 0.1115 - accuracy: 0.8367\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 994us/step - loss: 0.1099 - accuracy: 0.8416\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 939us/step - loss: 0.1089 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 922us/step - loss: 0.1107 - accuracy: 0.8389\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 938us/step - loss: 0.1076 - accuracy: 0.8445\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 912us/step - loss: 0.1080 - accuracy: 0.8441\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 877us/step - loss: 0.1647 - accuracy: 0.7763\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 918us/step - loss: 0.1120 - accuracy: 0.8369\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 942us/step - loss: 0.1115 - accuracy: 0.8372\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8378\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 918us/step - loss: 0.1130 - accuracy: 0.8337\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 955us/step - loss: 0.1120 - accuracy: 0.8361\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 962us/step - loss: 0.1114 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 948us/step - loss: 0.1095 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 958us/step - loss: 0.1108 - accuracy: 0.8388\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 918us/step - loss: 0.1102 - accuracy: 0.8390\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 961us/step - loss: 0.1693 - accuracy: 0.7663\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 852us/step - loss: 0.1166 - accuracy: 0.8317\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 890us/step - loss: 0.1156 - accuracy: 0.8294\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.8357\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 961us/step - loss: 0.1129 - accuracy: 0.8343\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 942us/step - loss: 0.1118 - accuracy: 0.8340\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8344\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.8365\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8359\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 914us/step - loss: 0.1762 - accuracy: 0.7652\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 926us/step - loss: 0.1151 - accuracy: 0.8344\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 922us/step - loss: 0.1116 - accuracy: 0.8358\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8337\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 951us/step - loss: 0.1122 - accuracy: 0.8340\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 953us/step - loss: 0.1108 - accuracy: 0.8391\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 982us/step - loss: 0.1115 - accuracy: 0.8352\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 923us/step - loss: 0.1122 - accuracy: 0.8349\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 884us/step - loss: 0.1094 - accuracy: 0.8401\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 887us/step - loss: 0.1096 - accuracy: 0.8394\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 950us/step - loss: 0.1708 - accuracy: 0.7663\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 979us/step - loss: 0.1151 - accuracy: 0.8345\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 919us/step - loss: 0.1137 - accuracy: 0.8352\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 902us/step - loss: 0.1127 - accuracy: 0.8358\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 861us/step - loss: 0.1131 - accuracy: 0.8314\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 940us/step - loss: 0.1116 - accuracy: 0.8336\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 943us/step - loss: 0.1114 - accuracy: 0.8388\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 913us/step - loss: 0.1108 - accuracy: 0.8378\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 950us/step - loss: 0.1092 - accuracy: 0.8417\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 954us/step - loss: 0.1114 - accuracy: 0.8366\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 939us/step - loss: 0.1682 - accuracy: 0.7736\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.8329\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 875us/step - loss: 0.1121 - accuracy: 0.8343\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 945us/step - loss: 0.1143 - accuracy: 0.8317\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 915us/step - loss: 0.1143 - accuracy: 0.8323\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 940us/step - loss: 0.1117 - accuracy: 0.8357\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 930us/step - loss: 0.1109 - accuracy: 0.8373\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 944us/step - loss: 0.1110 - accuracy: 0.8378\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 930us/step - loss: 0.1115 - accuracy: 0.8368\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 876us/step - loss: 0.1112 - accuracy: 0.8378\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 879us/step - loss: 0.1726 - accuracy: 0.7786\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 883us/step - loss: 0.1154 - accuracy: 0.8339\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 886us/step - loss: 0.1134 - accuracy: 0.8351\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8306\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 935us/step - loss: 0.1114 - accuracy: 0.8365\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 924us/step - loss: 0.1109 - accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 916us/step - loss: 0.1106 - accuracy: 0.8381\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 872us/step - loss: 0.1096 - accuracy: 0.8407\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 886us/step - loss: 0.1079 - accuracy: 0.8428\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 911us/step - loss: 0.1102 - accuracy: 0.8366\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 806us/step - loss: 0.1781 - accuracy: 0.7439\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 835us/step - loss: 0.1153 - accuracy: 0.8344\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 905us/step - loss: 0.1142 - accuracy: 0.8352\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 869us/step - loss: 0.1129 - accuracy: 0.8373\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 866us/step - loss: 0.1133 - accuracy: 0.8354\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 886us/step - loss: 0.1125 - accuracy: 0.8362\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 878us/step - loss: 0.1117 - accuracy: 0.8367\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 912us/step - loss: 0.1136 - accuracy: 0.8352\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 919us/step - loss: 0.1147 - accuracy: 0.8329\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 919us/step - loss: 0.1105 - accuracy: 0.8368\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 813us/step - loss: 0.1710 - accuracy: 0.7719\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 842us/step - loss: 0.1198 - accuracy: 0.8257\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 863us/step - loss: 0.1131 - accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 857us/step - loss: 0.1125 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 930us/step - loss: 0.1123 - accuracy: 0.8338\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 927us/step - loss: 0.1124 - accuracy: 0.8359\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 890us/step - loss: 0.1115 - accuracy: 0.8342\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 901us/step - loss: 0.1103 - accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 876us/step - loss: 0.1109 - accuracy: 0.8360\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 850us/step - loss: 0.1126 - accuracy: 0.8349\n",
            "relu: 0.839135 (0.005141)\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1888 - accuracy: 0.7484\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1886 - accuracy: 0.7482\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1577 - accuracy: 0.7721\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1397 - accuracy: 0.7985\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1376 - accuracy: 0.8016\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8028\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1376 - accuracy: 0.8054\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1373 - accuracy: 0.8066\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1384 - accuracy: 0.8028\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1377 - accuracy: 0.8056\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1916 - accuracy: 0.7398\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1865 - accuracy: 0.7529\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1783 - accuracy: 0.7575\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1400 - accuracy: 0.7961\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1390 - accuracy: 0.8037\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1373 - accuracy: 0.8049\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8020\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1373 - accuracy: 0.8065\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1381 - accuracy: 0.8033\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1383 - accuracy: 0.8050\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1888 - accuracy: 0.7493\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1877 - accuracy: 0.7505\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1518 - accuracy: 0.7794\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1395 - accuracy: 0.8024\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8055\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1397 - accuracy: 0.8019\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1379 - accuracy: 0.8032\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1404 - accuracy: 0.7999\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1383 - accuracy: 0.8028\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8042\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1889 - accuracy: 0.7484\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1880 - accuracy: 0.7500\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1603 - accuracy: 0.7690\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1402 - accuracy: 0.7997\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1382 - accuracy: 0.8025\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8028\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8040\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1378 - accuracy: 0.8057\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1392 - accuracy: 0.8048\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8022\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1880 - accuracy: 0.7501\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1879 - accuracy: 0.7502\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1795 - accuracy: 0.7595\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1416 - accuracy: 0.7981\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1399 - accuracy: 0.8033\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1400 - accuracy: 0.8010\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1392 - accuracy: 0.8036\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1406 - accuracy: 0.8028\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1402 - accuracy: 0.8020\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1383 - accuracy: 0.8031\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1882 - accuracy: 0.7536\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1857 - accuracy: 0.7545\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1837 - accuracy: 0.7530\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1423 - accuracy: 0.7954\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1391 - accuracy: 0.8023\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1400 - accuracy: 0.8031\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1368 - accuracy: 0.8101\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1366 - accuracy: 0.8073\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1371 - accuracy: 0.8073\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1405 - accuracy: 0.7995\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1934 - accuracy: 0.7351\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1880 - accuracy: 0.7500\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1645 - accuracy: 0.7662\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1371 - accuracy: 0.8049\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8040\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8066\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1422 - accuracy: 0.7960\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1390 - accuracy: 0.8034\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1377 - accuracy: 0.8067\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1398 - accuracy: 0.8024\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1928 - accuracy: 0.7367\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1882 - accuracy: 0.7495\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1695 - accuracy: 0.7643\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1394 - accuracy: 0.8003\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1379 - accuracy: 0.8041\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1396 - accuracy: 0.7994\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1384 - accuracy: 0.8069\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1395 - accuracy: 0.8015\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8023\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1380 - accuracy: 0.8051\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 2s 1ms/step - loss: 0.1861 - accuracy: 0.7541\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1860 - accuracy: 0.7536\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1624 - accuracy: 0.7691\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1398 - accuracy: 0.8010\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8031\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1395 - accuracy: 0.8035\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1371 - accuracy: 0.8066\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1408 - accuracy: 0.7986\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1388 - accuracy: 0.8040\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1404 - accuracy: 0.8008\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1911 - accuracy: 0.7452\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1859 - accuracy: 0.7540\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1619 - accuracy: 0.7678\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1394 - accuracy: 0.8009\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8020\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1410 - accuracy: 0.7976\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1382 - accuracy: 0.8050\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.8065\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1380 - accuracy: 0.8061\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8056\n",
            "sigmoid: 0.802831 (0.006296)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "TypeError: fit() missing 1 required positional argument: 'y'\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-c609495a7355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_embedded_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;31m#results.append(cv_results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;31m#model.fit(pca_embedded_train_features, Y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_embedded_test_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1527\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[1;32m   1528\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 10\n  y sizes: 30162\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxSU4YzbC8Fk"
      },
      "source": [
        "#Not needed lamba function to convert the target variable 'class'. As label encoder already assigns more than 50K as 1 and less than 50K as 0 and ignores the dot.\n",
        "# #Step1c converting the target variable class into 2 groups of zeros and ones. More than 50K as 1 and less than 50 K as 0\n",
        "# dataset['class'] = dataset['class'].apply(lambda x: str(x).replace('<=50K.', '0') if '<=50K.' in str(x) else str(x))\n",
        "# dataset['class'] = dataset['class'].apply(lambda x: str(x).replace('<=50K', '0') if '<=50K' in str(x) else str(x))\n",
        "# dataset['class'] = dataset['class'].apply(lambda x: str(x).replace('>50K.', '1') if '>50K.' in str(x) else str(x))\n",
        "# dataset['class'] = dataset['class'].apply(lambda x: str(x).replace('>50K', '1') if '>50K' in str(x) else str(x))\n",
        "# dataset['class'] = dataset['class'].apply(lambda x: int(x))\n",
        "# dataset\n",
        "#No need to drop anything now\n",
        "# #Step1d assigning the target variable to Y and then dropping it from training data\n",
        "# Y_train = dataset['class']\n",
        "# dataset.drop('class', axis = 1, inplace = True)\n",
        "# #Step 1e SELECTING ALL ROWS AND COLUMNS IN X and putting it in Z dataset to visualise\n",
        "# X_train = dataset.iloc[:, :].values\n",
        "# Z_train = pd.DataFrame(X_train)\n",
        "# X_train\n",
        "# #Step 1f --to do label encoding\n",
        "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "# labelencoder_X = LabelEncoder()\n",
        "# #for i in range(0, 13):\n",
        "#  # if isinstance(X[:, i], str):\n",
        "# X_train[:,1] = labelencoder_X.fit_transform(X_train[:,1])\n",
        "# X_train[:,3] = labelencoder_X.fit_transform(X_train[:,3])\n",
        "# X_train[:,5] = labelencoder_X.fit_transform(X_train[:,5])\n",
        "# X_train[:,6] = labelencoder_X.fit_transform(X_train[:,6])\n",
        "# X_train[:,7] = labelencoder_X.fit_transform(X_train[:,7])\n",
        "# X_train[:,8] = labelencoder_X.fit_transform(X_train[:,8])\n",
        "# X_train[:,9] = labelencoder_X.fit_transform(X_train[:,9])\n",
        "# X_train[:,13] = labelencoder_X.fit_transform(X_train[:,13])\n",
        "# Z_traindata = pd.DataFrame(X_train)\n",
        "# Z_traindata#this is the tests data ready for scaling\n",
        "##-------------------------\n",
        "#Step 1g the categorical columns are convered to 0s and 1s by using one hot encoder to the 8 categorical columns\n",
        "# #onehotvector gives a feature for every categorical option. No Need to do Feature extraction and Feature selection when using one hot encoder but Scaling is a must.\n",
        "# #onehotencoder = OneHotEncoder(categories = [1])\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# ct = ColumnTransformer(\n",
        "#     [('oh_enc', OneHotEncoder(sparse=False), [1, 3, 5, 6, 7, 8, 9, 13]),],  # the column numbers I want to apply this to\n",
        "#     remainder='passthrough'  # This leaves the rest of my columns in place\n",
        "# )\n",
        "# X_traindata = ct.fit_transform(X_train)\n",
        "# X_traindata\n",
        "\n",
        "# #print(ct.fit_transform(X)) # Notice the output is a string\n",
        "# Z_train = pd.DataFrame(X_traindata)\n",
        "# Z_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "iranONUXXG9a",
        "outputId": "fd93046d-54fd-422e-934b-44f93086dda3"
      },
      "source": [
        "# #step 1 h\n",
        "# #this is just to test step 1 h if this works that means after applying the one hot encoding on the categorical variable we then need to scale all the features to prepare it for the model so that ML algorithm can be applied to it\n",
        "# #Now Scaling the categorical and the continuous variables by using MinMaxScalar\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from numpy import set_printoptions\n",
        "# #rescaled data between 0 and 1\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# rescaledX = scaler.fit_transform(Z_train)\n",
        "# #summarize transformed data\n",
        "# set_printoptions(precision=3)\n",
        "# print(rescaledX)\n",
        "# M = pd.DataFrame(rescaledX) #M shows that the dataset has been scaled\n",
        "# M\n",
        "\n",
        "# #so rescaledX is the final training dataset that I will use"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.    0.    0.    ... 0.022 0.    0.398]\n",
            " [0.    0.    0.    ... 0.    0.    0.122]\n",
            " [0.    0.    0.    ... 0.    0.    0.398]\n",
            " ...\n",
            " [0.    0.    0.    ... 0.    0.    0.398]\n",
            " [0.    0.    0.    ... 0.    0.    0.194]\n",
            " [0.    0.    0.    ... 0.15  0.    0.398]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301370</td>\n",
              "      <td>0.044302</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.021740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.452055</td>\n",
              "      <td>0.048238</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.122449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.138113</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493151</td>\n",
              "      <td>0.151068</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.221488</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.166404</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.377551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315068</td>\n",
              "      <td>0.096500</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.561644</td>\n",
              "      <td>0.094827</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.068493</td>\n",
              "      <td>0.128499</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.193878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.479452</td>\n",
              "      <td>0.187203</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.150242</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 108 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2    3    4    ...       103       104       105  106       107\n",
              "0      0.0  0.0  0.0  0.0  0.0  ...  0.044302  0.800000  0.021740  0.0  0.397959\n",
              "1      0.0  0.0  0.0  0.0  0.0  ...  0.048238  0.800000  0.000000  0.0  0.122449\n",
              "2      0.0  0.0  0.0  0.0  1.0  ...  0.138113  0.533333  0.000000  0.0  0.397959\n",
              "3      0.0  0.0  0.0  0.0  1.0  ...  0.151068  0.400000  0.000000  0.0  0.397959\n",
              "4      0.0  0.0  0.0  0.0  1.0  ...  0.221488  0.800000  0.000000  0.0  0.397959\n",
              "...    ...  ...  ...  ...  ...  ...       ...       ...       ...  ...       ...\n",
              "32556  0.0  0.0  0.0  0.0  1.0  ...  0.166404  0.733333  0.000000  0.0  0.377551\n",
              "32557  0.0  0.0  0.0  0.0  1.0  ...  0.096500  0.533333  0.000000  0.0  0.397959\n",
              "32558  0.0  0.0  0.0  0.0  1.0  ...  0.094827  0.533333  0.000000  0.0  0.397959\n",
              "32559  0.0  0.0  0.0  0.0  1.0  ...  0.128499  0.533333  0.000000  0.0  0.193878\n",
              "32560  0.0  0.0  0.0  0.0  0.0  ...  0.187203  0.533333  0.150242  0.0  0.397959\n",
              "\n",
              "[32561 rows x 108 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "U-ON0vwqID_j",
        "outputId": "55fa7d5b-4d8f-4c49-db98-7ed21c4d7611"
      },
      "source": [
        "# #Step 1i -- check to see which dataset works Note this dosent work\n",
        "\n",
        "# # Now dropping the the last six columns from X_traindata as these last 6 columns are continuos data that needs to be scaled so I stored it first in L to scale them and then will combine with X again\n",
        "# #so dropping the unscaled last 6 col from X\n",
        "# #dataset.drop('class', axis = 1, inplace = True)\n",
        "# #all rows, all columns except the last six\n",
        "# L = X_traindata[:, -6 :]# so L has all the 6 continuous variables\n",
        "# O = pd.DataFrame(L)# to visualise L that the 6 continuos feature are assigned to L \n",
        "# O\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>77516</td>\n",
              "      <td>13</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>83311</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>215646</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>234721</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>338409</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>27</td>\n",
              "      <td>257302</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>40</td>\n",
              "      <td>154374</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>58</td>\n",
              "      <td>151910</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>22</td>\n",
              "      <td>201490</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>52</td>\n",
              "      <td>287927</td>\n",
              "      <td>9</td>\n",
              "      <td>15024</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0       1   2      3  4   5\n",
              "0      39   77516  13   2174  0  40\n",
              "1      50   83311  13      0  0  13\n",
              "2      38  215646   9      0  0  40\n",
              "3      53  234721   7      0  0  40\n",
              "4      28  338409  13      0  0  40\n",
              "...    ..     ...  ..    ... ..  ..\n",
              "32556  27  257302  12      0  0  38\n",
              "32557  40  154374   9      0  0  40\n",
              "32558  58  151910   9      0  0  40\n",
              "32559  22  201490   9      0  0  20\n",
              "32560  52  287927   9  15024  0  40\n",
              "\n",
              "[32561 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "5HyCR3B-L96b",
        "outputId": "26d88e78-6747-4f8d-92a6-5b4731e13263"
      },
      "source": [
        "# #Now dropping the 6 continuous variable from Ztrain --- Note this dosent work\n",
        "\n",
        "# #X_traindata.drop[:, -6 :]\n",
        "# Z_train2 = Z_train.drop([102,103,104,105,106,107], axis=1)\n",
        "\n",
        "# Z_train2 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0   1   2   3   4   5   6   7   8    ... 93  94  95  96  97  98  99  100 101\n",
              "0       0   0   0   0   0   0   0   1   0  ...   0   0   0   0   0   0   1   0   0\n",
              "1       0   0   0   0   0   0   1   0   0  ...   0   0   0   0   0   0   1   0   0\n",
              "2       0   0   0   0   1   0   0   0   0  ...   0   0   0   0   0   0   1   0   0\n",
              "3       0   0   0   0   1   0   0   0   0  ...   0   0   0   0   0   0   1   0   0\n",
              "4       0   0   0   0   1   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
              "32556   0   0   0   0   1   0   0   0   0  ...   0   0   0   0   0   0   1   0   0\n",
              "32557   0   0   0   0   1   0   0   0   0  ...   0   0   0   0   0   0   1   0   0\n",
              "32558   0   0   0   0   1   0   0   0   0  ...   0   0   0   0   0   0   1   0   0\n",
              "32559   0   0   0   0   1   0   0   0   0  ...   0   0   0   0   0   0   1   0   0\n",
              "32560   0   0   0   0   0   1   0   0   0  ...   0   0   0   0   0   0   1   0   0\n",
              "\n",
              "[32561 rows x 102 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "N0_LM9MGN1fF",
        "outputId": "53f537c1-2f37-481c-c954-00b4b2b35a16"
      },
      "source": [
        "# #Step 1 j --- Note this dosent work\n",
        "\n",
        "# #Now Scaling the continuous variables L by using MinMaxScalar\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from numpy import set_printoptions\n",
        "# #rescaled data between 0 and 1\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# rescaledX = scaler.fit_transform(L)\n",
        "# #summarize transformed data\n",
        "# set_printoptions(precision=3)\n",
        "# print(rescaledX)\n",
        "# M = pd.DataFrame(rescaledX) #M shows that L(6 continuous variables)has been scaled\n",
        "# M\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.301 0.044 0.8   0.022 0.    0.398]\n",
            " [0.452 0.048 0.8   0.    0.    0.122]\n",
            " [0.288 0.138 0.533 0.    0.    0.398]\n",
            " ...\n",
            " [0.562 0.095 0.533 0.    0.    0.398]\n",
            " [0.068 0.128 0.533 0.    0.    0.194]\n",
            " [0.479 0.187 0.533 0.15  0.    0.398]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.301370</td>\n",
              "      <td>0.044302</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.021740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.452055</td>\n",
              "      <td>0.048238</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.122449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.138113</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.493151</td>\n",
              "      <td>0.151068</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.221488</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.166404</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.377551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>0.315068</td>\n",
              "      <td>0.096500</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>0.561644</td>\n",
              "      <td>0.094827</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>0.068493</td>\n",
              "      <td>0.128499</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.193878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>0.479452</td>\n",
              "      <td>0.187203</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.150242</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2         3    4         5\n",
              "0      0.301370  0.044302  0.800000  0.021740  0.0  0.397959\n",
              "1      0.452055  0.048238  0.800000  0.000000  0.0  0.122449\n",
              "2      0.287671  0.138113  0.533333  0.000000  0.0  0.397959\n",
              "3      0.493151  0.151068  0.400000  0.000000  0.0  0.397959\n",
              "4      0.150685  0.221488  0.800000  0.000000  0.0  0.397959\n",
              "...         ...       ...       ...       ...  ...       ...\n",
              "32556  0.136986  0.166404  0.733333  0.000000  0.0  0.377551\n",
              "32557  0.315068  0.096500  0.533333  0.000000  0.0  0.397959\n",
              "32558  0.561644  0.094827  0.533333  0.000000  0.0  0.397959\n",
              "32559  0.068493  0.128499  0.533333  0.000000  0.0  0.193878\n",
              "32560  0.479452  0.187203  0.533333  0.150242  0.0  0.397959\n",
              "\n",
              "[32561 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "P-eORi-UOgnU",
        "outputId": "89e744d6-55c1-4e77-8e96-26644976591a"
      },
      "source": [
        "# #Step 1 k Combine the onehotvector Z_train2(8 categorical features) with the rescaledX(6 continuous features) and get the combined dataset for the model\n",
        "#  #--Note this dosent work\n",
        "\n",
        "# combined_dataset = np.hstack((Z_train2 , rescaledX))\n",
        "# combined_dataset.shape # combined_dataset is the final dataset for the train data but fit is failing for this type of dataset\n",
        "# CD = pd.DataFrame(combined_dataset)\n",
        "# CD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.30137</td>\n",
              "      <td>0.0443019</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0217402</td>\n",
              "      <td>0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.452055</td>\n",
              "      <td>0.0482376</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.122449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.138113</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.493151</td>\n",
              "      <td>0.151068</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.221488</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.166404</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.377551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.315068</td>\n",
              "      <td>0.0965003</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.561644</td>\n",
              "      <td>0.0948269</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0684932</td>\n",
              "      <td>0.128499</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.193878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.479452</td>\n",
              "      <td>0.187203</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.150242</td>\n",
              "      <td>0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 108 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0   1   2   3   4    ...        103       104        105 106       107\n",
              "0       0   0   0   0   0  ...  0.0443019       0.8  0.0217402   0  0.397959\n",
              "1       0   0   0   0   0  ...  0.0482376       0.8          0   0  0.122449\n",
              "2       0   0   0   0   1  ...   0.138113  0.533333          0   0  0.397959\n",
              "3       0   0   0   0   1  ...   0.151068       0.4          0   0  0.397959\n",
              "4       0   0   0   0   1  ...   0.221488       0.8          0   0  0.397959\n",
              "...    ..  ..  ..  ..  ..  ...        ...       ...        ...  ..       ...\n",
              "32556   0   0   0   0   1  ...   0.166404  0.733333          0   0  0.377551\n",
              "32557   0   0   0   0   1  ...  0.0965003  0.533333          0   0  0.397959\n",
              "32558   0   0   0   0   1  ...  0.0948269  0.533333          0   0  0.397959\n",
              "32559   0   0   0   0   1  ...   0.128499  0.533333          0   0  0.193878\n",
              "32560   0   0   0   0   0  ...   0.187203  0.533333   0.150242   0  0.397959\n",
              "\n",
              "[32561 rows x 108 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cq0bNZNqTyeY",
        "outputId": "48d1c531-ee11-4a37-df77-fcf408dcae2e"
      },
      "source": [
        "\n",
        "# #Buiding neural network\n",
        "# optimizers = ['WAME1()', 'WAME2()', 'WAME3()', 'WAME4()']#SO WAME IS THE BEST OPTIMISER\n",
        "# inits = ['uniform', 'glorot_uniform'] \n",
        "# epochs = [5, 7, 10]\n",
        "# batches = [20, 30, 40]\n",
        "\n",
        "\n",
        "# def create_model(optimizer=optimizers, init=inits):\n",
        "#   # create model\n",
        "#   mlp_model = Sequential()\n",
        "#   mlp_model.add(Dense(12, input_dim=108, activation='relu')) \n",
        "#   mlp_model.add(Dense(10, activation='relu')) \n",
        "#   mlp_model.add(Dense(20, activation='relu'))\n",
        "#   mlp_model.add(Dense(30, activation='relu'))\n",
        "#   mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        " \n",
        "#   mlp_model.compile(loss=tf.keras.losses.MeanSquaredError(),  metrics=[\"accuracy\"])\n",
        "#   return mlp_model\n",
        "\n",
        "# # create model\n",
        "\n",
        "# model_keras = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# # grid search epochs, batch size and optimizer\n",
        "# param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "# grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "# grid_result = grid.fit(Z_train, Y_train)# so i decided after app lying the one hot encoder, do the rescaling of all 108 features and then see if its work\n",
        "# #print(\"Best: %f using %s\" % (grid_result.bestscore, grid_result.bestparams))\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))#checking the best hyperparameter along with the best optimiser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m           \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    479\u001b[0m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0;32m--> 480\u001b[0;31m                   (element, type(element).__name__))\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for       0   1   2   3   4   5   6   7    ... 100 101 102     103 104    105 106 107\n0       0   0   0   0   0   0   0   1  ...   0   0  39   77516  13   2174   0  40\n1       0   0   0   0   0   0   1   0  ...   0   0  50   83311  13      0   0  13\n2       0   0   0   0   1   0   0   0  ...   0   0  38  215646   9      0   0  40\n3       0   0   0   0   1   0   0   0  ...   0   0  53  234721   7      0   0  40\n4       0   0   0   0   1   0   0   0  ...   0   0  28  338409  13      0   0  40\n...    ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..     ...  ..    ...  ..  ..\n32556   0   0   0   0   1   0   0   0  ...   0   0  27  257302  12      0   0  38\n32557   0   0   0   0   1   0   0   0  ...   0   0  40  154374   9      0   0  40\n32558   0   0   0   0   1   0   0   0  ...   0   0  58  151910   9      0   0  40\n32559   0   0   0   0   1   0   0   0  ...   0   0  22  201490   9      0   0  20\n32560   0   0   0   0   0   1   0   0  ...   0   0  52  287927   9  15024   0  40\n\n[32561 rows x 108 columns] with type DataFrame",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-5a41d6a6c763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_keras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;31m#print(\"Best: %f using %s\" % (grid_result.bestscore, grid_result.bestparams))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#checking the best hyperparameter along with the best optimiser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    379\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[1;32m    380\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     ))\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    611\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \"\"\"\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3135\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3136\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3137\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3138\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         normalized_components.append(\n\u001b[0;32m--> 111\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    112\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aes9KveX20Z",
        "outputId": "b04cf661-228e-4090-8a9f-deb493304b7d"
      },
      "source": [
        "# #Step 3 part 2 hypertuning the model to see which parameter are best for the neural network model. It took 48 minutes to execute \n",
        "\n",
        "# from keras.layers import Dense, Dropout,Flatten\n",
        "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "# from keras.models import Sequential\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from keras.layers import Dropout\n",
        "# from keras.layers.core import Activation\n",
        "# from keras.optimizers import Optimizer\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "# from keras.utils import to_categorical \n",
        "# from keras import backend as K\n",
        "# import keras\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import keras\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "# from sklearn.preprocessing import Normalizer\n",
        "# from numpy import reshape\n",
        "# from tensorflow.keras import backend\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dense, Input\n",
        "# from keras.regularizers import l1 \n",
        "# from keras.optimizers import Adam\n",
        "# from keras.losses import kullback_leibler_divergence\n",
        "# from keras.losses import mean_squared_error\n",
        "# from keras.models import Sequential \n",
        "# from keras.layers import Dropout\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier \n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# from keras.losses import MeanSquaredError\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.optimizers import Optimizer, RMSprop\n",
        "\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# #Buiding neural network\n",
        "# optimizers = ['WAME1()', 'WAME2()', 'WAME3()', 'WAME4()']#SO WAME IS THE BEST OPTIMISER\n",
        "# inits = ['uniform', 'glorot_uniform'] \n",
        "# epochs = [5, 7, 10]\n",
        "# batches = [20, 30, 40]\n",
        "\n",
        "\n",
        "# def create_model(optimizer=optimizers, init=inits):\n",
        "#   # create model\n",
        "#   mlp_model = Sequential()\n",
        "#   mlp_model.add(Dense(12, input_dim=108, activation='relu')) \n",
        "#   mlp_model.add(Dense(10, activation='relu')) \n",
        "#   mlp_model.add(Dense(20, activation='relu'))\n",
        "#   mlp_model.add(Dense(30, activation='relu'))\n",
        "#   mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        " \n",
        "#   mlp_model.compile(loss=tf.keras.losses.MeanSquaredError(),  metrics=[\"accuracy\"])\n",
        "#   return mlp_model\n",
        "\n",
        "# # create model\n",
        "\n",
        "# model_keras = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# # grid search epochs, batch size and optimizer\n",
        "# param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "# grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "# grid_result = grid.fit(rescaledX, Y_train)# so i decided after app lying the one hot encoder, do the rescaling of all 108 features and then see if its work\n",
        "# #print(\"Best: %f using %s\" % (grid_result.bestscore, grid_result.bestparams))\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))#checking the best hyperparameter along with the best optimiser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.852185 using {'batch_size': 30, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME1()'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "R0kf89-YiMBw",
        "outputId": "839bea44-9a71-4afb-93e5-7c63b0d3d117"
      },
      "source": [
        "# #Step 3 part2 to visualise the graph of WAME 1 accuracy but there is a problem\n",
        "# from keras.layers import Dense, Dropout,Flatten\n",
        "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "# from keras.models import Sequential\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from keras.layers import Dropout\n",
        "# from keras.layers.core import Activation\n",
        "# from keras.optimizers import Optimizer\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "# from keras.utils import to_categorical \n",
        "# from keras import backend as K\n",
        "# import keras\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import keras\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "# from sklearn.preprocessing import Normalizer\n",
        "# from numpy import reshape\n",
        "# from tensorflow.keras import backend\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dense, Input\n",
        "# from keras.regularizers import l1 \n",
        "# from keras.optimizers import Adam\n",
        "# from keras.losses import kullback_leibler_divergence\n",
        "# from keras.losses import mean_squared_error\n",
        "# from keras.models import Sequential \n",
        "# from keras.layers import Dropout\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier \n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# from keras.losses import MeanSquaredError\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.optimizers import Optimizer, RMSprop\n",
        "\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# #Buiding neural network\n",
        "# optimizers = ['WAME1()']#SO WAME IS THE BEST OPTIMISER\n",
        "# inits = ['uniform', 'glorot_uniform'] \n",
        "# epochs = [5, 7, 10]\n",
        "# batches = [20, 30, 40]\n",
        "\n",
        "\n",
        "# def create_model(optimizer=optimizers, init=inits):\n",
        "#   # create model\n",
        "#   mlp_model = Sequential()\n",
        "#   mlp_model.add(Dense(12, input_dim=108, activation='relu')) \n",
        "#   mlp_model.add(Dense(10, activation='relu')) \n",
        "#   mlp_model.add(Dense(20, activation='relu'))\n",
        "#   mlp_model.add(Dense(30, activation='relu'))\n",
        "#   mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        " \n",
        "#   mlp_model.compile(loss=tf.keras.losses.MeanSquaredError(),  metrics=[\"accuracy\"])\n",
        "#   return mlp_model\n",
        "\n",
        "# # create model\n",
        "\n",
        "# model_keras = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# # grid search epochs, batch size and optimizer\n",
        "# param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "# grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "# grid_result = grid.fit(rescaledX, Y_train)# so i decided after app lying the one hot encoder, do the rescaling of all 108 features and then see if its work\n",
        "# #print(\"Best: %f using %s\" % (grid_result.bestscore, grid_result.bestparams))\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "# print()\n",
        "\n",
        "# plt.plot(grid.grid['accuracy'])\n",
        "# plt.plot(grid.grid['val_accuracy'])\n",
        "# plt.title(\"Accuracy of WAME1\")\n",
        "# plt.xlabel('WAME1')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.legend(['train','test'])\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.849974 using {'batch_size': 20, 'epochs': 7, 'init': 'glorot_uniform', 'optimizer': 'WAME1()'}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-3432251ec930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy of WAME1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'grid'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "RhKFTOMnVA7n",
        "outputId": "1eb87208-a89b-4636-9304-ceb67fd8e1f3"
      },
      "source": [
        "# #Step 4a preparing the test data\n",
        "# import pandas as pd\n",
        "# names = ('age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','class')\n",
        "# dataset_test = pd.read_csv(filename2, names=names)\n",
        "# dataset_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>|1x3 Cross validator</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "      <td>Private</td>\n",
              "      <td>226802.0</td>\n",
              "      <td>11th</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>89814.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>336951.0</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>160323.0</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>39</td>\n",
              "      <td>Private</td>\n",
              "      <td>215419.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16278</th>\n",
              "      <td>64</td>\n",
              "      <td>?</td>\n",
              "      <td>321403.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>?</td>\n",
              "      <td>Other-relative</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>374983.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>83891.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Asian-Pac-Islander</td>\n",
              "      <td>Male</td>\n",
              "      <td>5455.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>35</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>182148.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16282 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        age      workclass  ...  native-country    class\n",
              "0      |1x3 Cross validator            NaN  ...             NaN      NaN\n",
              "1                        25        Private  ...   United-States   <=50K.\n",
              "2                        38        Private  ...   United-States   <=50K.\n",
              "3                        28      Local-gov  ...   United-States    >50K.\n",
              "4                        44        Private  ...   United-States    >50K.\n",
              "...                     ...            ...  ...             ...      ...\n",
              "16277                    39        Private  ...   United-States   <=50K.\n",
              "16278                    64              ?  ...   United-States   <=50K.\n",
              "16279                    38        Private  ...   United-States   <=50K.\n",
              "16280                    44        Private  ...   United-States   <=50K.\n",
              "16281                    35   Self-emp-inc  ...   United-States    >50K.\n",
              "\n",
              "[16282 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "4WKE8yUnsIAQ",
        "outputId": "989da23b-22e3-43ca-81a9-f2b0a3e5500a"
      },
      "source": [
        "# #Step 4d to visualise B_testdata to see the target variable is dropped\n",
        "# B_testdata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "      <td>Private</td>\n",
              "      <td>226802.0</td>\n",
              "      <td>11th</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>89814.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>336951.0</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>160323.0</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>18</td>\n",
              "      <td>?</td>\n",
              "      <td>103497.0</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>?</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>39</td>\n",
              "      <td>Private</td>\n",
              "      <td>215419.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16278</th>\n",
              "      <td>64</td>\n",
              "      <td>?</td>\n",
              "      <td>321403.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>?</td>\n",
              "      <td>Other-relative</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>374983.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>83891.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Asian-Pac-Islander</td>\n",
              "      <td>Male</td>\n",
              "      <td>5455.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>35</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>182148.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16281 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age      workclass  ...  hours-per-week  native-country\n",
              "1      25        Private  ...            40.0   United-States\n",
              "2      38        Private  ...            50.0   United-States\n",
              "3      28      Local-gov  ...            40.0   United-States\n",
              "4      44        Private  ...            40.0   United-States\n",
              "5      18              ?  ...            30.0   United-States\n",
              "...    ..            ...  ...             ...             ...\n",
              "16277  39        Private  ...            36.0   United-States\n",
              "16278  64              ?  ...            40.0   United-States\n",
              "16279  38        Private  ...            50.0   United-States\n",
              "16280  44        Private  ...            40.0   United-States\n",
              "16281  35   Self-emp-inc  ...            60.0   United-States\n",
              "\n",
              "[16281 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "2Wzt6o6KsOKg",
        "outputId": "ae13761c-30ec-4003-8d39-2f40d5191894"
      },
      "source": [
        "# #Step 4e SELECTING ALL ROWS AND COLUMNS IN X and putting it in Z dataset to visualise\n",
        "# X_testdata = B_testdata.iloc[:, :].values\n",
        "# Z_testdata = pd.DataFrame(X_testdata)\n",
        "# Z_testdata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "      <td>Private</td>\n",
              "      <td>226802</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>89814</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>336951</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>160323</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>?</td>\n",
              "      <td>103497</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>?</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>39</td>\n",
              "      <td>Private</td>\n",
              "      <td>215419</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>64</td>\n",
              "      <td>?</td>\n",
              "      <td>321403</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>?</td>\n",
              "      <td>Other-relative</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16278</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>374983</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>83891</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Asian-Pac-Islander</td>\n",
              "      <td>Male</td>\n",
              "      <td>5455</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>35</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>182148</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16281 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0              1       2              3   ...    10 11  12              13\n",
              "0      25        Private  226802           11th  ...     0  0  40   United-States\n",
              "1      38        Private   89814        HS-grad  ...     0  0  50   United-States\n",
              "2      28      Local-gov  336951     Assoc-acdm  ...     0  0  40   United-States\n",
              "3      44        Private  160323   Some-college  ...  7688  0  40   United-States\n",
              "4      18              ?  103497   Some-college  ...     0  0  30   United-States\n",
              "...    ..            ...     ...            ...  ...   ... ..  ..             ...\n",
              "16276  39        Private  215419      Bachelors  ...     0  0  36   United-States\n",
              "16277  64              ?  321403        HS-grad  ...     0  0  40   United-States\n",
              "16278  38        Private  374983      Bachelors  ...     0  0  50   United-States\n",
              "16279  44        Private   83891      Bachelors  ...  5455  0  40   United-States\n",
              "16280  35   Self-emp-inc  182148      Bachelors  ...     0  0  60   United-States\n",
              "\n",
              "[16281 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "oSjv7gQUtKQN",
        "outputId": "b4e45545-023b-4e7d-b5c9-3f9d394c9926"
      },
      "source": [
        "# #Step 4g the categorical columns are convered to 0s and 1s by using one hot encoder to the 8 categorical columns\n",
        "# #onehotvector gives a feature for every categorical option. No Need to do Feature extraction and Feature selection when using one hot encoder but Scaling is a must.\n",
        "# #onehotencoder = OneHotEncoder(categories = [1])\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# ct = ColumnTransformer(\n",
        "#     [('oh_enc', OneHotEncoder(sparse=False), [1, 3, 5, 6, 7, 8, 9, 13]),],  # the column numbers I want to apply this to\n",
        "#     remainder='passthrough'  # This leaves the rest of my columns in place\n",
        "# )\n",
        "# X_testdata = ct.fit_transform(X_testdata)\n",
        "# X_testdata\n",
        "\n",
        "# #print(ct.fit_transform(X)) # Notice the output is a string\n",
        "# Z_test = pd.DataFrame(X_testdata)\n",
        "# Z_test# No idea why the columns is 107 instead of 108"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>226802</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>89814</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>336951</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>160323</td>\n",
              "      <td>10</td>\n",
              "      <td>7688</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>103497</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>215419</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>321403</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16278</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>374983</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>83891</td>\n",
              "      <td>13</td>\n",
              "      <td>5455</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>182148</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16281 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0   1   2   3   4   5   6   7    ... 99  100 101     102 103   104 105 106\n",
              "0       0   0   0   0   1   0   0   0  ...   0   0  25  226802   7     0   0  40\n",
              "1       0   0   0   0   1   0   0   0  ...   0   0  38   89814   9     0   0  50\n",
              "2       0   0   1   0   0   0   0   0  ...   0   0  28  336951  12     0   0  40\n",
              "3       0   0   0   0   1   0   0   0  ...   0   0  44  160323  10  7688   0  40\n",
              "4       1   0   0   0   0   0   0   0  ...   0   0  18  103497  10     0   0  30\n",
              "...    ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..     ...  ..   ...  ..  ..\n",
              "16276   0   0   0   0   1   0   0   0  ...   0   0  39  215419  13     0   0  36\n",
              "16277   1   0   0   0   0   0   0   0  ...   0   0  64  321403   9     0   0  40\n",
              "16278   0   0   0   0   1   0   0   0  ...   0   0  38  374983  13     0   0  50\n",
              "16279   0   0   0   0   1   0   0   0  ...   0   0  44   83891  13  5455   0  40\n",
              "16280   0   0   0   0   0   1   0   0  ...   0   0  35  182148  13     0   0  60\n",
              "\n",
              "[16281 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "HzRyFQoDtapX",
        "outputId": "a952970b-4294-40db-9c4d-d62c51e20c6e"
      },
      "source": [
        "# #step 4 h\n",
        "# #this is just to test step 1 h if this works that means after applying the one hot encoding on the categorical variable we then need to scale all the features to prepare it for the model so that ML algorithm can be applied to it\n",
        "# #Now Scaling the categorical and the continuous variables by using MinMaxScalar\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from numpy import set_printoptions\n",
        "# #rescaled data between 0 and 1\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# rescaledY = scaler.fit_transform(Z_test)\n",
        "# #summarize transformed data\n",
        "# set_printoptions(precision=3)\n",
        "# print(rescaledY)\n",
        "# M = pd.DataFrame(rescaledY) #M shows that the dataset has been scaled\n",
        "# M\n",
        "\n",
        "# #so rescaledX is the final training dataset that I will use"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.    0.    0.    ... 0.    0.    0.398]\n",
            " [0.    0.    0.    ... 0.    0.    0.5  ]\n",
            " [0.    0.    1.    ... 0.    0.    0.398]\n",
            " ...\n",
            " [0.    0.    0.    ... 0.    0.    0.5  ]\n",
            " [0.    0.    0.    ... 0.055 0.    0.398]\n",
            " [0.    0.    0.    ... 0.    0.    0.602]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.109589</td>\n",
              "      <td>0.144430</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.051677</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.219011</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.369863</td>\n",
              "      <td>0.099418</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.076881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013699</td>\n",
              "      <td>0.060942</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.295918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301370</td>\n",
              "      <td>0.136723</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.643836</td>\n",
              "      <td>0.208484</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16278</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.244762</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.369863</td>\n",
              "      <td>0.047666</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.054551</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.246575</td>\n",
              "      <td>0.114195</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.602041</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16281 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2    3    4    ...       102       103       104  105       106\n",
              "0      0.0  0.0  0.0  0.0  1.0  ...  0.144430  0.400000  0.000000  0.0  0.397959\n",
              "1      0.0  0.0  0.0  0.0  1.0  ...  0.051677  0.533333  0.000000  0.0  0.500000\n",
              "2      0.0  0.0  1.0  0.0  0.0  ...  0.219011  0.733333  0.000000  0.0  0.397959\n",
              "3      0.0  0.0  0.0  0.0  1.0  ...  0.099418  0.600000  0.076881  0.0  0.397959\n",
              "4      1.0  0.0  0.0  0.0  0.0  ...  0.060942  0.600000  0.000000  0.0  0.295918\n",
              "...    ...  ...  ...  ...  ...  ...       ...       ...       ...  ...       ...\n",
              "16276  0.0  0.0  0.0  0.0  1.0  ...  0.136723  0.800000  0.000000  0.0  0.357143\n",
              "16277  1.0  0.0  0.0  0.0  0.0  ...  0.208484  0.533333  0.000000  0.0  0.397959\n",
              "16278  0.0  0.0  0.0  0.0  1.0  ...  0.244762  0.800000  0.000000  0.0  0.500000\n",
              "16279  0.0  0.0  0.0  0.0  1.0  ...  0.047666  0.800000  0.054551  0.0  0.397959\n",
              "16280  0.0  0.0  0.0  0.0  0.0  ...  0.114195  0.800000  0.000000  0.0  0.602041\n",
              "\n",
              "[16281 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "6OyYr-ActqvX",
        "outputId": "aa1af4bd-e912-424e-f66b-60863b1ce756"
      },
      "source": [
        "# #STEP 8 FirstTesting WITHOUT USING wame Best: 0.852185 using {'batch_size': 30, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME1()'}\n",
        "# from keras.models import Sequential\n",
        "\n",
        "# from keras.layers import Dense\n",
        "\n",
        "# inits = ['uniform'] \n",
        "# epochs = [10]\n",
        "# batches = [30]\n",
        "# #the SEQUENTIAL MODULE IS REQUIRED TO INITIALIZE THE ANN, AND DENSE MODULE IS REQUIRED TO BUILD THE LAYERS OF OUR ANN\n",
        "#   # create model\n",
        "# classifier = Sequential()#initialise the ANN by creating an instance of Sequential\n",
        "# classifier.add(Dense(12, input_dim=14, activation='relu')) \n",
        "# classifier.add(Dense(10, activation='relu')) \n",
        "# classifier.add(Dense(20, activation='relu'))\n",
        "# classifier.add(Dense(30, activation='relu'))\n",
        "# classifier.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "# opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# classifier.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(),  metrics=[\"accuracy\"])\n",
        "\n",
        "# classifier.fit(rescaledX, Y_train, batch_size=20, epochs=10) #init = 'global_uniform')\n",
        "# # create model\n",
        "# y_pred = classifier.predict(rescaledY)\n",
        "# y_pred = (y_pred > 0.5)\n",
        "  \n",
        "# from sklearn.metrics import confusion_matrix#, classification_report\n",
        "# #cm = confusion_matrix(Y_test, y_pred)\n",
        "\n",
        "# #model = LogisticRegression(solver='liblinear') model.fit(X_train, Y_train)\n",
        "# #predicted = model.predict(X_test)\n",
        "# report = classification_report(Y_test, y_pred) \n",
        "# print(report)\n",
        "\n",
        "# #predicted = model.predict(X_test)\n",
        "# matrix = confusion_matrix(Y_test, y_pred) \n",
        "# print(matrix)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-7bb77660c419>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaledX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#init = 'global_uniform')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaledY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_1627 is incompatible with the layer: expected axis -1 of input shape to have value 14 but received input with shape (None, 108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzCLDrM6NcY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}